{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MainRegressor notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "InXN9nCfMiKI",
        "rWvdPudwMiKI",
        "Udag3n7Ig5Zs"
      ],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7atJJ2fMiJ8"
      },
      "source": [
        "# I started with this notebook and adapated a bunch of things \n",
        "\n",
        "Source: \n",
        "https://www.kaggle.com/code1110/numerai-tournament\n",
        "\n",
        "## This Notebook does the following things:\n",
        "1. Get  the Training and Tournment data from the Numerai static links. \n",
        "2. Cast the data into Panadas DF and limit the memory use. \n",
        "3. Train two models and submit the result on numerai.\n",
        "4. I remove all the internal scoring for the two models. You just pick two Hyperparam sets for Light GBM and then submit them. \n",
        "\n",
        "\n",
        "\n",
        "## There are some problems in this. Spend a few hours to refactor the sumbit methods\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0iP-UgesMiKB",
        "outputId": "9d123831-9b4a-45b8-9a8b-796676e4a8f2"
      },
      "source": [
        "!pip install numerapi\n",
        "import numerapi"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting numerapi\n",
            "  Downloading https://files.pythonhosted.org/packages/fd/96/ebdbaff5a2fef49b212e4f40634166f59e45462a768c0136d148f00255c5/numerapi-2.4.5-py3-none-any.whl\n",
            "Requirement already satisfied: tqdm>=4.29.1 in /usr/local/lib/python3.7/dist-packages (from numerapi) (4.41.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from numerapi) (2.8.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from numerapi) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from numerapi) (2.23.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from numerapi) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil->numerapi) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->numerapi) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->numerapi) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->numerapi) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->numerapi) (2.10)\n",
            "Installing collected packages: numerapi\n",
            "Successfully installed numerapi-2.4.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4WFeO-ZMiKC"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os, sys\n",
        "import gc\n",
        "import pathlib\n",
        "import json\n",
        "from typing import List, NoReturn, Union, Tuple, Optional, Text, Generic, Callable, Dict\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, QuantileTransformer\n",
        "from sklearn.model_selection import KFold, StratifiedKFold, TimeSeriesSplit\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, log_loss, mean_squared_error, mean_absolute_error, f1_score\n",
        "from scipy.stats import spearmanr # -P I think this is corr. \n",
        "import joblib\n",
        "\n",
        "# model\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "import operator\n",
        "\n",
        "# visualize\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.style as style\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot\n",
        "from matplotlib.ticker import ScalarFormatter\n",
        "sns.set_context(\"talk\")\n",
        "style.use('seaborn-colorblind')\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b364c12-MiKD"
      },
      "source": [
        "Methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mm1stk1pMiKD"
      },
      "source": [
        "def cast_eras_as_int(x): # this is used to cast the eras from strings to ints\n",
        "    try:\n",
        "        return int(x[3:]) # the eras look like era03\n",
        "    except:\n",
        "        return 1000\n",
        "\n",
        "# unclear if numerapi.download_latest_data() would be faster\n",
        "def read_data(data='train'):\n",
        "    # get data \n",
        "    if data == 'train':\n",
        "        df = pd.read_csv('https://numerai-public-datasets.s3-us-west-2.amazonaws.com/latest_numerai_training_data.csv.xz')\n",
        "\n",
        "        # The test data is significantly larger.\n",
        "        # test data is the live tournment data\n",
        "    elif data == 'test':\n",
        "        df = pd.read_csv('https://numerai-public-datasets.s3-us-west-2.amazonaws.com/latest_numerai_tournament_data.csv.xz')\n",
        "        \n",
        "    \n",
        "    # features\n",
        "    feature_cols = df.columns[df.columns.str.startswith('feature')]\n",
        "    \n",
        "    # map to int, to reduce the memory demand\n",
        "    mapping = {0.0 : 0, 0.25 : 1, 0.5 : 2, 0.75 : 3, 1.0 : 4} # this is very clever -P\n",
        "    for c in feature_cols:\n",
        "        df[c] = df[c].map(mapping).astype(np.uint8)\n",
        "        \n",
        "    df[\"era\"] = df[\"era\"].apply(cast_eras_as_int)# also cast era to int\n",
        "    return df"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6qoKn7W4Za_"
      },
      "source": [
        "# Load the Data from Numerai"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyKw25FmMiKD",
        "outputId": "11da1b6c-810f-48db-e362-7d22530bf0cc"
      },
      "source": [
        "%%time\n",
        "# load in the Training data\n",
        "train_data = read_data('train')\n",
        "train_data.shape\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(501808, 314)\n",
            "CPU times: user 46.8 s, sys: 3.52 s, total: 50.3 s\n",
            "Wall time: 54.7 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SuYAVDvM_lyT",
        "outputId": "c023c620-74e0-4566-8be9-7f0b1492cffd"
      },
      "source": [
        "train_data.columns"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['id', 'era', 'feature_intelligence1', 'feature_intelligence2',\n",
              "       'feature_intelligence3', 'feature_intelligence4',\n",
              "       'feature_intelligence5', 'feature_intelligence6',\n",
              "       'feature_intelligence7', 'feature_intelligence8',\n",
              "       ...\n",
              "       'feature_wisdom38', 'feature_wisdom39', 'feature_wisdom40',\n",
              "       'feature_wisdom41', 'feature_wisdom42', 'feature_wisdom43',\n",
              "       'feature_wisdom44', 'feature_wisdom45', 'feature_wisdom46', 'target'],\n",
              "      dtype='object', length=313)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOiCNwHGMiKE",
        "outputId": "07221eaa-ff98-46d1-91c6-e6c9a9da9797"
      },
      "source": [
        "%%time\n",
        "# the testing data is the tournament data\n",
        "tournament_data = read_data('test')\n",
        "tournament_data.shape\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 2min 48s, sys: 10.4 s, total: 2min 58s\n",
            "Wall time: 3min 8s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqHOxBJ8_Xp7",
        "outputId": "1dc40879-f2d2-482c-cfd4-96fd186c4e9e"
      },
      "source": [
        "print(tournament_data['id'].count())\n",
        "tournament_data.columns\n",
        "tournament_data['target'].count() # count non nan targets"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1676742\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "137779"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "strAzqc-MiKE"
      },
      "source": [
        "Create a seperate valid split. \n",
        "Read more https://towardsdatascience.com/train-validation-and-test-sets-72cb40cba9e7\n",
        "Train: Model sees and learns this data\n",
        "\n",
        "Validation: Use this to see your score. This is what you use to tune hyperparameters.\n",
        "\n",
        "Valid is a subset of the test data. it is where the data_type is 'Validation'\n",
        "\n",
        "### I don't have good evidence for splits in this size. It might be better for some other splits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LD9w9dCYMiKF"
      },
      "source": [
        "# %%time\n",
        "# # validation is derived from the live tournament data\n",
        "# valid = tournament_data[tournment_data[\"data_type\"] == \"validation\"].reset_index(drop = True) # when the data_type == Validation that means there is already a target for those vectors.\n",
        "# # those are the last spot check on the model's vectors \n",
        "\n",
        "# print(valid.columns) # valid is a df where\n",
        "# # validation split\n",
        "# valid.loc[valid[\"era\"] > 180, \"valid2\"] = True # Every era after 180 is in validation\n",
        "\n",
        "# # you will want ot lookat the places where\n",
        "# valid.loc[valid[\"era\"] <= 180, \"valid2\"] = False # Every era before is not in the validation set. \n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qG2TlTRy_7OU"
      },
      "source": [
        "### There is no 'data_type' column when you are submittign out of the window. Today is Tuesday and there is no 'data_type' column. \n",
        "\n",
        "come back and finish refactoring this on saturday / Sunday."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "id": "9l_BqycKMiKF",
        "outputId": "034091b6-8dfd-409d-9825-3fd8e0e60918"
      },
      "source": [
        "# remove data_type to save memory\n",
        "train_data.drop(columns=[\"data_type\"], inplace=True)\n",
        "# valid.drop(columns=[\"data_type\"], inplace=True) You don't have validation anymore\n",
        "tournament_data.drop(columns=[\"data_type\"], inplace=True)\n",
        "\n",
        "print('The number of records: train {:,}, test {:,}'.format(train_data.shape[0], tournament_data.shape[0])) # df.shape[0] is number of rows."
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-ee8e5d56fe1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#train_data.drop(columns=[\"data_type\"], inplace=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# valid.drop(columns=[\"data_type\"], inplace=True) You don't have validation anymore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtournament_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"data_type\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4172\u001b[0m             \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4173\u001b[0m             \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4174\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4175\u001b[0m         )\n\u001b[1;32m   4176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   3887\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3888\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3889\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3891\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[1;32m   3921\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3922\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3923\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3924\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   5285\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5286\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5287\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{labels[mask]} not found in axis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5288\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5289\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"['data_type'] not found in axis\""
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AiydLA2MiKG"
      },
      "source": [
        "## Determine features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "box2-PgOMiKG",
        "outputId": "d137d2ff-29ef-4bf0-a5a1-f16c156a8201"
      },
      "source": [
        "# features\n",
        "features = [f for f in tournament_data.columns.values.tolist() if 'feature' in f] # fancy for loop to get all the feature names. -p\n",
        "print('There are {} features.'.format(len(features)))\n",
        "print(type(features))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 310 features.\n",
            "<class 'list'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQYEv6mBMiKG"
      },
      "source": [
        "## Target"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlZuKRcVMiKH"
      },
      "source": [
        "target = tournament_data.columns[tournament_data.columns.str.startswith('target')].values.tolist()[0] # 'target'"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCki_3ozMiKH"
      },
      "source": [
        "# Modeling\n",
        "The example script of xgboost made by numerai [Example](https://github.com/numerai/example-scripts/blob/master/example_model.py)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xg5NIX_pMiKH"
      },
      "source": [
        "Link to hyperparameters of xgboost \n",
        "https://xgboost.readthedocs.io/en/latest/parameter.html\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JnX3Se_MiKH"
      },
      "source": [
        "# # create a model and fit (公式example)\n",
        "\n",
        "# untested. -p\n",
        "# model = xgb.XGBRegressor(max_depth=5, learning_rate=0.01, n_estimators=2000, n_jobs=-1, colsample_bytree=0.1)\n",
        "# model.fit(train[features], train[target])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-IVQvIZ3saD"
      },
      "source": [
        "# Train mrquants model and tut_model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mujvbb65MiKI",
        "outputId": "706df647-fc0b-4ef4-ba7e-d5e2d7dbefae"
      },
      "source": [
        "%%time\n",
        "\n",
        "mr_quants_parmas = {\n",
        "            'n_estimators': 5000,\n",
        "            'objective': 'regression',\n",
        "            'boosting_type': 'gbdt',\n",
        "            'max_depth': 55,\n",
        "            'learning_rate': 0.013, \n",
        "            'feature_fraction': 0.095,\n",
        "            'seed': 42\n",
        "            }\n",
        "tut_parmas = {\n",
        "            'n_estimators': 3000,\n",
        "            'objective': 'regression',\n",
        "            'boosting_type': 'gbdt',\n",
        "            'max_depth': 20,\n",
        "            'learning_rate': 0.013, \n",
        "            'feature_fraction': 0.095,\n",
        "            'seed': 42\n",
        "            }\n",
        "\n",
        "mr_quants_model = lgb.LGBMRegressor(**mr_quants_parmas) \n",
        "mr_quants_model.fit(train_data[features], train_data[target])\n",
        "\n",
        "tut_model = lgb.LGBMRegressor(**tut_parmas) # I think ** \n",
        "tut_model.fit(train_data[features], train_data[target])"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 17min 13s, sys: 4.83 s, total: 17min 18s\n",
            "Wall time: 4min 25s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InXN9nCfMiKI"
      },
      "source": [
        "# Feature importance\n",
        "\n",
        "You can see what features are important and the distribution of weights assigned to each of the features.\n",
        "You might use this as part of the meta model of light xbgoost \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3ZzS4dXMiKI"
      },
      "source": [
        "# %%time\n",
        "# feature_importance =pd.DataFrame(model.feature_importances_, index=features, columns=['importance'])\n",
        "# feature_importance.describe()"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSL-9D4yMiKI"
      },
      "source": [
        "It might make sense to group your models into clusters based on this stat. \n",
        "You would want to scale them down to 1 when you group them. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWvdPudwMiKI"
      },
      "source": [
        "# Validation Score\n",
        "These are the methods to evaluate your model.\n",
        "I did not write these models, many of the comments written I wrote though.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szXbJM0mMiKJ"
      },
      "source": [
        "# naming conventions\n",
        "PREDICTION_NAME = 'prediction'\n",
        "TARGET_NAME = target # 'target is the string named 'target'\n",
        "# EXAMPLE_PRED = 'example_prediction'\n",
        "\n",
        "# ---------------------------\n",
        "# Functions\n",
        "# ---------------------------\n",
        "def valid4score(valid : pd.DataFrame, pred : np.ndarray, load_example: bool=True, save : bool=False) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Generate new valid pandas dataframe for computing scores\n",
        "    \n",
        "    :INPUT:\n",
        "    - valid : pd.DataFrame extracted from tournament data (data_type='validation')\n",
        "    \n",
        "    \"\"\"\n",
        "    valid_df = valid.copy() # the validation dataframe you use this to test the CORR and other values\n",
        "\n",
        "    # Your model creates an array of floats [0,1] rank method converst them in a list of ints. \n",
        "\n",
        "    # your lis tof ints is then compared to their list of ints. \n",
        "    valid_df['prediction'] = pd.Series(pred).rank(pct=True, method=\"first\") # pred is the array of predictions your model creates for the set of validation vectors.  \n",
        "    # I am unsure if this preds is a float only only between 0,1,2,3,4. \n",
        "    valid_df.rename(columns={target: 'target'}, inplace=True)\n",
        "    \n",
        "    # I don't know what the load example boolean is. I think you can use this to save predictions.\n",
        "    if load_example:\n",
        "        valid_df[EXAMPLE_PRED] = pd.read_csv(EXP_DIR + 'valid_df.csv')['prediction'].values\n",
        "    \n",
        "    if save==True:\n",
        "        valid_df.to_csv(OUTPUT_DIR + 'valid_df.csv', index=False)\n",
        "        print('Validation dataframe saved!')\n",
        "    \n",
        "    return valid_df\n",
        "\n",
        "def compute_corr(valid_df : pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Compute rank correlation\n",
        "\n",
        "    THIS IS WHAT YOU ARE PRIMARILY PAID ON \n",
        "    \n",
        "    :INPUT:\n",
        "    - valid_df : pd.DataFrame where at least 2 columns ('prediction' & 'target') exist\n",
        "    \n",
        "    \"\"\"\n",
        "    # this uses Person Correilation. \n",
        "    # I You are paid on spearman corrilation. That is where the ratio of change is important not the raw amount of change\n",
        "    # see: https://support.minitab.com/en-us/minitab-express/1/help-and-how-to/modeling-statistics/regression/supporting-topics/basics/a-comparison-of-the-pearson-and-spearman-correlation-methods/\n",
        "    return np.corrcoef(valid_df[\"target\"], valid_df['prediction'])[0, 1]\n",
        "\n",
        "def compute_max_drawdown(validation_correlations : pd.Series):\n",
        "    \"\"\"\n",
        "    Compute max drawdown\n",
        "    \n",
        "    :INPUT:\n",
        "    - validation_correaltions : pd.Series\n",
        "    \"\"\"\n",
        "    \n",
        "    rolling_max = (validation_correlations + 1).cumprod().rolling(window=100, min_periods=1).max()\n",
        "    daily_value = (validation_correlations + 1).cumprod()\n",
        "    max_drawdown = -(rolling_max - daily_value).max()\n",
        "    \n",
        "    return max_drawdown\n",
        "\n",
        "def compute_val_corr(valid_df : pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Compute rank correlation for valid periods\n",
        "    \n",
        "    :INPUT:\n",
        "    - valid_df : pd.DataFrame where at least 2 columns ('prediction' & 'target') exist\n",
        "    \"\"\"\n",
        "    \n",
        "    # all validation\n",
        "    correlation = compute_corr(valid_df)\n",
        "    print(\"rank corr = {:.4f}\".format(correlation))\n",
        "    return correlation\n",
        "    \n",
        "def compute_val_sharpe(valid_df : pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Compute sharpe ratio for valid periods\n",
        "    \n",
        "    :INPUT:\n",
        "    - valid_df : pd.DataFrame where at least 2 columns ('prediction' & 'target') exist\n",
        "    \"\"\"\n",
        "    # all validation\n",
        "    d = valid_df.groupby('era')[['target', 'prediction']].corr().iloc[0::2,-1].reset_index()\n",
        "    me = d['prediction'].mean()\n",
        "    sd = d['prediction'].std()\n",
        "    max_drawdown = compute_max_drawdown(d['prediction'])\n",
        "    print('sharpe ratio = {:.4f}, corr mean = {:.4f}, corr std = {:.4f}, max drawdown = {:.4f}'.format(me / sd, me, sd, max_drawdown))\n",
        "    \n",
        "    return me / sd, me, sd, max_drawdown\n",
        "    \n",
        "def feature_exposures(valid_df : pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Compute feature exposure\n",
        "    \n",
        "    :INPUT:\n",
        "    - valid_df : pd.DataFrame where at least 2 columns ('prediction' & 'target') exist\n",
        "    \"\"\"\n",
        "    feature_names = [f for f in valid_df.columns\n",
        "                     if f.startswith(\"feature\")]\n",
        "    exposures = []\n",
        "    for f in feature_names:\n",
        "        fe = spearmanr(valid_df['prediction'], valid_df[f])[0]\n",
        "        exposures.append(fe)\n",
        "    return np.array(exposures)\n",
        "\n",
        "def max_feature_exposure(fe : np.ndarray):\n",
        "    return np.max(np.abs(fe))\n",
        "\n",
        "def feature_exposure(fe : np.ndarray):\n",
        "    return np.sqrt(np.mean(np.square(fe)))\n",
        "\n",
        "def compute_val_feature_exposure(valid_df : pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Compute feature exposure for valid periods\n",
        "    \n",
        "    :INPUT:\n",
        "    - valid_df : pd.DataFrame where at least 2 columns ('prediction' & 'target') exist\n",
        "    \"\"\"\n",
        "    # all validation\n",
        "    fe = feature_exposures(valid_df)\n",
        "    fe1, fe2 = feature_exposure(fe), max_feature_exposure(fe)\n",
        "    print('feature exposure = {:.4f}, max feature exposure = {:.4f}'.format(fe1, fe2))\n",
        "     \n",
        "    return fe1, fe2\n",
        "\n",
        "# to neutralize a column in a df by many other columns\n",
        "#         I have no idea what this method does. -P. need to read about it and write up a link to it. \n",
        "def neutralize(df, columns, by, proportion=1.0):\n",
        "    scores = df.loc[:, columns]\n",
        "    exposures = df[by].values\n",
        "\n",
        "    # constant column to make sure the series is completely neutral to exposures\n",
        "    exposures = np.hstack(\n",
        "        (exposures,\n",
        "         np.asarray(np.mean(scores)) * np.ones(len(exposures)).reshape(-1, 1)))\n",
        "\n",
        "    scores = scores - proportion * exposures.dot(\n",
        "        np.linalg.pinv(exposures).dot(scores))\n",
        "    return scores / scores.std()\n",
        "\n",
        "\n",
        "# to neutralize any series by any other series\n",
        "def neutralize_series(series, by, proportion=1.0):\n",
        "    scores = series.values.reshape(-1, 1)\n",
        "    exposures = by.values.reshape(-1, 1)\n",
        "\n",
        "    # this line makes series neutral to a constant column so that it's centered and for sure gets corr 0 with exposures\n",
        "    exposures = np.hstack(\n",
        "        (exposures,\n",
        "         np.array([np.mean(series)] * len(exposures)).reshape(-1, 1)))\n",
        "\n",
        "    correction = proportion * (exposures.dot(\n",
        "        np.linalg.lstsq(exposures, scores, rcond=None)[0]))\n",
        "    corrected_scores = scores - correction\n",
        "    neutralized = pd.Series(corrected_scores.ravel(), index=series.index)\n",
        "    return neutralized\n",
        "\n",
        "\n",
        "def unif(df):\n",
        "    x = (df.rank(method=\"first\") - 0.5) / len(df)\n",
        "    return pd.Series(x, index=df.index)\n",
        "\n",
        "def get_feature_neutral_mean(df):\n",
        "    feature_cols = [c for c in df.columns if c.startswith(\"feature\")]\n",
        "    df.loc[:, \"neutral_sub\"] = neutralize(df, [PREDICTION_NAME],\n",
        "                                          feature_cols)[PREDICTION_NAME]\n",
        "    scores = df.groupby(\"era\").apply(\n",
        "        lambda x: np.corrcoef(x[\"neutral_sub\"].rank(pct=True, method=\"first\"), x[TARGET_NAME])).mean()\n",
        "    return np.mean(scores)\n",
        "\n",
        "def compute_val_mmc(valid_df : pd.DataFrame):    \n",
        "    # MMC over validation\n",
        "    mmc_scores = []\n",
        "    corr_scores = []\n",
        "    for _, x in valid_df.groupby(\"era\"):\n",
        "        series = neutralize_series(pd.Series(unif(x[PREDICTION_NAME])),\n",
        "                                   pd.Series(unif(x[EXAMPLE_PRED])))\n",
        "        mmc_scores.append(np.cov(series, x[TARGET_NAME])[0, 1] / (0.29 ** 2))\n",
        "        corr_scores.append(np.corrcoef(unif(x[PREDICTION_NAME]).rank(pct=True, method=\"first\"), x[TARGET_NAME]))\n",
        "\n",
        "    val_mmc_mean = np.mean(mmc_scores)\n",
        "    val_mmc_std = np.std(mmc_scores)\n",
        "    val_mmc_sharpe = val_mmc_mean / val_mmc_std\n",
        "    corr_plus_mmcs = [c + m for c, m in zip(corr_scores, mmc_scores)]\n",
        "    corr_plus_mmc_sharpe = np.mean(corr_plus_mmcs) / np.std(corr_plus_mmcs)\n",
        "    corr_plus_mmc_mean = np.mean(corr_plus_mmcs)\n",
        "\n",
        "    print(\"MMC Mean = {:.6f}, MMC Std = {:.6f}, CORR+MMC Sharpe = {:.4f}\".format(val_mmc_mean, val_mmc_std, corr_plus_mmc_sharpe))\n",
        "\n",
        "    # Check correlation with example predictions\n",
        "    corr_with_example_preds = np.corrcoef(valid_df[EXAMPLE_PRED].rank(pct=True, method=\"first\"),\n",
        "                                          valid_df[PREDICTION_NAME].rank(pct=True, method=\"first\"))[0, 1]\n",
        "    print(\"Corr with example preds: {:.4f}\".format(corr_with_example_preds))\n",
        "    \n",
        "    return val_mmc_mean, val_mmc_std, corr_plus_mmc_sharpe, corr_with_example_preds\n",
        "\n",
        "\n",
        "# this is the main method. The rest are just called interanlly. \n",
        "def score_summary(valid_df : pd.DataFrame):\n",
        "    score_df = {}\n",
        "    \n",
        "    try:\n",
        "        score_df['correlation'] = compute_val_corr(valid_df)\n",
        "    except:\n",
        "        print('ERR: computing correlation')\n",
        "    try:\n",
        "        score_df['corr_sharpe'], score_df['corr_mean'], score_df['corr_std'], score_df['max_drawdown'] = compute_val_sharpe(valid_df)\n",
        "    except:\n",
        "        print('ERR: computing sharpe')\n",
        "    try:\n",
        "        score_df['feature_exposure'], score_df['max_feature_exposure'] = compute_val_feature_exposure(valid_df)\n",
        "    except:\n",
        "        print('ERR: computing feature exposure')\n",
        "    try:\n",
        "        score_df['mmc_mean'], score_df['mmc_std'], score_df['corr_mmc_sharpe'], score_df['corr_with_example_xgb'] = compute_val_mmc(valid_df)\n",
        "    except:\n",
        "        print('ERR: computing MMC')\n",
        "    \n",
        "    return pd.DataFrame.from_dict(score_df, orient='index')"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBmC2Gx8MiKM"
      },
      "source": [
        "# # prediction for valid periods   \n",
        "# # peek at the number of prediction\n",
        "# pred = model.predict(valid[features])\n",
        "# print(f'You made {len(pred)} total predictions')\n",
        "# print(type(pred))\n",
        "# print(pred[0])"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfFcD_UlMiKQ"
      },
      "source": [
        "# # scores\n",
        "\n",
        "\n",
        "# valid_df = valid4score(valid, pred, load_example=False, save=False)\n",
        "\n",
        "# score_df = pd.DataFrame()\n",
        "# print('------------------')\n",
        "# print('ALL:')\n",
        "# print('------------------')\n",
        "# all_ = score_summary(valid_df).rename(columns={0: 'all'})\n",
        "\n",
        "# print('------------------')\n",
        "# print('VALID 1:')\n",
        "# print('------------------')\n",
        "# val1_ = score_summary(valid_df.query('era < 150')).rename(columns={0: 'val1'}) \n",
        "# # there might be something strange with these numbers. before they are 180 need to verify unsure what valid2 = True means.\n",
        "\n",
        "# print('------------------')\n",
        "# print('VALID 2:')\n",
        "# print('------------------')\n",
        "# val2_ = score_summary(valid_df.query('era > 150')).rename(columns={0: 'val2'})"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Udag3n7Ig5Zs"
      },
      "source": [
        "# See Scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeodHr9WMiKQ"
      },
      "source": [
        "# # scores\n",
        "# score_df = pd.concat([all_, val1_, val2_], axis=1)\n",
        "# score_df.style.background_gradient(cmap='viridis', axis=0)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsRJSpJNMiKQ"
      },
      "source": [
        "# Submission\n",
        "\n",
        "\n",
        "1. Create the prediction list.\n",
        "2. Link those predictions with the tournment data\n",
        "3. Write the id, prediction to a csv file.\n",
        "4. Use numerai wrapper to submit that .csv file as your current model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSo8yDCxTBB1"
      },
      "source": [
        "### Methods to handle submission"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EoYEwqg4MiKQ",
        "outputId": "39b98001-c783-46bb-8490-cc7d6f154553"
      },
      "source": [
        "def load_api_creds_into_dict(): # works\n",
        "  \"\"\"\n",
        "    Read creds.json and return a dictionary of your API keys.\n",
        "  \"\"\"\n",
        "  creds  = open('creds.json','r')\n",
        "  api_keys_dict = json.load(creds)\n",
        "  creds.close()\n",
        "  return api_keys_dict\n",
        "\n",
        "\n",
        "def open_api_access(): # works\n",
        "    \"\"\"\n",
        "    Read in my private key from creds.json and return the numerai api wrapper\n",
        "    \"\"\"\n",
        "    api_keys_dict = load_api_creds_into_dict()\n",
        "    my_secret_key = api_keys_dict['secret_key']\n",
        "    my_public_id = api_keys_dict['public_id']\n",
        "    napi = numerapi.NumerAPI(secret_key=my_secret_key, public_id=my_public_id)\n",
        "    return napi\n",
        "\n",
        "\n",
        "def create_id_prediction_df(tournament_data: pd.DataFrame, model_predictions : np.ndarray): # works\n",
        "    \"\"\"\n",
        "    Create a dataframe that looks like \n",
        "    id,prediction\n",
        "    asdfads,.5429\n",
        "    asdfaddsss,.5051\n",
        "    ...\n",
        "    \"\"\"\n",
        "    predictions_df = tournament_data[\"id\"].to_frame() # get all the Ids and cast them to a Dataframe\n",
        "    predictions_df[PREDICTION_NAME] = model_predictions #add your predictions to the id frame\n",
        "    return predictions_df # data frame of id, prediction\n",
        "\n",
        "\n",
        "def write_predictions_to_file(prediction_df: pd.DataFrame): # works\n",
        "    my_file_name = 'myPredictions.csv'\n",
        "    try:\n",
        "      out_location = open(my_file_name, 'x')\n",
        "    except:\n",
        "      out_location = open(my_file_name, 'w')\n",
        "\n",
        "    prediction_df.to_csv(out_location, index=False)\n",
        "    out_location.close()\n",
        "    return my_file_name \n",
        "\n",
        "\n",
        "def run_model_and_create_prediction_file(model_object, tournament_data: pd.DataFrame, features: list): # untested\n",
        "  \"\"\"\n",
        "    This stitches everything together.\n",
        "\n",
        "    Pass it a trained model and the tournament data set, the list of feature columns\n",
        "    1. Does preditions\n",
        "    2. write the predictions to a file.\n",
        "    3. returns the name fo the file where my predictions are saved data is saved\n",
        "  \"\"\"\n",
        "  model_predictions = model_object.predict(tournament_data[features])\n",
        "  prediction_df = create_id_prediction_df(tournament_data,model_predictions)\n",
        "  file_with_predictions = write_predictions_to_file(prediction_df)\n",
        "  return file_with_predictions\n",
        "\n",
        "\n",
        "def submit_predictions_to_numerai(filename_of_predictions, sumbit_model_id):\n",
        "    napi = open_api_access() # open a connection to the numerai API with your creds.json file\n",
        "    submission_id = napi.upload_predictions(filename_of_predictions, model_id=sumbit_model_id)\n",
        "    print(f'You successfully submitted for {sumbit_model_id}')\n",
        "    print(type(submission_id))\n",
        "    return submission_id\n",
        "\n",
        "print('your helper methods work correctly')\n",
        "    "
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "your helper methods work correctly\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54YIKJia7TNa"
      },
      "source": [
        "api_keys_dict = load_api_creds_into_dict()\n",
        "mrquantsalot_model_id = api_keys_dict['mr_quants_model_id']\n",
        "tut_model_model_id = api_keys_dict['tutmodel_model_id']\n",
        "PREDICTION_NAME = \"prediction\" # this is the header of the csv file you are creating\n",
        "OUTPUT_DIR = '' # just the root of your local folder in this instance of google colab"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vwKpIz2XEw2"
      },
      "source": [
        "### Run your model on the real tournament data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4vmuIio762C"
      },
      "source": [
        "## Run and submit MRQUANTSALOT\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8FKXKURMiKR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "outputId": "a86e6dc4-1c90-4b0d-9dd2-1e7834a887c2"
      },
      "source": [
        "%%time\n",
        "mr_quants_file_with_predictions = run_model_and_create_prediction_file(mr_quants_model, tournament_data, features)\n",
        "numerai_submission_id_mrQ = submit_predictions_to_numerai(mr_quants_file_with_predictions, mrquantsalot_model_id)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-03-23 21:55:17,761 INFO numerapi.base_api: uploading predictions...\n",
            "2021-03-23 21:55:23,774 ERROR numerapi.base_api: Can't update submission after deadline if previous submission was on time\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-882a510ed055>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mr_quants_file_with_predictions = run_model_and_create_prediction_file(mr_quants_model, tournament_data, features)\\nnumerai_submission_id_mrQ = submit_predictions_to_numerai(mr_quants_file_with_predictions, mrquantsalot_model_id)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-53>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-33-2288aaddd594>\u001b[0m in \u001b[0;36msubmit_predictions_to_numerai\u001b[0;34m(filename_of_predictions, sumbit_model_id)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msubmit_predictions_to_numerai\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_of_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msumbit_model_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mnapi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen_api_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# open a connection to the numerai API with your creds.json file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0msubmission_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_of_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msumbit_model_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'You successfully submitted for {sumbit_model_id}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubmission_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numerapi/numerapi.py\u001b[0m in \u001b[0;36mupload_predictions\u001b[0;34m(self, file_path, tournament, model_id)\u001b[0m\n\u001b[1;32m    778\u001b[0m                      \u001b[0;34m'modelId'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m                      'triggerId': os.getenv('TRIGGER_ID', None)}\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mcreate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcreate_query\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthorization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m         \u001b[0msubmission_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'create_submission'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msubmission_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numerapi/base_api.py\u001b[0m in \u001b[0;36mraw_query\u001b[0;34m(self, query, variables, authorization)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_call_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'errors'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# fail!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Can't update submission after deadline if previous submission was on time"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3RY5VyX8C1w"
      },
      "source": [
        "## Run and submit TUT_MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "id": "Ygq0jiVE8Cg0",
        "outputId": "3a0fde3b-0c5a-4f2e-d83a-e105f8e3d8d1"
      },
      "source": [
        "tut_model_file_with_predictions = run_model_and_create_prediction_file(tut_model, tournament_data, features)\n",
        "numerai_submission_id_tut = submit_predictions_to_numerai(tut_model_file_with_predictions, tut_model_model_id)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-03-23 22:04:40,134 INFO numerapi.base_api: uploading predictions...\n",
            "2021-03-23 22:04:46,269 ERROR numerapi.base_api: Can't update submission after deadline if previous submission was on time\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-f2230ac62c97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtut_model_file_with_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_model_and_create_prediction_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtut_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtournament_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnumerai_submission_id_tut\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubmit_predictions_to_numerai\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtut_model_file_with_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtut_model_model_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-33-2288aaddd594>\u001b[0m in \u001b[0;36msubmit_predictions_to_numerai\u001b[0;34m(filename_of_predictions, sumbit_model_id)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msubmit_predictions_to_numerai\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_of_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msumbit_model_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mnapi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen_api_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# open a connection to the numerai API with your creds.json file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0msubmission_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_of_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msumbit_model_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'You successfully submitted for {sumbit_model_id}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubmission_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numerapi/numerapi.py\u001b[0m in \u001b[0;36mupload_predictions\u001b[0;34m(self, file_path, tournament, model_id)\u001b[0m\n\u001b[1;32m    778\u001b[0m                      \u001b[0;34m'modelId'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m                      'triggerId': os.getenv('TRIGGER_ID', None)}\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mcreate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcreate_query\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthorization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m         \u001b[0msubmission_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'create_submission'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msubmission_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numerapi/base_api.py\u001b[0m in \u001b[0;36mraw_query\u001b[0;34m(self, query, variables, authorization)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_call_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'errors'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# fail!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Can't update submission after deadline if previous submission was on time"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3aOmG--eQjt"
      },
      "source": [
        ""
      ]
    }
  ]
}