{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MainRegressor notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "rWvdPudwMiKI"
      ],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7atJJ2fMiJ8"
      },
      "source": [
        "# I started with this notebook and adapated a bunch of things \n",
        "\n",
        "Source: \n",
        "https://www.kaggle.com/code1110/numerai-tournament\n",
        "\n",
        "## This Notebook does the following things:\n",
        "1. Get the Training and tournment data from the numerai static links. \n",
        "2. Cast the data into Panadas DF and limit the memory use. \n",
        "3. Train a single light Gradient Boost Machine on the training data.\n",
        "4. Print diagonstics of the model on two subsets of the validation data.\n",
        "5. Write the predictions  to a file \n",
        "\n",
        "6. Submit the Predictions to numerai at the MRQUANTSALOT\n",
        "\n",
        "\n",
        "Known Problem \n",
        "\n",
        "Loa"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0iP-UgesMiKB",
        "outputId": "38488869-30cb-42d4-8a0a-2eae7d2ccf40"
      },
      "source": [
        "!pip install numerapi\n",
        "import numerapi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numerapi in /usr/local/lib/python3.7/dist-packages (2.4.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from numerapi) (2.23.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from numerapi) (7.1.2)\n",
            "Requirement already satisfied: tqdm>=4.29.1 in /usr/local/lib/python3.7/dist-packages (from numerapi) (4.41.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from numerapi) (2.8.1)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from numerapi) (2018.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->numerapi) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->numerapi) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->numerapi) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->numerapi) (3.0.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil->numerapi) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4WFeO-ZMiKC"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os, sys\n",
        "import gc\n",
        "import pathlib\n",
        "import json\n",
        "from typing import List, NoReturn, Union, Tuple, Optional, Text, Generic, Callable, Dict\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, QuantileTransformer\n",
        "from sklearn.model_selection import KFold, StratifiedKFold, TimeSeriesSplit\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, log_loss, mean_squared_error, mean_absolute_error, f1_score\n",
        "from scipy.stats import spearmanr # -P I think this is corr. \n",
        "import joblib\n",
        "\n",
        "# model\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "import operator\n",
        "\n",
        "# visualize\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.style as style\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot\n",
        "from matplotlib.ticker import ScalarFormatter\n",
        "sns.set_context(\"talk\")\n",
        "style.use('seaborn-colorblind')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b364c12-MiKD"
      },
      "source": [
        "Methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mm1stk1pMiKD"
      },
      "source": [
        "def cast_eras_as_int(x): # this is used to cast the eras from strings to ints\n",
        "    try:\n",
        "        return int(x[3:]) # the eras look like era####\n",
        "    except:\n",
        "        return 1000\n",
        "\n",
        "# unclear if numerapi.download_latest_data() would be faster\n",
        "def read_data(data='train'):\n",
        "    # get data \n",
        "    if data == 'train':\n",
        "        df = pd.read_csv('https://numerai-public-datasets.s3-us-west-2.amazonaws.com/latest_numerai_training_data.csv.xz')\n",
        "\n",
        "        # The test data is significantly larger.\n",
        "        # test data is the live tournment data\n",
        "    elif data == 'test':\n",
        "        df = pd.read_csv('https://numerai-public-datasets.s3-us-west-2.amazonaws.com/latest_numerai_tournament_data.csv.xz')\n",
        "        \n",
        "    \n",
        "    # features\n",
        "    feature_cols = df.columns[df.columns.str.startswith('feature')]\n",
        "    \n",
        "    # map to int, to reduce the memory demand\n",
        "    mapping = {0.0 : 0, 0.25 : 1, 0.5 : 2, 0.75 : 3, 1.0 : 4} # this is very clever -P\n",
        "    for c in feature_cols:\n",
        "        df[c] = df[c].map(mapping).astype(np.uint8)\n",
        "        \n",
        "    df[\"era\"] = df[\"era\"].apply(cast_eras_as_int)# also cast era to int\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "eyKw25FmMiKD",
        "outputId": "c24d4d7c-2fc7-490d-ba85-1562bd6a5eeb"
      },
      "source": [
        "%%time\n",
        "# load in the Training data\n",
        "train = read_data('train')\n",
        "print(train.shape)\n",
        "train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(501808, 314)\n",
            "CPU times: user 1min 28s, sys: 6.1 s, total: 1min 35s\n",
            "Wall time: 1min 36s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lOiCNwHGMiKE",
        "outputId": "8ef7bb3a-ac24-49f2-bfeb-f9d678171810"
      },
      "source": [
        "%%time\n",
        "# the testing data is the tournement data\n",
        "test = read_data('test')\n",
        "print(test.columns)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index(['id', 'era', 'data_type', 'feature_intelligence1',\n",
            "       'feature_intelligence2', 'feature_intelligence3',\n",
            "       'feature_intelligence4', 'feature_intelligence5',\n",
            "       'feature_intelligence6', 'feature_intelligence7',\n",
            "       ...\n",
            "       'feature_wisdom38', 'feature_wisdom39', 'feature_wisdom40',\n",
            "       'feature_wisdom41', 'feature_wisdom42', 'feature_wisdom43',\n",
            "       'feature_wisdom44', 'feature_wisdom45', 'feature_wisdom46', 'target'],\n",
            "      dtype='object', length=314)\n",
            "CPU times: user 4min 36s, sys: 16.2 s, total: 4min 52s\n",
            "Wall time: 4min 56s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "strAzqc-MiKE"
      },
      "source": [
        "Create a seperate valid split. \n",
        "Read more https://towardsdatascience.com/train-validation-and-test-sets-72cb40cba9e7\n",
        "Train: Model sees and learns this data\n",
        "\n",
        "Validation: Use this to see your score. This is what you use to tune hyperparameters.\n",
        "\n",
        "Valid is a subset of the test data. it is where the data_type is 'Validation'\n",
        "\n",
        "### I don't have good evidence for splits in this size. It might be better for some other splits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LD9w9dCYMiKF",
        "outputId": "bc0e3155-bbe2-4f4d-de1e-f3a64b7a5fdd"
      },
      "source": [
        "%%time\n",
        "# validation is derived from the live tournement data\n",
        "valid = test[test[\"data_type\"] == \"validation\"].reset_index(drop = True)\n",
        "\n",
        "print(valid.columns) # want ot see if there is a target value here\n",
        "# validation split\n",
        "valid.loc[valid[\"era\"] > 180, \"valid2\"] = True # Every era after 180 is in validation\n",
        "valid.loc[valid[\"era\"] <= 180, \"valid2\"] = False # Every era before is not in the validation set. \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index(['id', 'era', 'data_type', 'feature_intelligence1',\n",
            "       'feature_intelligence2', 'feature_intelligence3',\n",
            "       'feature_intelligence4', 'feature_intelligence5',\n",
            "       'feature_intelligence6', 'feature_intelligence7',\n",
            "       ...\n",
            "       'feature_wisdom38', 'feature_wisdom39', 'feature_wisdom40',\n",
            "       'feature_wisdom41', 'feature_wisdom42', 'feature_wisdom43',\n",
            "       'feature_wisdom44', 'feature_wisdom45', 'feature_wisdom46', 'target'],\n",
            "      dtype='object', length=314)\n",
            "CPU times: user 744 ms, sys: 12 ms, total: 756 ms\n",
            "Wall time: 755 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9l_BqycKMiKF",
        "outputId": "36a8e6a7-4722-4f24-9a7d-f044122770f7"
      },
      "source": [
        "# remove data_type to save memory\n",
        "train.drop(columns=[\"data_type\"], inplace=True)\n",
        "valid.drop(columns=[\"data_type\"], inplace=True)\n",
        "test.drop(columns=[\"data_type\"], inplace=True)\n",
        "\n",
        "print('The number of records: train {:,}, valid {:,}, test {:,}'.format(train.shape[0], valid.shape[0], test.shape[0])) # df.shape[0] is number of rows."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of records: train 501,808, valid 137,779, test 1,671,309\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFj9qac9MiKF"
      },
      "source": [
        "# EDA (Exploratory Data Analysis)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AiydLA2MiKG"
      },
      "source": [
        "## Determine features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "box2-PgOMiKG",
        "outputId": "ed20b006-a72f-47bd-dd90-b76dca546d37"
      },
      "source": [
        "# features\n",
        "features = [f for f in train.columns.values.tolist() if 'feature' in f] # fancy for loop to get all the feature names. -p\n",
        "print('There are {} features.'.format(len(features)))\n",
        "features"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 310 features.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['feature_intelligence1',\n",
              " 'feature_intelligence2',\n",
              " 'feature_intelligence3',\n",
              " 'feature_intelligence4',\n",
              " 'feature_intelligence5',\n",
              " 'feature_intelligence6',\n",
              " 'feature_intelligence7',\n",
              " 'feature_intelligence8',\n",
              " 'feature_intelligence9',\n",
              " 'feature_intelligence10',\n",
              " 'feature_intelligence11',\n",
              " 'feature_intelligence12',\n",
              " 'feature_charisma1',\n",
              " 'feature_charisma2',\n",
              " 'feature_charisma3',\n",
              " 'feature_charisma4',\n",
              " 'feature_charisma5',\n",
              " 'feature_charisma6',\n",
              " 'feature_charisma7',\n",
              " 'feature_charisma8',\n",
              " 'feature_charisma9',\n",
              " 'feature_charisma10',\n",
              " 'feature_charisma11',\n",
              " 'feature_charisma12',\n",
              " 'feature_charisma13',\n",
              " 'feature_charisma14',\n",
              " 'feature_charisma15',\n",
              " 'feature_charisma16',\n",
              " 'feature_charisma17',\n",
              " 'feature_charisma18',\n",
              " 'feature_charisma19',\n",
              " 'feature_charisma20',\n",
              " 'feature_charisma21',\n",
              " 'feature_charisma22',\n",
              " 'feature_charisma23',\n",
              " 'feature_charisma24',\n",
              " 'feature_charisma25',\n",
              " 'feature_charisma26',\n",
              " 'feature_charisma27',\n",
              " 'feature_charisma28',\n",
              " 'feature_charisma29',\n",
              " 'feature_charisma30',\n",
              " 'feature_charisma31',\n",
              " 'feature_charisma32',\n",
              " 'feature_charisma33',\n",
              " 'feature_charisma34',\n",
              " 'feature_charisma35',\n",
              " 'feature_charisma36',\n",
              " 'feature_charisma37',\n",
              " 'feature_charisma38',\n",
              " 'feature_charisma39',\n",
              " 'feature_charisma40',\n",
              " 'feature_charisma41',\n",
              " 'feature_charisma42',\n",
              " 'feature_charisma43',\n",
              " 'feature_charisma44',\n",
              " 'feature_charisma45',\n",
              " 'feature_charisma46',\n",
              " 'feature_charisma47',\n",
              " 'feature_charisma48',\n",
              " 'feature_charisma49',\n",
              " 'feature_charisma50',\n",
              " 'feature_charisma51',\n",
              " 'feature_charisma52',\n",
              " 'feature_charisma53',\n",
              " 'feature_charisma54',\n",
              " 'feature_charisma55',\n",
              " 'feature_charisma56',\n",
              " 'feature_charisma57',\n",
              " 'feature_charisma58',\n",
              " 'feature_charisma59',\n",
              " 'feature_charisma60',\n",
              " 'feature_charisma61',\n",
              " 'feature_charisma62',\n",
              " 'feature_charisma63',\n",
              " 'feature_charisma64',\n",
              " 'feature_charisma65',\n",
              " 'feature_charisma66',\n",
              " 'feature_charisma67',\n",
              " 'feature_charisma68',\n",
              " 'feature_charisma69',\n",
              " 'feature_charisma70',\n",
              " 'feature_charisma71',\n",
              " 'feature_charisma72',\n",
              " 'feature_charisma73',\n",
              " 'feature_charisma74',\n",
              " 'feature_charisma75',\n",
              " 'feature_charisma76',\n",
              " 'feature_charisma77',\n",
              " 'feature_charisma78',\n",
              " 'feature_charisma79',\n",
              " 'feature_charisma80',\n",
              " 'feature_charisma81',\n",
              " 'feature_charisma82',\n",
              " 'feature_charisma83',\n",
              " 'feature_charisma84',\n",
              " 'feature_charisma85',\n",
              " 'feature_charisma86',\n",
              " 'feature_strength1',\n",
              " 'feature_strength2',\n",
              " 'feature_strength3',\n",
              " 'feature_strength4',\n",
              " 'feature_strength5',\n",
              " 'feature_strength6',\n",
              " 'feature_strength7',\n",
              " 'feature_strength8',\n",
              " 'feature_strength9',\n",
              " 'feature_strength10',\n",
              " 'feature_strength11',\n",
              " 'feature_strength12',\n",
              " 'feature_strength13',\n",
              " 'feature_strength14',\n",
              " 'feature_strength15',\n",
              " 'feature_strength16',\n",
              " 'feature_strength17',\n",
              " 'feature_strength18',\n",
              " 'feature_strength19',\n",
              " 'feature_strength20',\n",
              " 'feature_strength21',\n",
              " 'feature_strength22',\n",
              " 'feature_strength23',\n",
              " 'feature_strength24',\n",
              " 'feature_strength25',\n",
              " 'feature_strength26',\n",
              " 'feature_strength27',\n",
              " 'feature_strength28',\n",
              " 'feature_strength29',\n",
              " 'feature_strength30',\n",
              " 'feature_strength31',\n",
              " 'feature_strength32',\n",
              " 'feature_strength33',\n",
              " 'feature_strength34',\n",
              " 'feature_strength35',\n",
              " 'feature_strength36',\n",
              " 'feature_strength37',\n",
              " 'feature_strength38',\n",
              " 'feature_dexterity1',\n",
              " 'feature_dexterity2',\n",
              " 'feature_dexterity3',\n",
              " 'feature_dexterity4',\n",
              " 'feature_dexterity5',\n",
              " 'feature_dexterity6',\n",
              " 'feature_dexterity7',\n",
              " 'feature_dexterity8',\n",
              " 'feature_dexterity9',\n",
              " 'feature_dexterity10',\n",
              " 'feature_dexterity11',\n",
              " 'feature_dexterity12',\n",
              " 'feature_dexterity13',\n",
              " 'feature_dexterity14',\n",
              " 'feature_constitution1',\n",
              " 'feature_constitution2',\n",
              " 'feature_constitution3',\n",
              " 'feature_constitution4',\n",
              " 'feature_constitution5',\n",
              " 'feature_constitution6',\n",
              " 'feature_constitution7',\n",
              " 'feature_constitution8',\n",
              " 'feature_constitution9',\n",
              " 'feature_constitution10',\n",
              " 'feature_constitution11',\n",
              " 'feature_constitution12',\n",
              " 'feature_constitution13',\n",
              " 'feature_constitution14',\n",
              " 'feature_constitution15',\n",
              " 'feature_constitution16',\n",
              " 'feature_constitution17',\n",
              " 'feature_constitution18',\n",
              " 'feature_constitution19',\n",
              " 'feature_constitution20',\n",
              " 'feature_constitution21',\n",
              " 'feature_constitution22',\n",
              " 'feature_constitution23',\n",
              " 'feature_constitution24',\n",
              " 'feature_constitution25',\n",
              " 'feature_constitution26',\n",
              " 'feature_constitution27',\n",
              " 'feature_constitution28',\n",
              " 'feature_constitution29',\n",
              " 'feature_constitution30',\n",
              " 'feature_constitution31',\n",
              " 'feature_constitution32',\n",
              " 'feature_constitution33',\n",
              " 'feature_constitution34',\n",
              " 'feature_constitution35',\n",
              " 'feature_constitution36',\n",
              " 'feature_constitution37',\n",
              " 'feature_constitution38',\n",
              " 'feature_constitution39',\n",
              " 'feature_constitution40',\n",
              " 'feature_constitution41',\n",
              " 'feature_constitution42',\n",
              " 'feature_constitution43',\n",
              " 'feature_constitution44',\n",
              " 'feature_constitution45',\n",
              " 'feature_constitution46',\n",
              " 'feature_constitution47',\n",
              " 'feature_constitution48',\n",
              " 'feature_constitution49',\n",
              " 'feature_constitution50',\n",
              " 'feature_constitution51',\n",
              " 'feature_constitution52',\n",
              " 'feature_constitution53',\n",
              " 'feature_constitution54',\n",
              " 'feature_constitution55',\n",
              " 'feature_constitution56',\n",
              " 'feature_constitution57',\n",
              " 'feature_constitution58',\n",
              " 'feature_constitution59',\n",
              " 'feature_constitution60',\n",
              " 'feature_constitution61',\n",
              " 'feature_constitution62',\n",
              " 'feature_constitution63',\n",
              " 'feature_constitution64',\n",
              " 'feature_constitution65',\n",
              " 'feature_constitution66',\n",
              " 'feature_constitution67',\n",
              " 'feature_constitution68',\n",
              " 'feature_constitution69',\n",
              " 'feature_constitution70',\n",
              " 'feature_constitution71',\n",
              " 'feature_constitution72',\n",
              " 'feature_constitution73',\n",
              " 'feature_constitution74',\n",
              " 'feature_constitution75',\n",
              " 'feature_constitution76',\n",
              " 'feature_constitution77',\n",
              " 'feature_constitution78',\n",
              " 'feature_constitution79',\n",
              " 'feature_constitution80',\n",
              " 'feature_constitution81',\n",
              " 'feature_constitution82',\n",
              " 'feature_constitution83',\n",
              " 'feature_constitution84',\n",
              " 'feature_constitution85',\n",
              " 'feature_constitution86',\n",
              " 'feature_constitution87',\n",
              " 'feature_constitution88',\n",
              " 'feature_constitution89',\n",
              " 'feature_constitution90',\n",
              " 'feature_constitution91',\n",
              " 'feature_constitution92',\n",
              " 'feature_constitution93',\n",
              " 'feature_constitution94',\n",
              " 'feature_constitution95',\n",
              " 'feature_constitution96',\n",
              " 'feature_constitution97',\n",
              " 'feature_constitution98',\n",
              " 'feature_constitution99',\n",
              " 'feature_constitution100',\n",
              " 'feature_constitution101',\n",
              " 'feature_constitution102',\n",
              " 'feature_constitution103',\n",
              " 'feature_constitution104',\n",
              " 'feature_constitution105',\n",
              " 'feature_constitution106',\n",
              " 'feature_constitution107',\n",
              " 'feature_constitution108',\n",
              " 'feature_constitution109',\n",
              " 'feature_constitution110',\n",
              " 'feature_constitution111',\n",
              " 'feature_constitution112',\n",
              " 'feature_constitution113',\n",
              " 'feature_constitution114',\n",
              " 'feature_wisdom1',\n",
              " 'feature_wisdom2',\n",
              " 'feature_wisdom3',\n",
              " 'feature_wisdom4',\n",
              " 'feature_wisdom5',\n",
              " 'feature_wisdom6',\n",
              " 'feature_wisdom7',\n",
              " 'feature_wisdom8',\n",
              " 'feature_wisdom9',\n",
              " 'feature_wisdom10',\n",
              " 'feature_wisdom11',\n",
              " 'feature_wisdom12',\n",
              " 'feature_wisdom13',\n",
              " 'feature_wisdom14',\n",
              " 'feature_wisdom15',\n",
              " 'feature_wisdom16',\n",
              " 'feature_wisdom17',\n",
              " 'feature_wisdom18',\n",
              " 'feature_wisdom19',\n",
              " 'feature_wisdom20',\n",
              " 'feature_wisdom21',\n",
              " 'feature_wisdom22',\n",
              " 'feature_wisdom23',\n",
              " 'feature_wisdom24',\n",
              " 'feature_wisdom25',\n",
              " 'feature_wisdom26',\n",
              " 'feature_wisdom27',\n",
              " 'feature_wisdom28',\n",
              " 'feature_wisdom29',\n",
              " 'feature_wisdom30',\n",
              " 'feature_wisdom31',\n",
              " 'feature_wisdom32',\n",
              " 'feature_wisdom33',\n",
              " 'feature_wisdom34',\n",
              " 'feature_wisdom35',\n",
              " 'feature_wisdom36',\n",
              " 'feature_wisdom37',\n",
              " 'feature_wisdom38',\n",
              " 'feature_wisdom39',\n",
              " 'feature_wisdom40',\n",
              " 'feature_wisdom41',\n",
              " 'feature_wisdom42',\n",
              " 'feature_wisdom43',\n",
              " 'feature_wisdom44',\n",
              " 'feature_wisdom45',\n",
              " 'feature_wisdom46']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQYEv6mBMiKG"
      },
      "source": [
        "## Target"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PlZuKRcVMiKH",
        "outputId": "1d20c33f-4e2f-411b-c1bf-a0617636642e"
      },
      "source": [
        "target = train.columns[train.columns.str.startswith('target')].values.tolist()[0] # I dont know what this line does\n",
        "# this just the string 'target'\n",
        "print(target)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "target\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCki_3ozMiKH"
      },
      "source": [
        "# Modeling\n",
        "The example script of xgboost made by numerai [Example](https://github.com/numerai/example-scripts/blob/master/example_model.py)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xg5NIX_pMiKH"
      },
      "source": [
        "Link to hyperparameters of xgboost \n",
        "https://xgboost.readthedocs.io/en/latest/parameter.html\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8JnX3Se_MiKH"
      },
      "source": [
        "# # create a model and fit (公式example)\n",
        "\n",
        "# untested. -p\n",
        "# model = xgb.XGBRegressor(max_depth=5, learning_rate=0.01, n_estimators=2000, n_jobs=-1, colsample_bytree=0.1)\n",
        "# model.fit(train[features], train[target])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Mujvbb65MiKI",
        "outputId": "9f7f21e5-da98-4c63-9dc3-25ff2c15d69f"
      },
      "source": [
        "%%time\n",
        "\n",
        "# create a model based on these params.\n",
        "# for all intents and purposes these params are random.\n",
        "# once this set is automated, I would need to verify what parmas are best. \n",
        "# Figure out how to do this on Google Cloud\n",
        "params1 = {\n",
        "            'n_estimators': 5000,\n",
        "            'objective': 'regression',\n",
        "            'boosting_type': 'gbdt',\n",
        "            'max_depth': 55,\n",
        "            'learning_rate': 0.013, \n",
        "            'feature_fraction': 0.095,\n",
        "            'seed': 52\n",
        "            }\n",
        "model = lgb.LGBMRegressor(**params1) # now increasing the model n_estimators and sumbitting that under tutmodel\n",
        "model.fit(train[features], train[target]) # usally takes ~9 minutes to run."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 9min 15s, sys: 1.33 s, total: 9min 16s\n",
            "Wall time: 2min 21s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InXN9nCfMiKI"
      },
      "source": [
        "# Feature importance\n",
        "\n",
        "You can see what features are important and the distribution of weights assigned to each of the features.\n",
        "You might use this as part of the meta model of light xbgoost \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "C3ZzS4dXMiKI",
        "outputId": "79c24c9b-ae23-4b76-ef9b-9665a433e1da"
      },
      "source": [
        "feature_importance =pd.DataFrame(model.feature_importances_, index=features, columns=['importance'])\n",
        "feature_importance.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>importance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>310.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>483.870968</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>56.475009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>314.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>450.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>484.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>521.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>640.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       importance\n",
              "count  310.000000\n",
              "mean   483.870968\n",
              "std     56.475009\n",
              "min    314.000000\n",
              "25%    450.000000\n",
              "50%    484.000000\n",
              "75%    521.000000\n",
              "max    640.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSL-9D4yMiKI"
      },
      "source": [
        "It might make sense to group your models into clusters based on this stat. \n",
        "You would want to scale them down to 1 when you group them. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWvdPudwMiKI"
      },
      "source": [
        "# Validation Score\n",
        "These are the methods to evaluate your model.\n",
        "I did not write these models, many of the comments written I wrote though.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "szXbJM0mMiKJ"
      },
      "source": [
        "# naming conventions\n",
        "PREDICTION_NAME = 'prediction'\n",
        "TARGET_NAME = target # 'target is the string named 'target'\n",
        "# EXAMPLE_PRED = 'example_prediction'\n",
        "\n",
        "# ---------------------------\n",
        "# Functions\n",
        "# ---------------------------\n",
        "def valid4score(valid : pd.DataFrame, pred : np.ndarray, load_example: bool=True, save : bool=False) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Generate new valid pandas dataframe for computing scores\n",
        "    \n",
        "    :INPUT:\n",
        "    - valid : pd.DataFrame extracted from tournament data (data_type='validation')\n",
        "    \n",
        "    \"\"\"\n",
        "    valid_df = valid.copy() # the validation dataframe you use this to test the CORR and other values\n",
        "\n",
        "    # Your model creates an array of floats [0,1] rank method converst them in a list of ints. \n",
        "\n",
        "    # your lis tof ints is then compared to their list of ints. \n",
        "    valid_df['prediction'] = pd.Series(pred).rank(pct=True, method=\"first\") # pred is the array of predictions your model creates for the set of validation vectors.  \n",
        "    # I am unsure if this preds is a float only only between 0,1,2,3,4. \n",
        "    valid_df.rename(columns={target: 'target'}, inplace=True)\n",
        "    \n",
        "    # I don't know what the load example boolean is. I think you can use this to save predictions.\n",
        "    if load_example:\n",
        "        valid_df[EXAMPLE_PRED] = pd.read_csv(EXP_DIR + 'valid_df.csv')['prediction'].values\n",
        "    \n",
        "    if save==True:\n",
        "        valid_df.to_csv(OUTPUT_DIR + 'valid_df.csv', index=False)\n",
        "        print('Validation dataframe saved!')\n",
        "    \n",
        "    return valid_df\n",
        "\n",
        "def compute_corr(valid_df : pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Compute rank correlation\n",
        "\n",
        "    THIS IS WHAT YOU ARE PRIMARILY PAID ON \n",
        "    \n",
        "    :INPUT:\n",
        "    - valid_df : pd.DataFrame where at least 2 columns ('prediction' & 'target') exist\n",
        "    \n",
        "    \"\"\"\n",
        "    # this uses Person Correilation. \n",
        "    # I You are paid on spearman corrilation. That is where the ratio of change is important not the raw amount of change\n",
        "    # see: https://support.minitab.com/en-us/minitab-express/1/help-and-how-to/modeling-statistics/regression/supporting-topics/basics/a-comparison-of-the-pearson-and-spearman-correlation-methods/\n",
        "    return np.corrcoef(valid_df[\"target\"], valid_df['prediction'])[0, 1]\n",
        "\n",
        "def compute_max_drawdown(validation_correlations : pd.Series):\n",
        "    \"\"\"\n",
        "    Compute max drawdown\n",
        "    \n",
        "    :INPUT:\n",
        "    - validation_correaltions : pd.Series\n",
        "    \"\"\"\n",
        "    \n",
        "    rolling_max = (validation_correlations + 1).cumprod().rolling(window=100, min_periods=1).max()\n",
        "    daily_value = (validation_correlations + 1).cumprod()\n",
        "    max_drawdown = -(rolling_max - daily_value).max()\n",
        "    \n",
        "    return max_drawdown\n",
        "\n",
        "def compute_val_corr(valid_df : pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Compute rank correlation for valid periods\n",
        "    \n",
        "    :INPUT:\n",
        "    - valid_df : pd.DataFrame where at least 2 columns ('prediction' & 'target') exist\n",
        "    \"\"\"\n",
        "    \n",
        "    # all validation\n",
        "    correlation = compute_corr(valid_df)\n",
        "    print(\"rank corr = {:.4f}\".format(correlation))\n",
        "    return correlation\n",
        "    \n",
        "def compute_val_sharpe(valid_df : pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Compute sharpe ratio for valid periods\n",
        "    \n",
        "    :INPUT:\n",
        "    - valid_df : pd.DataFrame where at least 2 columns ('prediction' & 'target') exist\n",
        "    \"\"\"\n",
        "    # all validation\n",
        "    d = valid_df.groupby('era')[['target', 'prediction']].corr().iloc[0::2,-1].reset_index()\n",
        "    me = d['prediction'].mean()\n",
        "    sd = d['prediction'].std()\n",
        "    max_drawdown = compute_max_drawdown(d['prediction'])\n",
        "    print('sharpe ratio = {:.4f}, corr mean = {:.4f}, corr std = {:.4f}, max drawdown = {:.4f}'.format(me / sd, me, sd, max_drawdown))\n",
        "    \n",
        "    return me / sd, me, sd, max_drawdown\n",
        "    \n",
        "def feature_exposures(valid_df : pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Compute feature exposure\n",
        "    \n",
        "    :INPUT:\n",
        "    - valid_df : pd.DataFrame where at least 2 columns ('prediction' & 'target') exist\n",
        "    \"\"\"\n",
        "    feature_names = [f for f in valid_df.columns\n",
        "                     if f.startswith(\"feature\")]\n",
        "    exposures = []\n",
        "    for f in feature_names:\n",
        "        fe = spearmanr(valid_df['prediction'], valid_df[f])[0]\n",
        "        exposures.append(fe)\n",
        "    return np.array(exposures)\n",
        "\n",
        "def max_feature_exposure(fe : np.ndarray):\n",
        "    return np.max(np.abs(fe))\n",
        "\n",
        "def feature_exposure(fe : np.ndarray):\n",
        "    return np.sqrt(np.mean(np.square(fe)))\n",
        "\n",
        "def compute_val_feature_exposure(valid_df : pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Compute feature exposure for valid periods\n",
        "    \n",
        "    :INPUT:\n",
        "    - valid_df : pd.DataFrame where at least 2 columns ('prediction' & 'target') exist\n",
        "    \"\"\"\n",
        "    # all validation\n",
        "    fe = feature_exposures(valid_df)\n",
        "    fe1, fe2 = feature_exposure(fe), max_feature_exposure(fe)\n",
        "    print('feature exposure = {:.4f}, max feature exposure = {:.4f}'.format(fe1, fe2))\n",
        "     \n",
        "    return fe1, fe2\n",
        "\n",
        "# to neutralize a column in a df by many other columns\n",
        "#         I have no idea what this method does. -P. need to read about it and write up a link to it. \n",
        "def neutralize(df, columns, by, proportion=1.0):\n",
        "    scores = df.loc[:, columns]\n",
        "    exposures = df[by].values\n",
        "\n",
        "    # constant column to make sure the series is completely neutral to exposures\n",
        "    exposures = np.hstack(\n",
        "        (exposures,\n",
        "         np.asarray(np.mean(scores)) * np.ones(len(exposures)).reshape(-1, 1)))\n",
        "\n",
        "    scores = scores - proportion * exposures.dot(\n",
        "        np.linalg.pinv(exposures).dot(scores))\n",
        "    return scores / scores.std()\n",
        "\n",
        "\n",
        "# to neutralize any series by any other series\n",
        "def neutralize_series(series, by, proportion=1.0):\n",
        "    scores = series.values.reshape(-1, 1)\n",
        "    exposures = by.values.reshape(-1, 1)\n",
        "\n",
        "    # this line makes series neutral to a constant column so that it's centered and for sure gets corr 0 with exposures\n",
        "    exposures = np.hstack(\n",
        "        (exposures,\n",
        "         np.array([np.mean(series)] * len(exposures)).reshape(-1, 1)))\n",
        "\n",
        "    correction = proportion * (exposures.dot(\n",
        "        np.linalg.lstsq(exposures, scores, rcond=None)[0]))\n",
        "    corrected_scores = scores - correction\n",
        "    neutralized = pd.Series(corrected_scores.ravel(), index=series.index)\n",
        "    return neutralized\n",
        "\n",
        "\n",
        "def unif(df):\n",
        "    x = (df.rank(method=\"first\") - 0.5) / len(df)\n",
        "    return pd.Series(x, index=df.index)\n",
        "\n",
        "def get_feature_neutral_mean(df):\n",
        "    feature_cols = [c for c in df.columns if c.startswith(\"feature\")]\n",
        "    df.loc[:, \"neutral_sub\"] = neutralize(df, [PREDICTION_NAME],\n",
        "                                          feature_cols)[PREDICTION_NAME]\n",
        "    scores = df.groupby(\"era\").apply(\n",
        "        lambda x: np.corrcoef(x[\"neutral_sub\"].rank(pct=True, method=\"first\"), x[TARGET_NAME])).mean()\n",
        "    return np.mean(scores)\n",
        "\n",
        "def compute_val_mmc(valid_df : pd.DataFrame):    \n",
        "    # MMC over validation\n",
        "    mmc_scores = []\n",
        "    corr_scores = []\n",
        "    for _, x in valid_df.groupby(\"era\"):\n",
        "        series = neutralize_series(pd.Series(unif(x[PREDICTION_NAME])),\n",
        "                                   pd.Series(unif(x[EXAMPLE_PRED])))\n",
        "        mmc_scores.append(np.cov(series, x[TARGET_NAME])[0, 1] / (0.29 ** 2))\n",
        "        corr_scores.append(np.corrcoef(unif(x[PREDICTION_NAME]).rank(pct=True, method=\"first\"), x[TARGET_NAME]))\n",
        "\n",
        "    val_mmc_mean = np.mean(mmc_scores)\n",
        "    val_mmc_std = np.std(mmc_scores)\n",
        "    val_mmc_sharpe = val_mmc_mean / val_mmc_std\n",
        "    corr_plus_mmcs = [c + m for c, m in zip(corr_scores, mmc_scores)]\n",
        "    corr_plus_mmc_sharpe = np.mean(corr_plus_mmcs) / np.std(corr_plus_mmcs)\n",
        "    corr_plus_mmc_mean = np.mean(corr_plus_mmcs)\n",
        "\n",
        "    print(\"MMC Mean = {:.6f}, MMC Std = {:.6f}, CORR+MMC Sharpe = {:.4f}\".format(val_mmc_mean, val_mmc_std, corr_plus_mmc_sharpe))\n",
        "\n",
        "    # Check correlation with example predictions\n",
        "    corr_with_example_preds = np.corrcoef(valid_df[EXAMPLE_PRED].rank(pct=True, method=\"first\"),\n",
        "                                          valid_df[PREDICTION_NAME].rank(pct=True, method=\"first\"))[0, 1]\n",
        "    print(\"Corr with example preds: {:.4f}\".format(corr_with_example_preds))\n",
        "    \n",
        "    return val_mmc_mean, val_mmc_std, corr_plus_mmc_sharpe, corr_with_example_preds\n",
        "\n",
        "\n",
        "# this is the main method. The rest are just called interanlly. \n",
        "def score_summary(valid_df : pd.DataFrame):\n",
        "    score_df = {}\n",
        "    \n",
        "    try:\n",
        "        score_df['correlation'] = compute_val_corr(valid_df)\n",
        "    except:\n",
        "        print('ERR: computing correlation')\n",
        "    try:\n",
        "        score_df['corr_sharpe'], score_df['corr_mean'], score_df['corr_std'], score_df['max_drawdown'] = compute_val_sharpe(valid_df)\n",
        "    except:\n",
        "        print('ERR: computing sharpe')\n",
        "    try:\n",
        "        score_df['feature_exposure'], score_df['max_feature_exposure'] = compute_val_feature_exposure(valid_df)\n",
        "    except:\n",
        "        print('ERR: computing feature exposure')\n",
        "    try:\n",
        "        score_df['mmc_mean'], score_df['mmc_std'], score_df['corr_mmc_sharpe'], score_df['corr_with_example_xgb'] = compute_val_mmc(valid_df)\n",
        "    except:\n",
        "        print('ERR: computing MMC')\n",
        "    \n",
        "    return pd.DataFrame.from_dict(score_df, orient='index')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VBmC2Gx8MiKM",
        "outputId": "a395fd14-f5db-474a-de55-3d3bd20b8506"
      },
      "source": [
        "# prediction for valid periods   \n",
        "# peek at the number of prediction\n",
        "pred = model.predict(valid[features])\n",
        "print(f'You made {len(pred)} total predictions')\n",
        "print(type(pred))\n",
        "print(pred[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "You made 137779 total predictions\n",
            "<class 'numpy.ndarray'>\n",
            "0.486884289595734\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "RfFcD_UlMiKQ",
        "outputId": "d405e336-3755-4d66-bb7b-3c8449cc1195"
      },
      "source": [
        "# scores\n",
        "\n",
        "\n",
        "valid_df = valid4score(valid, pred, load_example=False, save=False)\n",
        "\n",
        "score_df = pd.DataFrame()\n",
        "print('------------------')\n",
        "print('ALL:')\n",
        "print('------------------')\n",
        "all_ = score_summary(valid_df).rename(columns={0: 'all'})\n",
        "\n",
        "print('------------------')\n",
        "print('VALID 1:')\n",
        "print('------------------')\n",
        "val1_ = score_summary(valid_df.query('era < 150')).rename(columns={0: 'val1'}) \n",
        "# there might be something strange with these numbers. before they are 180 need to verify unsure what valid2 = True means.\n",
        "\n",
        "print('------------------')\n",
        "print('VALID 2:')\n",
        "print('------------------')\n",
        "val2_ = score_summary(valid_df.query('era > 150')).rename(columns={0: 'val2'})"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------\n",
            "ALL:\n",
            "------------------\n",
            "rank corr = 0.0238\n",
            "sharpe ratio = 0.8472, corr mean = 0.0243, corr std = 0.0286, max drawdown = -0.0723\n",
            "feature exposure = 0.0825, max feature exposure = 0.3018\n",
            "ERR: computing MMC\n",
            "------------------\n",
            "VALID 1:\n",
            "------------------\n",
            "rank corr = 0.0351\n",
            "sharpe ratio = 1.2671, corr mean = 0.0351, corr std = 0.0277, max drawdown = -0.0290\n",
            "feature exposure = 0.0741, max feature exposure = 0.2894\n",
            "ERR: computing MMC\n",
            "------------------\n",
            "VALID 2:\n",
            "------------------\n",
            "rank corr = 0.0160\n",
            "sharpe ratio = 0.5896, corr mean = 0.0162, corr std = 0.0274, max drawdown = -0.0480\n",
            "feature exposure = 0.0942, max feature exposure = 0.3105\n",
            "ERR: computing MMC\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Udag3n7Ig5Zs"
      },
      "source": [
        "# See Scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OeodHr9WMiKQ",
        "outputId": "57ce2b1e-efd3-444e-90d3-36344e70400d"
      },
      "source": [
        "# scores\n",
        "score_df = pd.concat([all_, val1_, val2_], axis=1)\n",
        "score_df.style.background_gradient(cmap='viridis', axis=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<style  type=\"text/css\" >\n",
              "#T_4a1cdd30_8901_11eb_a4cf_0242ac1c0002row0_col0,#T_4a1cdd30_8901_11eb_a4cf_0242ac1c0002row2_col0{\n",
              "            background-color:  #482576;\n",
              "            color:  #f1f1f1;\n",
              "        }#T_4a1cdd30_8901_11eb_a4cf_0242ac1c0002row0_col1,#T_4a1cdd30_8901_11eb_a4cf_0242ac1c0002row2_col1{\n",
              "            background-color:  #471365;\n",
              "            color:  #f1f1f1;\n",
              "        }#T_4a1cdd30_8901_11eb_a4cf_0242ac1c0002row0_col2,#T_4a1cdd30_8901_11eb_a4cf_0242ac1c0002row2_col2{\n",
              "            background-color:  #482475;\n",
              "            color:  #f1f1f1;\n",
              "        }#T_4a1cdd30_8901_11eb_a4cf_0242ac1c0002row1_col0,#T_4a1cdd30_8901_11eb_a4cf_0242ac1c0002row1_col1,#T_4a1cdd30_8901_11eb_a4cf_0242ac1c0002row1_col2{\n",
              "            background-color:  #fde725;\n",
              "            color:  #000000;\n",
              "        }#T_4a1cdd30_8901_11eb_a4cf_0242ac1c0002row3_col0{\n",
              "            background-color:  #482878;\n",
              "            color:  #f1f1f1;\n",
              "        }#T_4a1cdd30_8901_11eb_a4cf_0242ac1c0002row3_col1{\n",
              "            background-color:  #471164;\n",
              "            color:  #f1f1f1;\n",
              "        }#T_4a1cdd30_8901_11eb_a4cf_0242ac1c0002row3_col2{\n",
              "            background-color:  #472a7a;\n",
              "            color:  #f1f1f1;\n",
              "        }#T_4a1cdd30_8901_11eb_a4cf_0242ac1c0002row4_col0,#T_4a1cdd30_8901_11eb_a4cf_0242ac1c0002row4_col1,#T_4a1cdd30_8901_11eb_a4cf_0242ac1c0002row4_col2{\n",
              "            background-color:  #440154;\n",
              "            color:  #f1f1f1;\n",
              "        }#T_4a1cdd30_8901_11eb_a4cf_0242ac1c0002row5_col0{\n",
              "            background-color:  #443a83;\n",
              "            color:  #f1f1f1;\n",
              "        }#T_4a1cdd30_8901_11eb_a4cf_0242ac1c0002row5_col1{\n",
              "            background-color:  #481d6f;\n",
              "            color:  #f1f1f1;\n",
              "        }#T_4a1cdd30_8901_11eb_a4cf_0242ac1c0002row5_col2{\n",
              "            background-color:  #3e4a89;\n",
              "            color:  #f1f1f1;\n",
              "        }#T_4a1cdd30_8901_11eb_a4cf_0242ac1c0002row6_col0{\n",
              "            background-color:  #297a8e;\n",
              "            color:  #000000;\n",
              "        }#T_4a1cdd30_8901_11eb_a4cf_0242ac1c0002row6_col1{\n",
              "            background-color:  #3c508b;\n",
              "            color:  #f1f1f1;\n",
              "        }#T_4a1cdd30_8901_11eb_a4cf_0242ac1c0002row6_col2{\n",
              "            background-color:  #1f9f88;\n",
              "            color:  #000000;\n",
              "        }</style><table id=\"T_4a1cdd30_8901_11eb_a4cf_0242ac1c0002\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >all</th>        <th class=\"col_heading level0 col1\" >val1</th>        <th class=\"col_heading level0 col2\" >val2</th>    </tr></thead><tbody>\n",
              "                <tr>\n",
              "                        <th id=\"T_4a1cdd30_8901_11eb_a4cf_0242ac1c0002level0_row0\" class=\"row_heading level0 row0\" >correlation</th>\n",
              "                        <td id=\"T_4a1cdd30_8901_11eb_a4cf_0242ac1c0002row0_col0\" class=\"data row0 col0\" >0.023830</td>\n",
              "                        <td id=\"T_4a1cdd30_8901_11eb_a4cf_0242ac1c0002row0_col1\" class=\"data row0 col1\" >0.035123</td>\n",
              "                        <td id=\"T_4a1cdd30_8901_11eb_a4cf_0242ac1c0002row0_col2\" class=\"data row0 col2\" >0.016025</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_4a1cdd30_8901_11eb_a4cf_0242ac1c0002level0_row1\" class=\"row_heading level0 row1\" >corr_sharpe</th>\n",
              "                        <td id=\"T_4a1cdd30_8901_11eb_a4cf_0242ac1c0002row1_col0\" class=\"data row1 col0\" >0.847194</td>\n",
              "                        <td id=\"T_4a1cdd30_8901_11eb_a4cf_0242ac1c0002row1_col1\" class=\"data row1 col1\" >1.267120</td>\n",
              "                        <td id=\"T_4a1cdd30_8901_11eb_a4cf_0242ac1c0002row1_col2\" class=\"data row1 col2\" >0.589563</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_4a1cdd30_8901_11eb_a4cf_0242ac1c0002level0_row2\" class=\"row_heading level0 row2\" >corr_mean</th>\n",
              "                        <td id=\"T_4a1cdd30_8901_11eb_a4cf_0242ac1c0002row2_col0\" class=\"data row2 col0\" >0.024271</td>\n",
              "                        <td id=\"T_4a1cdd30_8901_11eb_a4cf_0242ac1c0002row2_col1\" class=\"data row2 col1\" >0.035079</td>\n",
              "                        <td id=\"T_4a1cdd30_8901_11eb_a4cf_0242ac1c0002row2_col2\" class=\"data row2 col2\" >0.016164</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_4a1cdd30_8901_11eb_a4cf_0242ac1c0002level0_row3\" class=\"row_heading level0 row3\" >corr_std</th>\n",
              "                        <td id=\"T_4a1cdd30_8901_11eb_a4cf_0242ac1c0002row3_col0\" class=\"data row3 col0\" >0.028648</td>\n",
              "                        <td id=\"T_4a1cdd30_8901_11eb_a4cf_0242ac1c0002row3_col1\" class=\"data row3 col1\" >0.027684</td>\n",
              "                        <td id=\"T_4a1cdd30_8901_11eb_a4cf_0242ac1c0002row3_col2\" class=\"data row3 col2\" >0.027417</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_4a1cdd30_8901_11eb_a4cf_0242ac1c0002level0_row4\" class=\"row_heading level0 row4\" >max_drawdown</th>\n",
              "                        <td id=\"T_4a1cdd30_8901_11eb_a4cf_0242ac1c0002row4_col0\" class=\"data row4 col0\" >-0.072331</td>\n",
              "                        <td id=\"T_4a1cdd30_8901_11eb_a4cf_0242ac1c0002row4_col1\" class=\"data row4 col1\" >-0.028963</td>\n",
              "                        <td id=\"T_4a1cdd30_8901_11eb_a4cf_0242ac1c0002row4_col2\" class=\"data row4 col2\" >-0.048014</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_4a1cdd30_8901_11eb_a4cf_0242ac1c0002level0_row5\" class=\"row_heading level0 row5\" >feature_exposure</th>\n",
              "                        <td id=\"T_4a1cdd30_8901_11eb_a4cf_0242ac1c0002row5_col0\" class=\"data row5 col0\" >0.082518</td>\n",
              "                        <td id=\"T_4a1cdd30_8901_11eb_a4cf_0242ac1c0002row5_col1\" class=\"data row5 col1\" >0.074126</td>\n",
              "                        <td id=\"T_4a1cdd30_8901_11eb_a4cf_0242ac1c0002row5_col2\" class=\"data row5 col2\" >0.094228</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_4a1cdd30_8901_11eb_a4cf_0242ac1c0002level0_row6\" class=\"row_heading level0 row6\" >max_feature_exposure</th>\n",
              "                        <td id=\"T_4a1cdd30_8901_11eb_a4cf_0242ac1c0002row6_col0\" class=\"data row6 col0\" >0.301773</td>\n",
              "                        <td id=\"T_4a1cdd30_8901_11eb_a4cf_0242ac1c0002row6_col1\" class=\"data row6 col1\" >0.289383</td>\n",
              "                        <td id=\"T_4a1cdd30_8901_11eb_a4cf_0242ac1c0002row6_col2\" class=\"data row6 col2\" >0.310453</td>\n",
              "            </tr>\n",
              "    </tbody></table>"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7fb173c98b90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsRJSpJNMiKQ"
      },
      "source": [
        "# Submission\n",
        "\n",
        "\n",
        "1. Create the prediction list.\n",
        "2. Link those predictions with the tournment data\n",
        "3. Write the id, prediction to a csv file.\n",
        "4. Use numerai wrapper to submit that .csv file as your current model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSo8yDCxTBB1"
      },
      "source": [
        "### Methods to handle submission"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "EoYEwqg4MiKQ",
        "outputId": "c6ebbb13-429e-448d-ee92-4ed8c0d3c26e"
      },
      "source": [
        "def load_api_creds_into_dict():\n",
        "  \"\"\"\n",
        "    Read creds.json and return a dictionary of your API keys.\n",
        "  \"\"\"\n",
        "  creds  = open('creds.json','r')\n",
        "  api_keys_dict = json.load(creds) # untested\n",
        "  creds.close()\n",
        "  return api_keys_dict\n",
        "\n",
        "api_keys_dict = load_api_creds_into_dict()\n",
        "mrquantsalot_model_id = api_keys_dict['mr_quants_model_id']\n",
        "tutModel_model_id = api_keys_dict['tutmodel_model_id']\n",
        "PREDICTION_NAME = \"prediction\" # this is the header of the csv file you are creating\n",
        "OUTPUT_DIR = '' # just the root of your local folder in this instance of google colab\n",
        "\n",
        "def open_api_access():\n",
        "    \"\"\"\n",
        "    Read in my private key from creds.json and return the numerai api wrapper\n",
        "    \"\"\"\n",
        "    api_keys_dict = load_api_creds_into_dict()\n",
        "    my_secret_key = api_keys_dict['secret_key']\n",
        "    my_public_id = api_keys_dict['public_id'] # unclear if this is your public id (the long string or the public_id ) \n",
        "    napi = numerapi.NumerAPI(secret_key=my_secret_key, public_id=my_public_id)\n",
        "    return napi\n",
        "\n",
        "\n",
        "def merge_predictions_id(tournament_vectors: pd.DataFrame, tournament_pred : np.ndarray):\n",
        "    \"\"\"\n",
        "    Create a dataframe that looks like \n",
        "    id,prediction\n",
        "    asdfads,.5429\n",
        "    asdfaddsss,.5051\n",
        "    ...\n",
        "    \"\"\"\n",
        "    predictions_df = tournament_vectors[\"id\"].to_frame() # get all the Ids and cast them to a frame\n",
        "    predictions_df[PREDICTION_NAME] = tournament_pred\n",
        "    return predictions_df\n",
        "\n",
        "\n",
        "def write_predictions_to_file(predictions_df: pd.DataFrame):\n",
        "    try:\n",
        "      out_location = open('myPredictions.csv', 'x')\n",
        "    except:\n",
        "      out_location = open('myPredictions.csv', 'w')\n",
        "\n",
        "    predictions_df.to_csv(out_location, index=False)\n",
        "    out_location.close()\n",
        "    return 'myPredictions.csv' # the file name where you save the predictions\n",
        "\n",
        "\n",
        "def submit_predictions_to_numerai(file_of_predictions='myPredictions.csv', model_id=mrquantsalot_model_id):\n",
        "    napi = open_api_access() # open a connection to the numerai API with your creds.\n",
        "    submission_id = napi.upload_predictions(file_of_predictions, model_id=mrquantsalot_model_id)\n",
        "    print(f'You successfully submitted for {mrquantsalot_model_id}')\n",
        "\n",
        "print('your helper methods work correctly')\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "your helper methods work correctly\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vwKpIz2XEw2"
      },
      "source": [
        "### Run your model on the real tournament data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8FKXKURMiKR"
      },
      "source": [
        "%%time\n",
        "# It might makes more sense to split this into several cells\n",
        "# predict using your model on tournment data\n",
        "tournament_pred = model.predict(test[features]) #it takes ~ 16 minutes to run your predictions\n",
        "print(f'The live tournament has {test.shape()} shape')\n",
        "print('it took you this long to run your model on the predictions.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Gz_qc5sXC5_"
      },
      "source": [
        "# add create a dataframe based on the id and predictiosn\n",
        "predictions_df = merge_predictions_id(tournament_vectors=test,tournament_pred =tournament_pred)\n",
        "print(predictions_df.head(10))\n",
        "predictions_file = write_predictions_to_file(predictions_df)\n",
        "print('wrote predictions to file')\n",
        "submit_predictions_to_numerai(file_of_predictions = file_with_predictions_to_submit, model_id=tutModel_model_id ) # default to mrquantsalot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLw0732kS-VT"
      },
      "source": [
        "### Results\n",
        "From the \"Diagnostic\" results of the MRQuants a lot on 3/19/2021\n",
        "CORR =.0260\n",
        "\n",
        "Std dev on CORR .0279.\n",
        "\n",
        "This is very bad.\n",
        "\n",
        "I need to incease my feature exposure and decrease the std dev of per era corrilations between my predictions and the true results. \n",
        "\n",
        "Unsure yet how to do that. \n",
        "\n",
        "params1 = {\n",
        "            'n_estimators': 2000,\n",
        "            'objective': 'regression',\n",
        "            'boosting_type': 'gbdt',\n",
        "            'max_depth': 55,\n",
        "            'learning_rate': 0.013, \n",
        "            'feature_fraction': 0.095,\n",
        "            'seed': 52\n",
        "            }\n",
        "\n",
        "            current params for this Mr quants a lot\n",
        "\n",
        "\n",
        "Lets see how increasing the n_estimators changes the feature exposure.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3aOmG--eQjt"
      },
      "source": [
        ""
      ]
    }
  ]
}