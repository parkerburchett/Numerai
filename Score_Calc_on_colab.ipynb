{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Score Calc on colab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "sf0mcGT2fzIT"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class ScoreCalculator:\n",
        "    \"\"\"\n",
        "        Calcuating various metrics on the relationship between your predictions, example predictions and validation data.\n",
        "        Call compute_numerai_diagnostics(a pd.Series of your predictions on the validation data)\n",
        "        Add method to compute more differnent diagonistcs\n",
        "        Currently not throughly tested.\n",
        "        Primarily based on: example.py \n",
        "    \"\"\"\n",
        "    def __init__(self, validation_data, example_preds):\n",
        "        \"\"\"\n",
        "          validation_data['rank_target'] is handeled in ping validation data\n",
        "        \"\"\"\n",
        "\n",
        "        self.validation_data = validation_data\n",
        "        self._rank_normalized_validation_targets = validation_data['rank_target'] \n",
        "        self._feature_col_names = [column_name for column_name in self.validation_data.columns if 'feature' in column_name]\n",
        "        self.example_predictions = example_preds\n",
        "        self._rank_normalized_example_predictions = example_preds['rank_example_prediction'] # hardcoded\n",
        "        self.starting_cols = validation_data.columns\n",
        "    \n",
        "    def score(self, df: pd.DataFrame)-> float:\n",
        "        \"\"\"\n",
        "          # You should replace with lambda for speed\n",
        "            utility to compute corr on a grouping of self._validation_data \n",
        "        \"\"\"\n",
        "        return _compute_corr(df['rank_target'], df['rank_prediction'])\n",
        "\n",
        "    # suspect\n",
        "    def richards_dependence(self, df, target_col, era_col, prediction_col) -> float: \n",
        "        \"\"\"\n",
        "            Measures the independendence of prediction with the targets\n",
        "            \n",
        "            Currently unused \n",
        "            example call:\n",
        "            richards_dependence(df, 'target', 'era', 'prediction'))\n",
        "            Source: Numerai Forumn user:richai @ https://forum.numer.ai/t/independence-and-sharpe/2560 | May 3 ,2021\n",
        "        \"\"\"  \n",
        "        scores_by_era = df.groupby(era_col).apply(lambda d: d[[prediction_col, target_col]].corr()[target_col][0])\n",
        "            \n",
        "        # these need to be ranked within era so \"error\" makes sense\n",
        "        df[prediction_col] = df.groupby(era_col)[prediction_col].rank(pct=True)\n",
        "        df[target_col] = df.groupby(era_col)[target_col].rank(pct=True)\n",
        "\n",
        "        df[\"era_score\"] = df[era_col].map(scores_by_era)\n",
        "\n",
        "        df[\"error\"] = (df[target_col] - df[prediction_col]) ** 2\n",
        "        df[\"1-error\"] = 1 - df[\"error\"]\n",
        "\n",
        "        # Returns the correlation of the 1-error with the era_score\n",
        "        # i.e. how dependent/correlated each prediction is with its era_score\n",
        "        return df[[\"1-error\", \"era_score\"]].corr()[\"era_score\"][0]\n",
        "\n",
        "\n",
        "    def rank_noramalize_series(self, col:pd.Series)-> pd.Series:\n",
        "        \"\"\"\n",
        "            Compute the rank ordering of col. Scale each element of col between 0 and 1 based on their relative size\n",
        "            Returns: a pd.Series\n",
        "        \"\"\" \n",
        "        scaled_col = (col.rank(method=\"first\") - 0.5) / len(col)\n",
        "        scaled_col.index = col.index\n",
        "        return scaled_col\n",
        "\n",
        "        # BROKEN\n",
        "    def compute_validation_corr(self)-> float:\n",
        "        \"\"\"\n",
        "            pred: your predictions on the validation data.\n",
        "            Compute your corr on the validation data.\n",
        "            # need to call add_predictions_to_validation_df() before you call this or it will throw an derror\n",
        "        \"\"\"\n",
        "  \n",
        "        ranked_targets = self.validation_data['rank_target']\n",
        "        ranked_preds = self.validation_data['rank_prediction']\n",
        "        print('inside compute validation corr doing corr on this dataframe')\n",
        "        print(self.validation_data[['rank_target', 'rank_prediction']].corr())\n",
        "        print('should be near .02 and negative when I do the corr ')\n",
        "        return self._compute_corr(ranked_targets, ranked_preds)\n",
        "\n",
        "\n",
        "    def _compute_corr(self, a: pd.Series, b: pd.Series )->float:\n",
        "        \"\"\"\n",
        "            Returns np.corrcoef on a and b. pass this only ranked correlations\n",
        "        \"\"\"\n",
        "        return np.corrcoef(a, b)[0, 1] # not ranked \n",
        "\n",
        "\n",
        "    def compute_validation_std(self) -> float:\n",
        "      \"\"\"\n",
        "          Returns the Standard Deviation of corr by era.\n",
        "      \"\"\"\n",
        "      return self.create_per_era_grouper().std()\n",
        "\n",
        "    def compute_validation_per_era_mean_corr(self)-> float:\n",
        "      \"\"\"\n",
        "      Returns the mean corr by era.\n",
        "      \"\"\"\n",
        "      return self.create_per_era_grouper().mean()\n",
        "\n",
        "    def create_per_era_grouper(self) -> pd.Series:\n",
        "      \"\"\"\n",
        "        Returns an array of era, Corr on validation targets for that era.\n",
        "      \"\"\"\n",
        "      return self.validation_data.groupby(\"era\").apply(lambda df: np.corrcoef(df['rank_target'], df['rank_prediction'])[0][1])\n",
        "\n",
        "    def compute_validation_sharpe(self)-> float:\n",
        "        \"\"\"\n",
        "            Computes your sharpe corr socre on each era \n",
        "            sharpe = average corr per era / std dev of corr per era. \n",
        "        \"\"\"\n",
        "        per_era_corr_grouper = self.create_per_era_grouper()\n",
        "        mean_per_era_corr = per_era_corr_grouper.mean()\n",
        "        print('inside of compute_validation_sharpe it says the average per era corr on validation dat is')\n",
        "        print(mean_per_era_corr)\n",
        "        std_per_era_corr = per_era_corr_grouper.std()\n",
        "        return mean_per_era_corr / std_per_era_corr\n",
        "        \n",
        "\n",
        "    def compute_max_drawdown(self)-> float:\n",
        "        \"\"\"\n",
        "            Copied from Numerai's example_model.py\n",
        "            Max drawdown is the \"largest cumulative decrease between any two eras in terms of validation correlation\"\n",
        "            In short this keeps a running total of corr between eras. Then it find the length of the largest decrease and returns that number. \n",
        "            Is an estimate of risk\n",
        "        \"\"\"\n",
        "        validation_correlations = self.create_per_era_grouper() # this needs to be stored in the class variables do avoid doing it twice\n",
        "        rolling_max = (validation_correlations + 1).cumprod().rolling(window=100, min_periods=1).max()                                                           \n",
        "        daily_value = (validation_correlations + 1).cumprod()\n",
        "        max_drawdown = -(rolling_max - daily_value).max()\n",
        "        return max_drawdown\n",
        "\n",
        "\n",
        "    def compute_feature_exposure(self)-> float:\n",
        "        \"\"\"\n",
        "            The maximum corrilatiosn your predictions have with any single feature\n",
        "            Copied from Numerai's example_model.py\n",
        "        \"\"\"\n",
        "        # pred_valid_df = self.validation_data # unclear if the default to \n",
        "        # pred_valid_df['prediction'] = rank_noramalize_series(pred) # add prediction column\n",
        "\n",
        "        # feature_names = [f for f in self.validation_data.columns if f.startswith(\"feature\")]\n",
        "        # feature_exposures = validation_data[feature_names].apply(lambda d: correlation(pred_valid_df['prediction'], d), axis=0) # axis =0 means by columns\n",
        "        feature_exposures = []\n",
        "        for col in self._feature_col_names:\n",
        "          feature_exposure_for_col = self._compute_corr(self.validation_data[col], self.validation_data['prediction']) # does feature exposrue look at rank corr \n",
        "          feature_exposures.append(feature_exposure_for_col)\n",
        "                                                             \n",
        "        max_feature_exposure = np.max(np.abs(np.array(feature_exposures)))\n",
        "        return max_feature_exposure\n",
        "\n",
        " ################################################################################\n",
        "    def compute_feature_neutral_corr_mean(self)-> float:\n",
        "        \"\"\"\n",
        "            Broken, and this is worse that just looking at feature exposure\n",
        "            The mean of your per era correlation after your predictions have been neutralized to all the features\n",
        "            Copied from Numerai's example_model.py\n",
        "        \"\"\"\n",
        "        pred_valid_df = self.validation_data\n",
        "        print('there should be prediction column and a rank_normalized prediction column')\n",
        "        print(pred_valid_df.columns)\n",
        "        # print(pred_valid_df.shape) \n",
        "\n",
        "        pred_valid_df.loc[:, \"neutralized_predictions\"] = self.neutralize(pred_valid_df, ['prediction'],\n",
        "                                            self._feature_col_names)['prediction'] \n",
        "        num_rows = pred_valid_df.shape[0]\n",
        "        scores = self.validation_data.groupby(\"era\").apply(lambda df:self._compute_corr(df['neutralized_predictions'], \n",
        "                                                                                        df['rank_prediction']))\n",
        "\n",
        "        # I the rank normalize within the lambda\n",
        "        # scores = pred_valid_df.groupby(\"era\").apply(\n",
        "        #          lambda df: self._compute_corr(self.rank_noramalize_series(df[\"neutral_sub\"]), \n",
        "        #                                        self.validation_data['rank_target'])).mean()\n",
        "\n",
        "        print('scores is :')\n",
        "        print(scores)\n",
        "        return np.mean(scores)\n",
        "\n",
        "####################################################################################\n",
        "\n",
        "    #suspect\n",
        "    def neutralize(self, df, columns, by, proportion=1.0):\n",
        "        \"\"\"\n",
        "            Copied as is from example_model.py\n",
        "        \"\"\"\n",
        "        scores = df.loc[:, columns] # scores is a df of all rows and only the coluns in columsn in \n",
        "        exposures = df[by].values\n",
        "\n",
        "        # constant column to make sure the series is completely neutral to exposures\n",
        "        exposures = np.hstack(\n",
        "            (exposures,\n",
        "            np.asarray(np.mean(scores)) * np.ones(len(exposures)).reshape(-1, 1)))\n",
        "\n",
        "        scores = scores - proportion * exposures.dot(\n",
        "            np.linalg.pinv(exposures).dot(scores))\n",
        "        return scores / scores.std()\n",
        "    \n",
        "    #suspect\n",
        "    def compute_mmc_stats(self, pred:pd.Series) -> tuple:\n",
        "        \"\"\"\n",
        "            Using example predictions as an estimate for the meta model, compute mmc stats\n",
        "            Copied from example_model.py\n",
        "            returns val_mmc_mean, corr_plus_mmc_sharpe, \n",
        "\n",
        "            Not refractored. Copied as is. Only variable and function names are changed\n",
        "\n",
        "        \"\"\"\n",
        "        pred_valid_df = self.validation_data # unclear if the default to \n",
        "        pred_valid_df['prediction'] = rank_noramalize_series(pred) # add prediction column\n",
        "        pred_valid_df['ExamplePreds'] = self.example_predictions\n",
        "        mmc_scores = []\n",
        "        corr_scores = []\n",
        "\n",
        "        for _, x in validation_data.groupby(\"era\"):\n",
        "            series = self.neutralize_series(pd.Series(self.rank_noramalize_series(x['prediction'])),\n",
        "                                    pd.Series(self.rank_noramalize_series(x[\"ExamplePreds\"])))\n",
        "            mmc_scores.append(np.cov(series, x['target'])[0, 1] / (0.29 ** 2))\n",
        "            corr_scores.append(correlation(self.rank_noramalize_series(x['prediction']), x['target']))\n",
        "\n",
        "        val_mmc_mean = np.mean(mmc_scores)\n",
        "        val_mmc_std = np.std(mmc_scores)\n",
        "        val_mmc_sharpe = val_mmc_mean / val_mmc_std\n",
        "        corr_plus_mmcs = [c + m for c, m in zip(corr_scores, mmc_scores)]\n",
        "        corr_plus_mmc_sharpe = np.mean(corr_plus_mmcs) / np.std(corr_plus_mmcs)\n",
        "        corr_plus_mmc_mean = np.mean(corr_plus_mmcs)\n",
        "        #corr_plus_mmc_sharpe_diff = corr_plus_mmc_sharpe - validation_sharpe\n",
        "\n",
        "        # print(\n",
        "        #     f\"MMC Mean: {val_mmc_mean}\\n\"\n",
        "        #     f\"Corr Plus MMC Sharpe:{corr_plus_mmc_sharpe}\\n\"\n",
        "        #     f\"Corr Plus MMC Diff:{corr_plus_mmc_sharpe_diff}\"\n",
        "        # )\n",
        "        return  val_mmc_mean, corr_plus_mmc_sharpe, \n",
        "\n",
        "    #suspect\n",
        "    def neutralize_series(self, series, by, proportion=1.0):\n",
        "        \"\"\"\n",
        "            Copied from example_model.py\n",
        "            not refactored\n",
        "        \"\"\"\n",
        "        scores = series.values.reshape(-1, 1)\n",
        "        exposures = by.values.reshape(-1, 1)\n",
        "\n",
        "        # this line makes series neutral to a constant column so that it's centered and for sure gets corr 0 with exposures\n",
        "        exposures = np.hstack(\n",
        "            (exposures,\n",
        "            np.array([np.mean(series)] * len(exposures)).reshape(-1, 1)))\n",
        "\n",
        "        correction = proportion * (exposures.dot(\n",
        "            np.linalg.lstsq(exposures, scores, rcond=None)[0]))\n",
        "        corrected_scores = scores - correction\n",
        "        neutralized = pd.Series(corrected_scores.ravel(), index=series.index)\n",
        "        return neutralized\n",
        "\n",
        "    \n",
        "    def compute_corr_with_example_preds(self, tournament_pred:pd.DataFrame) -> float:\n",
        "        \"\"\"\n",
        "            Compute the rank corrilation between your tournament_pred and the example predictions\n",
        "            tournament_pred: pd.DataFrame must have 'rank_prediction' column\n",
        "            WORKS \n",
        "        \"\"\"\n",
        "        return self._compute_corr(tournament_pred['rank_prediction'], self.example_predictions['rank_example_prediction'])\n",
        "    \n",
        "\n",
        "    def add_predictions_to_validation_df(self, tournament_preds:pd.DataFrame) -> None:\n",
        "      \"\"\"\n",
        "        updates the self.validation_df with your prediction in tournament_predss\n",
        "        tournament_df: pd.DataFrame Your predictions for this round.\n",
        "        Must have index = 'id'\n",
        "        Must have column 'prediction' \n",
        "      \"\"\"\n",
        "      valid_ids = self.validation_data.index # get all the ids with the valaidatino data\n",
        "      preds_on_valid_data = tournament_preds.loc[valid_ids,:] # subset on the validation data\n",
        "      self.validation_data['prediction'] = preds_on_valid_data['prediction']\n",
        "\n",
        "      rows = self.validation_data.shape[0] # number of rows\n",
        "      self.validation_data['rank_prediction'] = self.validation_data['prediction'].rank(method='first') / rows\n",
        "      return\n",
        "    \n",
        "    def reset_validation_df(self)-> None:\n",
        "      \"\"\"\n",
        "          Remove the columns added to compute the scores\n",
        "      \"\"\"\n",
        "      self.validation_data = self.validation_data.loc[:,self.starting_cols]\n",
        "\n",
        "\n",
        "    def compute_numerai_diagnostics(self, tournament_preds: pd.DataFrame):\n",
        "      \"\"\"\n",
        "\n",
        "          this is the main method that you call on your validation predictions\n",
        "          my_score_calculator = ScoreCalculator()\n",
        "          scores =my_score_calculator.compute_numerai_diagnostics(my_model.predict(validation_data[features]))\n",
        "          print(scores)\n",
        "\n",
        "          Return a dataframe that is equivalent to the diagnostics tab on numerai\n",
        "          preds: A dataframe of your model's prediction accross the entire live tournament data for this round.\n",
        "            Must have index='id'\n",
        "                      columns = 'prediction', 'rank_prediction'\n",
        "      \"\"\"\n",
        "      self.add_predictions_to_validation_df(tournament_preds)\n",
        "      diagnostics_df = pd.DataFrame()\n",
        "      diagnostics_df['valid_sharpe'] = [self.compute_validation_sharpe()]\n",
        "      diagnostics_df['valid_corr'] = [self.compute_validation_corr()] # wrong\n",
        "      diagnostics_df['valid_std_dev'] = [self.compute_validation_std()]\n",
        "      diagnostics_df['feature_exposure'] = [self.compute_feature_exposure()]\n",
        "      diagnostics_df['max_drawdown'] = [self.compute_max_drawdown()]\n",
        "      diagnostics_df['corr_with_example_preds '] = [self.compute_corr_with_example_preds(tournament_preds)]\n",
        "\n",
        "      #diagnostics_df['valid_FNC'] = [self.compute_feature_neutral_corr_mean()] # hard copied as is from example_model.py\n",
        "      #val_mmc_mean, corr_plus_mmc_sharpe = self.compute_mmc_stats(tournament_preds) # Hard copied as is from example_model.py\n",
        "      #diagnostics_df['corr_plus_MMC_sharpe'] = [corr_plus_mmc_sharpe]\n",
        "      #diagnostics_df['MMC_mean '] = [val_mmc_mean]\n",
        "      self.reset_validation_df()\n",
        "      \n",
        "      return diagnostics_df\n",
        "\n"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvB7bsbhgAnW"
      },
      "source": [
        "## Class to ping scores. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_G2EHwZVgDZO"
      },
      "source": [
        "class NumeraiDataLoader:\n",
        "  \"\"\"\n",
        "      Pings and cleans the raw data from Numerai\n",
        "  \"\"\"\n",
        "  def ping_validation_data(self) -> pd.DataFrame:\n",
        "            \"\"\"\n",
        "            Ping Numerai to create get the live tournament data and extact all the validation data.\n",
        "\n",
        "            Adapted from : https://www.kaggle.com/code1110/numerai-tournament | May 3, 2021\n",
        "            \"\"\"\n",
        "            tournament_data_url = 'https://numerai-public-datasets.s3-us-west-2.amazonaws.com/latest_numerai_tournament_data.csv.xz'\n",
        "            tournament_df = pd.read_csv(tournament_data_url)\n",
        "            valid_df = tournament_df[tournament_df[\"data_type\"] == \"validation\"].reset_index(drop = True)\n",
        "            feature_cols = valid_df.columns[valid_df.columns.str.startswith('feature')]\n",
        "\n",
        "            map_floats_to_ints = {0.0 : 0, 0.25 : 1, 0.5 : 2, 0.75 : 3, 1.0 : 4}\n",
        "            for col in feature_cols:\n",
        "                valid_df[col] = valid_df[col].map(map_floats_to_ints).astype(np.uint8) # reduce space costs by casting features as ints\n",
        "                \n",
        "            valid_df[\"era\"] = valid_df[\"era\"].apply(lambda x: int(x[3:])) # strip the word 'era' from the era column\n",
        "            valid_df.drop(columns=[\"data_type\"], inplace=True)\n",
        "\n",
        "            total_valid_rows = valid_df.shape[0]\n",
        "          \n",
        "            valid_df['rank_target'] = valid_df['target'].rank(method='first') / total_valid_rows\n",
        "            \n",
        "            print('inside of NumeraiDataLoader.ping_validation_data')\n",
        "            print('rank target value counts')\n",
        "            print(valid_df['rank_target'].value_counts())\n",
        "\n",
        "            print('target value counts')\n",
        "            print(valid_df['target'].value_counts())\n",
        "            valid_df.set_index('id', inplace=True)\n",
        "            return valid_df\n",
        "\n",
        "        # called during init # broken You need to specify that this is example preds over the entire df\n",
        "  def ping_example_predictions(self)-> pd.DataFrame:\n",
        "      \"\"\"\n",
        "          Create a dataframe of Id, Prediction, rank_prediction, where Id, is the id column in tournament_data.csv prediction is the numerai provided example model, and rank_prediction is the normalized prediction target\n",
        "         \t                  prediction  rank_prediction\n",
        "          id\t\t\n",
        "          n0003aa52cab36c2\t0.49\t0.097334\n",
        "          n000920ed083903f\t0.49\t0.097335\n",
        "          n0038e640522c4a6\t0.53\t0.969455\n",
        "          n004ac94a87dc54b\t0.51\t0.656894\n",
        "          n0052fe97ea0c05f\t0.50\t0.332613\n",
        "      \"\"\"\n",
        "      example_predictions_url = \"https://numerai-public-datasets.s3-us-west-2.amazonaws.com/latest_numerai_example_predictions_data.csv.xz\"\n",
        "      example_preds =  pd.read_csv(example_predictions_url, index_col=0) # defaults the index to be the 0th column\n",
        "      total_example_prediction_rows = example_preds.shape[0]\n",
        "      example_preds['rank_example_prediction'] = example_preds['prediction'].rank(method='first') / total_example_prediction_rows\n",
        "      return example_preds\n"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onM80O8sj7j9"
      },
      "source": [
        "### tester"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6yVBBx0gce-",
        "outputId": "34283afd-64f6-4b95-ff07-68b9e18e4e5f"
      },
      "source": [
        "myLoader = NumeraiDataLoader()\n",
        "validation_data, example_preds = myLoader.ping_validation_data(), myLoader.ping_example_predictions()\n",
        "\n"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "inside of NumeraiDataLoader.ping_validation_data\n",
            "rank target value counts\n",
            "0.458423    1\n",
            "0.142264    1\n",
            "0.564607    1\n",
            "0.514316    1\n",
            "0.658671    1\n",
            "           ..\n",
            "0.224461    1\n",
            "0.703177    1\n",
            "0.826911    1\n",
            "0.985375    1\n",
            "0.337562    1\n",
            "Name: rank_target, Length: 137779, dtype: int64\n",
            "target value counts\n",
            "0.50    68954\n",
            "0.75    27533\n",
            "0.25    27531\n",
            "1.00     6882\n",
            "0.00     6879\n",
            "Name: target, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROAYPsPXrghu"
      },
      "source": [
        "def rank_normalize_tournament_preds(tournament_preds: pd.DataFrame) -> pd.DataFrame:\n",
        "  \"\"\"\n",
        "      Add rank_prediction column to tournament_preds. The rank Normalized predictions\n",
        "      tournament_preds must have 'prediction' column\n",
        "  \"\"\"\n",
        "  if 'prediction' not in tournament_preds.columns:\n",
        "    raise ValueError(\"\"\"tournament_preds must contain a column named 'prediction'. You passed a df without that column\"\"\")\n",
        "  tournament_preds_rows = tournament_preds.shape[0]\n",
        "  tournament_preds['rank_prediction'] = tournament_preds['prediction'].rank(method='first') / tournament_preds_rows\n",
        "  return tournament_preds\n",
        "\n"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyD42vEP0Hqg"
      },
      "source": [
        "##Test Calc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-1VD3sTnklP"
      },
      "source": [
        "tournament_preds = np.random.shuffle(example_preds['prediction'])\n",
        "print('corr between example and my randomized and example preds')\n",
        "\n",
        "tournament_preds = rank_normalize_tournament_preds(tournament_preds)\n",
        "print(np.corrcoef(tournament_preds['rank_prediction'],example_preds['rank_example_prediction']))\n",
        "\n",
        "calc = ScoreCalculator(validation_data, example_preds)\n",
        "calc.compute_numerai_diagnostics(tournament_preds)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}