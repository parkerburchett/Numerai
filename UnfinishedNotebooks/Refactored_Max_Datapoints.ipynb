{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Refactored Max Datapoints.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "b364c12-MiKD"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parkerburchett/Numerai/blob/Refactor-Max-DataPoints/Refactored_Max_Datapoints.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7atJJ2fMiJ8"
      },
      "source": [
        "# This notebook explores the Hyper Parameter Space of the lightgbm library\n",
        "\n",
        "\n",
        "You might want to find out how corrilataed different seeds of the optimal hyper params are. Then submit 4 versions of it, that are the most un  corrilated. but 3 nmr on each of them\n",
        "\n",
        "\n",
        "Create a sub evaluation method to count the % of eras that you have >0 corr for. \n",
        "\n",
        "You might want  explor hyper parm for xgboost as well. and some other out of the box algos\n",
        "\n",
        "You might want a custom loss function of \"brair score\"  sum(true_outcome - prediction)^2 for all outcome, preiction in df. \n",
        "\n",
        "Put this in the validation scorring methods. add it to score summary\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0iP-UgesMiKB",
        "outputId": "4468f83d-9005-4fec-fbae-a561499aae7a"
      },
      "source": [
        "!pip install numerapi\n",
        "import numerapi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting numerapi\n",
            "  Downloading https://files.pythonhosted.org/packages/fd/96/ebdbaff5a2fef49b212e4f40634166f59e45462a768c0136d148f00255c5/numerapi-2.4.5-py3-none-any.whl\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from numerapi) (2018.9)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from numerapi) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from numerapi) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.29.1 in /usr/local/lib/python3.7/dist-packages (from numerapi) (4.41.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from numerapi) (2.8.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->numerapi) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->numerapi) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->numerapi) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->numerapi) (2.10)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil->numerapi) (1.15.0)\n",
            "Installing collected packages: numerapi\n",
            "Successfully installed numerapi-2.4.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4WFeO-ZMiKC"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os, sys\n",
        "import gc\n",
        "import pathlib\n",
        "import json\n",
        "import datetime\n",
        "from typing import List, NoReturn, Union, Tuple, Optional, Text, Generic, Callable, Dict\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, QuantileTransformer\n",
        "from sklearn.model_selection import KFold, StratifiedKFold, TimeSeriesSplit\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, log_loss, mean_squared_error, mean_absolute_error, f1_score\n",
        "from scipy.stats import spearmanr # -P I think this is corr. \n",
        "import joblib\n",
        "\n",
        "# model\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "import operator\n",
        "\n",
        "# visualize\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.style as style\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot\n",
        "from matplotlib.ticker import ScalarFormatter\n",
        "sns.set_context(\"talk\")\n",
        "style.use('seaborn-colorblind')\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0ELMvF79l3D",
        "outputId": "44de643f-faf1-4d92-f731-6036982ec0ab"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b364c12-MiKD"
      },
      "source": [
        "## Methods to Gather and Clean Incoming Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmV-K6xHoqxW"
      },
      "source": [
        "def create_global_variables()-> None:\n",
        "  \"\"\"\n",
        "    Create all global variables. \n",
        "    ROUND_NUMBER,FEATURES,TARGET,\n",
        "    TOURNAMENT_DATA,TRAINING_DATA,VALIDATION_DATA\n",
        "  \"\"\"\n",
        "  try:\n",
        "    if HAVE_GATHERED_DATA == FALSE:\n",
        "      ping_training_data()\n",
        "      ping_tournament_data()\n",
        "      create_validation_data(df = TOURNAMENT_DATA)\n",
        "\n",
        "      create_global_constants()\n",
        "      drop_data_type_columns()\n",
        "      HAVE_GATHERED_DATA = True\n",
        "  except NameError:\n",
        "      ping_training_data()\n",
        "      ping_tournament_data()\n",
        "      create_validation_data(df = TOURNAMENT_DATA)\n",
        "      \n",
        "      create_global_constants()\n",
        "      drop_data_type_columns()\n",
        "      HAVE_GATHERED_DATA = True\n",
        "\n",
        "def drop_data_type_columns():\n",
        "  TRAINING_DATA.drop(columns=[\"data_type\"], inplace=True)\n",
        "  VALIDATION_DATA.drop(columns=[\"data_type\"], inplace=True) #\n",
        "  TOURNAMENT_DATA.drop(columns=[\"data_type\"], inplace=True)\n",
        "\n",
        "def ping_training_data():\n",
        "  global TRAINING_DATA\n",
        "  TRAINING_DATA = read_data('train')\n",
        "  \n",
        "def ping_tournament_data():\n",
        "  global TOURNAMENT_DATA\n",
        "  TOURNAMENT_DATA = read_data('tournament')\n",
        "\n",
        "def create_validation_data(df):\n",
        "  global VALIDATION_DATA\n",
        "  VALIDATION_DATA  = df[df[\"data_type\"] == \"validation\"].reset_index(drop = True)\n",
        "\n",
        "def cast_eras_as_int(x): \n",
        "    try:\n",
        "        return int(x[3:]) # strip the first 3 characters from each era\n",
        "    except:\n",
        "        return -99\n",
        "\n",
        "def read_data(data):\n",
        "    if data == 'train':\n",
        "        df = pd.read_csv('https://numerai-public-datasets.s3-us-west-2.amazonaws.com/latest_numerai_training_data.csv.xz')\n",
        "    elif data == 'tournament':\n",
        "        df = pd.read_csv('https://numerai-public-datasets.s3-us-west-2.amazonaws.com/latest_numerai_tournament_data.csv.xz')\n",
        "        \n",
        "    feature_cols = df.columns[df.columns.str.startswith('feature')]\n",
        "    mapping = {0.0 : 0, 0.25 : 1, 0.5 : 2, 0.75 : 3, 1.0 : 4}\n",
        "\n",
        "    for c in feature_cols:\n",
        "        df[c] = df[c].map(mapping).astype(np.uint8)\n",
        "        \n",
        "    df[\"era\"] = df[\"era\"].apply(cast_eras_as_int)\n",
        "    return df\n",
        "\n",
        "\n",
        "def create_global_constants() -> None:\n",
        "  global TARGET\n",
        "  TARGET = get_target_constant(TOURNAMENT_DATA)\n",
        "  global FEATURES\n",
        "  FEATURES = get_features_constant(TOURNAMENT_DATA)\n",
        "  napi = open_api_access()\n",
        "  global ROUND_NUMBER\n",
        "  ROUND_NUMBER = napi.get_current_round()\n",
        "\n",
        "\n",
        "def get_target_constant(tournament_data: pd.DataFrame):\n",
        "  return tournament_data.columns[tournament_data.columns.str.startswith('target')].values.tolist()[0]\n",
        "\n",
        "\n",
        "def get_features_constant(tournament_data: pd.DataFrame):\n",
        "  return [column_names for column_names in tournament_data.columns.values.tolist() if 'feature' in column_names]\n",
        "\n",
        "\n",
        "\n",
        "def load_api_creds_into_dict():\n",
        "  creds  = open('/content/drive/MyDrive/creds.json','r') \n",
        "  api_keys_dict = json.load(creds)\n",
        "  creds.close()\n",
        "  return api_keys_dict\n",
        "\n",
        "\n",
        "def open_api_access():\n",
        "    api_keys_dict = load_api_creds_into_dict()\n",
        "    my_secret_key = api_keys_dict['secret_key']\n",
        "    my_public_id = api_keys_dict['public_id']\n",
        "    napi = numerapi.NumerAPI(secret_key=my_secret_key, public_id=my_public_id)\n",
        "    return napi\n",
        "\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBuYhojHFcQW"
      },
      "source": [
        "# Get the training and testing data and create the global variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3eMSLtXzspp",
        "outputId": "d0ef020e-21c3-4f1d-d63a-957b46237813"
      },
      "source": [
        "%%time\n",
        "create_global_variables()\n",
        "HAVE_GATHERED_DATA = True\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 5min 21s, sys: 19.8 s, total: 5min 41s\n",
            "Wall time: 5min 47s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4khTMbONzW-"
      },
      "source": [
        "### ModelStats Object\n",
        "\n",
        "1. Stores the Trained Model\n",
        "2. Stores the Hyper Parameters\n",
        "3. Stores the Validation Scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LCuE_km4Y_3"
      },
      "source": [
        "PATH_TO_SAVE_SCORES = '/content/drive/MyDrive/numerai_hyperparams_scores.csv' # change this when you get different models.\n",
        "class ModelStats():\n",
        "  \"\"\"\n",
        "  An object that tracks Hyper Parameters, Time Costs and Scores. \n",
        "  \"\"\"\n",
        "  def __init__(self, model, scores:dict, total_time):\n",
        "        self.model = model \n",
        "        self.hyperparams = model.get_params() \n",
        "        self.scores = scores \n",
        "        self.total_time = total_time\n",
        "        self.params_scores_df = None \n",
        "\n",
        "\n",
        "  def create_params_scores_df(self):\n",
        "    \"\"\"\n",
        "    Create a DataFrame Representing the Hyper Parameters and Scores of this model.\n",
        "    \"\"\"\n",
        "    if self.params_scores_df == None:\n",
        "      all_stats_dict = {}\n",
        "      all_stats_dict['total_time'] = self.total_time\n",
        "      all_stats_dict['round_number'] = ROUND_NUMBER\n",
        "      all_stats_dict.update(self.hyperparams) # dict.update(dict) merges two dictionaries\n",
        "      all_stats_dict.update(self.scores)\n",
        "      DECIMALS = 4 \n",
        "      for key in all_stats_dict.keys():\n",
        "          try:\n",
        "            all_stats_dict[key] = [round(all_stats_dict[key],DECIMALS)]\n",
        "          except:\n",
        "            all_stats_dict[key] = [all_stats_dict[key]]\n",
        "\n",
        "      self.params_scores_df = pd.DataFrame.from_dict(all_stats_dict)\n",
        "\n",
        "  \n",
        "  def save_hyperparams_scores_to_google_drive_tabular(self)-> None:\n",
        "    \"\"\"\n",
        "        Appends this model's scores into your Google Drive with the other scores.\n",
        "    \"\"\"\n",
        "    self.create_params_scores_df()\n",
        "    # try to load that current df into memory\n",
        "    disk_df = pd.read_csv(PATH_TO_SAVE_SCORES)\n",
        "    #print(f'Read in new saved scores with {disk_df.shape} shape')\n",
        "    new_updated_disk_df = merge_dfs_horizontally(disk_df, self.params_scores_df)\n",
        "    #print(f'added next line of scores with {new_updated_disk_df.shape} shape')\n",
        "    new_updated_disk_df.to_csv(PATH_TO_SAVE_SCORES, index=False)\n",
        "    #print('Overwrote the new_updated_disk_df to your Google Drive')\n",
        "\n",
        "    try:\n",
        "      with open(PATH_TO_SAVE_SCORES, 'r') as scores_file:\n",
        "          lines = scores_file.readlines()\n",
        "          if len(lines) == 0:\n",
        "            print(\"the file does not exist. You are good to save your first score df\")\n",
        "    except:\n",
        "      self.params_scores_df.to_csv(PATH_TO_SAVE_SCORES, index=False)\n",
        "      # not exhaustively tested   \n",
        "           \n",
        "\n",
        "def merge_dfs_horizontally(df1 : pd.DataFrame, df2: pd.DataFrame)-> pd.DataFrame:\n",
        "  merged_df = pd.concat([df1, df2], axis=0)\n",
        "  return merged_df\n",
        "\n",
        "\n",
        "def train_LGBMRegressor(params: dict, train_data): \n",
        "  \"\"\"\n",
        "  Inputs: a dict of hyper paramaters for the model, \n",
        "  train_data: a pd.DataFrame of the Training Data\n",
        "\n",
        "  Returns a trained model\n",
        "  \"\"\"\n",
        "  model = lgb.LGBMRegressor(**params) \n",
        "  model.fit(train_data[FEATURES], train_data[TARGET])\n",
        "  return model\n",
        "\n",
        "def train_xgboost(train_data): \n",
        "  \"\"\"\n",
        "    Not deeply tested. Has much higher 10x time costs to train a single modle. \n",
        "    with these params it is low end but respectable. but takes about 2 hours to train via google colab\n",
        "  \"\"\"\n",
        "  model = xgb.XGBRegressor(max_depth=5, learning_rate=0.01, n_estimators=2000, n_jobs=-1, colsample_bytree=0.1)\n",
        "  model.fit(train_data[FEATURES], train_data[TARGET])\n",
        "  return model\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bWSBkk2_m3h"
      },
      "source": [
        "#### Methods to Determine Validation Scores\n",
        "\n",
        "1. I did not write these. I added the English comments\n",
        "source https://www.kaggle.com/code1110/numerai-tournament"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szXbJM0mMiKJ"
      },
      "source": [
        "# naming conventions\n",
        "PREDICTION_NAME = 'prediction'\n",
        "TARGET_NAME = TARGET # 'target is the string named 'target'\n",
        "# EXAMPLE_PRED = 'example_prediction'\n",
        "\n",
        "# ---------------------------\n",
        "# Functions\n",
        "# ---------------------------\n",
        "def valid4score(valid : pd.DataFrame, pred : np.ndarray, load_example: bool=True, save : bool=False) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Generate new valid pandas dataframe for computing scores\n",
        "    \n",
        "    :INPUT:\n",
        "    - valid : pd.DataFrame extracted from tournament data (data_type='validation')\n",
        "    \n",
        "    \"\"\"\n",
        "    valid_df = valid.copy() # the validation dataframe you use this to test the CORR and other values\n",
        "\n",
        "    # Your model creates an array of floats [0,1] rank method converst them in a list of ints. \n",
        "\n",
        "    # your lis tof ints is then compared to their list of ints. \n",
        "    valid_df['prediction'] = pd.Series(pred).rank(pct=True, method=\"first\") # pred is the array of predictions your model creates for the set of validation vectors.  \n",
        "    # I am unsure if this preds is a float only only between 0,1,2,3,4. \n",
        "    valid_df.rename(columns={TARGET: 'target'}, inplace=True)\n",
        "    \n",
        "    # I don't know what the load example boolean is. I think you can use this to save predictions.\n",
        "    if load_example:\n",
        "        valid_df[EXAMPLE_PRED] = pd.read_csv(EXP_DIR + 'valid_df.csv')['prediction'].values\n",
        "    \n",
        "    if save==True:\n",
        "        valid_df.to_csv(OUTPUT_DIR + 'valid_df.csv', index=False)\n",
        "        print('Validation dataframe saved!')\n",
        "    \n",
        "    return valid_df\n",
        "\n",
        "def compute_corr(valid_df : pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Compute rank correlation\n",
        "\n",
        "    THIS IS WHAT YOU ARE PRIMARILY PAID ON \n",
        "    \n",
        "    :INPUT:\n",
        "    - valid_df : pd.DataFrame where at least 2 columns ('prediction' & 'target') exist\n",
        "    \n",
        "    \"\"\"\n",
        "    # this uses Person Correilation. \n",
        "    # I You are paid on spearman corrilation. That is where the ratio of change is important not the raw amount of change\n",
        "    # see: https://support.minitab.com/en-us/minitab-express/1/help-and-how-to/modeling-statistics/regression/supporting-topics/basics/a-comparison-of-the-pearson-and-spearman-correlation-methods/\n",
        "    return np.corrcoef(valid_df[\"target\"], valid_df['prediction'])[0, 1]\n",
        "\n",
        "def compute_max_drawdown(validation_correlations : pd.Series):\n",
        "    \"\"\"\n",
        "    Compute max drawdown\n",
        "    \n",
        "    :INPUT:\n",
        "    - validation_correaltions : pd.Series\n",
        "    \"\"\"\n",
        "    \n",
        "    rolling_max = (validation_correlations + 1).cumprod().rolling(window=100, min_periods=1).max()\n",
        "    daily_value = (validation_correlations + 1).cumprod()\n",
        "    max_drawdown = -(rolling_max - daily_value).max()\n",
        "    \n",
        "    return max_drawdown\n",
        "\n",
        "def compute_val_corr(valid_df : pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Compute rank correlation for valid periods\n",
        "    \n",
        "    :INPUT:\n",
        "    - valid_df : pd.DataFrame where at least 2 columns ('prediction' & 'target') exist\n",
        "    \"\"\"\n",
        "    \n",
        "    # all validation\n",
        "    correlation = compute_corr(valid_df)\n",
        "    #print(\"rank corr = {:.4f}\".format(correlation))\n",
        "    return correlation\n",
        "    \n",
        "def compute_val_sharpe(valid_df : pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Compute sharpe ratio for valid periods\n",
        "    \n",
        "    :INPUT:\n",
        "    - valid_df : pd.DataFrame where at least 2 columns ('prediction' & 'target') exist\n",
        "    \"\"\"\n",
        "    # all validation\n",
        "    d = valid_df.groupby('era')[['target', 'prediction']].corr().iloc[0::2,-1].reset_index()\n",
        "    me = d['prediction'].mean()\n",
        "    sd = d['prediction'].std()\n",
        "    max_drawdown = compute_max_drawdown(d['prediction'])\n",
        "    #print('sharpe ratio = {:.4f}, corr mean = {:.4f}, corr std = {:.4f}, max drawdown = {:.4f}'.format(me / sd, me, sd, max_drawdown))\n",
        "    \n",
        "    return me / sd, me, sd, max_drawdown\n",
        "    \n",
        "def feature_exposures(valid_df : pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Compute feature exposure\n",
        "    \n",
        "    :INPUT:\n",
        "    - valid_df : pd.DataFrame where at least 2 columns ('prediction' & 'target') exist\n",
        "    \"\"\"\n",
        "    feature_names = [f for f in valid_df.columns\n",
        "                     if f.startswith(\"feature\")]\n",
        "    exposures = []\n",
        "    for f in feature_names:\n",
        "        fe = spearmanr(valid_df['prediction'], valid_df[f])[0]\n",
        "        exposures.append(fe)\n",
        "    return np.array(exposures)\n",
        "\n",
        "def max_feature_exposure(fe : np.ndarray):\n",
        "    return np.max(np.abs(fe))\n",
        "\n",
        "def feature_exposure(fe : np.ndarray):\n",
        "    return np.sqrt(np.mean(np.square(fe)))\n",
        "\n",
        "def compute_val_feature_exposure(valid_df : pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Compute feature exposure for valid periods\n",
        "    \n",
        "    :INPUT:\n",
        "    - valid_df : pd.DataFrame where at least 2 columns ('prediction' & 'target') exist\n",
        "    \"\"\"\n",
        "    # all validation\n",
        "    fe = feature_exposures(valid_df)\n",
        "    fe1, fe2 = feature_exposure(fe), max_feature_exposure(fe)\n",
        "    #print('feature exposure = {:.4f}, max feature exposure = {:.4f}'.format(fe1, fe2))\n",
        "     \n",
        "    return fe1, fe2\n",
        "\n",
        "# to neutralize a column in a df by many other columns\n",
        "#         I have no idea what this method does. -P. need to read about it and write up a link to it. \n",
        "def neutralize(df, columns, by, proportion=1.0):\n",
        "    scores = df.loc[:, columns]\n",
        "    exposures = df[by].values\n",
        "\n",
        "    # constant column to make sure the series is completely neutral to exposures\n",
        "    exposures = np.hstack(\n",
        "        (exposures,\n",
        "         np.asarray(np.mean(scores)) * np.ones(len(exposures)).reshape(-1, 1)))\n",
        "\n",
        "    scores = scores - proportion * exposures.dot(\n",
        "        np.linalg.pinv(exposures).dot(scores))\n",
        "    return scores / scores.std()\n",
        "\n",
        "\n",
        "# to neutralize any series by any other series\n",
        "def neutralize_series(series, by, proportion=1.0):\n",
        "    scores = series.values.reshape(-1, 1)\n",
        "    exposures = by.values.reshape(-1, 1)\n",
        "\n",
        "    # this line makes series neutral to a constant column so that it's centered and for sure gets corr 0 with exposures\n",
        "    exposures = np.hstack(\n",
        "        (exposures,\n",
        "         np.array([np.mean(series)] * len(exposures)).reshape(-1, 1)))\n",
        "\n",
        "    correction = proportion * (exposures.dot(\n",
        "        np.linalg.lstsq(exposures, scores, rcond=None)[0]))\n",
        "    corrected_scores = scores - correction\n",
        "    neutralized = pd.Series(corrected_scores.ravel(), index=series.index)\n",
        "    return neutralized\n",
        "\n",
        "\n",
        "def unif(df):\n",
        "    x = (df.rank(method=\"first\") - 0.5) / len(df)\n",
        "    return pd.Series(x, index=df.index)\n",
        "\n",
        "def get_feature_neutral_mean(df):\n",
        "    feature_cols = [c for c in df.columns if c.startswith(\"feature\")]\n",
        "    df.loc[:, \"neutral_sub\"] = neutralize(df, [PREDICTION_NAME],\n",
        "                                          feature_cols)[PREDICTION_NAME]\n",
        "    scores = df.groupby(\"era\").apply(\n",
        "        lambda x: np.corrcoef(x[\"neutral_sub\"].rank(pct=True, method=\"first\"), x[TARGET_NAME])).mean()\n",
        "    return np.mean(scores)\n",
        "\n",
        "def compute_val_mmc(valid_df : pd.DataFrame):    \n",
        "    # MMC over validation\n",
        "    mmc_scores = []\n",
        "    corr_scores = []\n",
        "    for _, x in valid_df.groupby(\"era\"):\n",
        "        series = neutralize_series(pd.Series(unif(x[PREDICTION_NAME])),\n",
        "                                   pd.Series(unif(x[EXAMPLE_PRED])))\n",
        "        mmc_scores.append(np.cov(series, x[TARGET_NAME])[0, 1] / (0.29 ** 2)) # I have no idea what htis line does (0.29 ** 2)\n",
        "        corr_scores.append(np.corrcoef(unif(x[PREDICTION_NAME]).rank(pct=True, method=\"first\"), x[TARGET_NAME]))\n",
        "\n",
        "    val_mmc_mean = np.mean(mmc_scores)\n",
        "    val_mmc_std = np.std(mmc_scores)\n",
        "    val_mmc_sharpe = val_mmc_mean / val_mmc_std\n",
        "    corr_plus_mmcs = [c + m for c, m in zip(corr_scores, mmc_scores)]\n",
        "    corr_plus_mmc_sharpe = np.mean(corr_plus_mmcs) / np.std(corr_plus_mmcs)\n",
        "    corr_plus_mmc_mean = np.mean(corr_plus_mmcs)\n",
        "\n",
        "    #print(\"MMC Mean = {:.6f}, MMC Std = {:.6f}, CORR+MMC Sharpe = {:.4f}\".format(val_mmc_mean, val_mmc_std, corr_plus_mmc_sharpe))\n",
        "\n",
        "    # Check correlation with example predictions\n",
        "    corr_with_example_preds = np.corrcoef(valid_df[EXAMPLE_PRED].rank(pct=True, method=\"first\"),\n",
        "                                          valid_df[PREDICTION_NAME].rank(pct=True, method=\"first\"))[0, 1]\n",
        "    #print(\"Corr with example preds: {:.4f}\".format(corr_with_example_preds))\n",
        "    \n",
        "    return val_mmc_mean, val_mmc_std, corr_plus_mmc_sharpe, corr_with_example_preds\n",
        "\n",
        "\n",
        "# this is the main method. The rest are just called interanlly. \n",
        "def score_summary(valid_df : pd.DataFrame):\n",
        "    score_dict = {}\n",
        "    \n",
        "    try:\n",
        "        score_dict['correlation'] = compute_val_corr(valid_df)\n",
        "    except:\n",
        "        print('ERR: computing correlation')\n",
        "    try:\n",
        "        score_dict['corr_sharpe'], score_dict['corr_mean'], score_dict['corr_std'], score_dict['max_drawdown'] = compute_val_sharpe(valid_df)\n",
        "    except:\n",
        "        print('ERR: computing sharpe')\n",
        "    try:\n",
        "        score_dict['feature_exposure'], score_dict['max_feature_exposure'] = compute_val_feature_exposure(valid_df)\n",
        "    except:\n",
        "        print('ERR: computing feature exposure')\n",
        "    # try:\n",
        "    #     score_dict['mmc_mean'], score_dict['mmc_std'], score_dict['corr_mmc_sharpe'], score_dict['corr_with_example_xgb'] = compute_val_mmc(valid_df)\n",
        "    # except:\n",
        "    #     print('ERR: computing MMC')\n",
        "    \n",
        "    return score_dict"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlvD8U4bv072"
      },
      "source": [
        "def look_at_best_models_so_far():\n",
        "  df = load_saved_params()\n",
        "  filter = df['correlation'] >.02\n",
        "  best = df[filter].sort_values(by ='correlation', ascending=False).head(20)\n",
        "  best.describe()"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lBD0oUmBWzx"
      },
      "source": [
        "### Command and cotrol methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80k6-VBtBWhf"
      },
      "source": [
        "def train_validate_store(params:dict, train_data: pd.DataFrame, validation_data: pd.DataFrame):\n",
        "  \"\"\"\n",
        "    Create a LGBM model based on the hyper paramters in params trained on train_data.\n",
        "    Compute validation scores from the validation_data.\n",
        "    Append the hyperparams and scores to a .csv file in your Google Drive.\n",
        "    Silent: a boolean if you want to see a CORR score for this call\n",
        "  \"\"\"\n",
        "  start_time = datetime.datetime.now()\n",
        "  my_model = train_LGBMRegressor(params=params, train_data=train_data)\n",
        "  my_predictions = my_model.predict(validation_data[FEATURES])\n",
        "  valid_df = valid4score(validation_data, my_predictions, load_example=False, save=False)\n",
        "  my_scores = score_summary(valid_df)\n",
        "  my_total_time = (datetime.datetime.now() - start_time).total_seconds() \n",
        "  my_model_stats = ModelStats(model=my_model, scores=my_scores, total_time=my_total_time)\n",
        "  my_model_stats.save_hyperparams_scores_to_google_drive_tabular()\n",
        "  print(round(my_model_stats.scores['correlation'], 4))\n",
        "\n",
        "def load_saved_params():\n",
        "  return pd.read_csv(PATH_TO_SAVE_SCORES)\n",
        "\n",
        "\n",
        "def create_score_summary(model):\n",
        "  my_predictions = model.predict(VALIDATION_DATA[FEATURES])\n",
        "  valid_df = valid4score(VALIDATION_DATA, my_predictions, load_example=False, save=False)\n",
        "  return score_summary(valid_df)\n",
        "\n",
        "\n",
        "def generate_param_set():\n",
        "  \"\"\"\n",
        "  Create a set of hyper parameters to test on the validation data.  \n",
        "  Returns a list of dictionaries\n",
        "  \"\"\"\n",
        "  param_set=[]\n",
        "  for i in range(20):\n",
        "      for n_estimators in range(2500,4500,100):\n",
        "          param_set.append({\n",
        "                'n_estimators': n_estimators,\n",
        "                'objective': 'regression',\n",
        "                'boosting_type': 'gbdt',\n",
        "                'max_depth': 4,\n",
        "                'learning_rate': round(np.random.uniform(.02,.035),3),\n",
        "                'feature_fraction': round(np.random.uniform(0.2,.3),3), \n",
        "                'seed': 42 # exhaustive study has proven this to be the best possible seed. (joke)\n",
        "                  })\n",
        "  \n",
        "  return param_set\n",
        "\n",
        "\n",
        "def explore_lgbm():\n",
        "  \"\"\"\n",
        "    The main function to explore the hyper parameter space on the current data.\n",
        "  \"\"\"\n",
        "  param_set = generate_param_set()\n",
        "  for index, param in enumerate(param_set):\n",
        "    train_validate_store(param,TRAINING_DATA,VALIDATION_DATA, silent=False)\n",
        "    print(f'Completed {index} models')\n",
        "  df = load_saved_params()\n",
        "  print(df.loc[:,['correlation', 'corr_sharpe']].tail(3))\n",
        "\n",
        "\n",
        " "
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_x1Van5HwCHc"
      },
      "source": [
        "## Custom Loss functions\n",
        "\n",
        "Broad Goals:\n",
        "\n",
        "1. Increase Sharpe. (currently ~.9\n",
        "2. Decrease Validation CORR standard Deviation ( currently 0.0265)\n",
        "3. Decrease Feature exposure (currently .26 )\n",
        "\n",
        "\n",
        "\n",
        "What an abstract class would look like\n",
        "\n",
        "abstract class custom_train_loss_eval_function(y_true: np.Array, y_pred: np.Array):\n",
        "\n",
        "# it seems like there are 50k examples where the errror is always extreme You should figure out if there are any patterns there"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ym0A8ZCa0Esr"
      },
      "source": [
        "# source: https://towardsdatascience.com/custom-loss-functions-for-gradient-boosting-f79c1b40466d | April 2021\n",
        "# and https://github.com/manifoldai/mf-eng-public/blob/master/notebooks/custom_loss_lightgbm.ipynb | April 2021\n",
        "\n",
        "\n",
        "# my local copy of the notebook I am using as a tutorial https://colab.research.google.com/drive/1GZjZw3uJ3dyb_QhQZf0U7HjMKyznFp-B#scrollTo=JfAlK75Q4MlJ\n",
        "\n",
        "\n",
        "# This makes overestimates much more expensive than underestimates. \n",
        "\n",
        "\n",
        "# you should set up an inheritace strucure for the absract class of training_evaluators\n",
        "#I expect this to just make the model worse but is to make sure I can do it before testign more\n",
        "def custom_asymmetric_train(y_true, y_pred):\n",
        "  scaler = 1\n",
        "  residual = (y_true - y_pred).astype(\"float\")\n",
        "  grad = np.where(residual<0, -2*scaler*residual, -2*residual)\n",
        "  hess = np.where(residual<0, 2*scaler, 2.0)\n",
        "  return grad, hess\n",
        "\n",
        "def custom_bias_against_extreme_train(y_true, y_pred):\n",
        "  \"\"\"\n",
        "  if the squared differnce is more thanEXTREME_THRESHOLD\n",
        "  incrase the gradient and hessian by the scaler.\n",
        "\n",
        "  Just theory but very interesting. Right now this is just worse than otherwise\n",
        "\n",
        "\n",
        "  # each round you can print the composition of errors\n",
        "  \"\"\"\n",
        "  EXTREME_THRESHOLD = .1 # constant for what \"extremeness\" means\n",
        "  scaler =1\n",
        "  residual = (y_true - y_pred).astype(\"float\")\n",
        "  grad = np.where(residual**2 > EXTREME_THRESHOLD, -2*scaler*residual, -2*residual)\n",
        "  hess = np.where(residual**2 > EXTREME_THRESHOLD, 2.1*scaler, 2.0)\n",
        "  error_targets = y_true[(residual**2 > EXTREME_THRESHOLD)]\n",
        "  print(f'Average squared residual {np.average(residual**2)} count of extreme condition {(residual**2 > EXTREME_THRESHOLD).sum()} Error targets avg{error_targets.mean()} ', end='') # You can use this to see the residual o ver \n",
        "  print(f'Error targets Mean{error_targets.mean()}Error targets mode{error_targets.mode()}')\n",
        "  #print(f'Describe error targets {error_targets.value_counts().to_dict()}')\n",
        "  \n",
        "\n",
        "  return grad, hess\n",
        "\n",
        "def create_custom_loss_model():\n",
        "  \"\"\"\n",
        "    Train a lightGBM model with a custom loss function\n",
        "    # figure out the syntax to pass this function another function in python\n",
        "  \"\"\"\n",
        "  custom_loss_model = lightgbm.LGBMRegressor(random_state=33)\n",
        "  custom_loss_model.set_params(**{'n_estimators':50,'objective': custom_bias_against_extreme_train})\n",
        "  # you need to include some ways to do early stopping \n",
        "  custom_loss_model.fit(\n",
        "      TRAINING_DATA[FEATURES],\n",
        "      TRAINING_DATA[TARGET],\n",
        "  )\n",
        "  return custom_loss_model\n"
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WIkAxCho7O4T",
        "outputId": "23235c1f-633b-4d31-95fb-b5cf246e3974"
      },
      "source": [
        "%%time\n",
        "custom_loss_model = create_custom_loss_model()\n",
        "custom_scores = create_score_summary(custom_loss_model)\n",
        "print(custom_scores)"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average squared residual 0.29984550864075504 count of extreme condition 376739 Error targets avg0.5995908307873621 Error targets Mean0.5995908307873621Error targets mode0    0.5\n",
            "dtype: float64\n",
            "Average squared residual 0.25396070407256827 count of extreme condition 376739 Error targets avg0.5995908307873621 Error targets Mean0.5995908307873621Error targets mode0    0.5\n",
            "dtype: float64\n",
            "Average squared residual 0.21649164076890065 count of extreme condition 376739 Error targets avg0.5995908307873621 Error targets Mean0.5995908307873621Error targets mode0    0.5\n",
            "dtype: float64\n",
            "Average squared residual 0.1858969072416084 count of extreme condition 376739 Error targets avg0.5995908307873621 Error targets Mean0.5995908307873621Error targets mode0    0.5\n",
            "dtype: float64\n",
            "Average squared residual 0.16091265490778647 count of extreme condition 376727 Error targets avg0.5995940030844616 Error targets Mean0.5995940030844616Error targets mode0    0.5\n",
            "dtype: float64\n",
            "Average squared residual 0.14050966509386004 count of extreme condition 125079 Error targets avg0.7999684199585861 Error targets Mean0.7999684199585861Error targets mode0    0.75\n",
            "dtype: float64\n",
            "Average squared residual 0.12345708189904017 count of extreme condition 125062 Error targets avg0.8000091954390622 Error targets Mean0.8000091954390622Error targets mode0    0.75\n",
            "dtype: float64\n",
            "Average squared residual 0.1096056399840415 count of extreme condition 125062 Error targets avg0.8000091954390622 Error targets Mean0.8000091954390622Error targets mode0    0.75\n",
            "dtype: float64\n",
            "Average squared residual 0.09835171385813564 count of extreme condition 125062 Error targets avg0.8000091954390622 Error targets Mean0.8000091954390622Error targets mode0    0.75\n",
            "dtype: float64\n",
            "Average squared residual 0.08920987935593414 count of extreme condition 125075 Error targets avg0.799926044373376 Error targets Mean0.799926044373376Error targets mode0    0.75\n",
            "dtype: float64\n",
            "Average squared residual 0.0817822887332887 count of extreme condition 145944 Error targets avg0.6855420572274297 Error targets Mean0.6855420572274297Error targets mode0    0.75\n",
            "dtype: float64\n",
            "Average squared residual 0.07575890574810125 count of extreme condition 149910 Error targets avg0.6674054432659596 Error targets Mean0.6674054432659596Error targets mode0    0.75\n",
            "dtype: float64\n",
            "Average squared residual 0.07086358803653724 count of extreme condition 150055 Error targets avg0.6667605211422478 Error targets Mean0.6667605211422478Error targets mode0    0.75\n",
            "dtype: float64\n",
            "Average squared residual 0.06688395040862538 count of extreme condition 150075 Error targets avg0.666671664167916 Error targets Mean0.666671664167916Error targets mode0    0.75\n",
            "dtype: float64\n",
            "Average squared residual 0.06364797564111269 count of extreme condition 150077 Error targets avg0.6666627797730499 Error targets Mean0.6666627797730499Error targets mode0    0.75\n",
            "dtype: float64\n",
            "Average squared residual 0.061016784522871605 count of extreme condition 150078 Error targets avg0.6666583376644145 Error targets Mean0.6666583376644145Error targets mode0    0.75\n",
            "dtype: float64\n",
            "Average squared residual 0.05887608989050279 count of extreme condition 150071 Error targets avg0.6666544502268926 Error targets Mean0.6666544502268926Error targets mode0    0.75\n",
            "dtype: float64\n",
            "Average squared residual 0.057134709477885764 count of extreme condition 149978 Error targets avg0.6666027684060328 Error targets Mean0.6666027684060328Error targets mode0    0.75\n",
            "dtype: float64\n",
            "Average squared residual 0.055717800349252826 count of extreme condition 147352 Error targets avg0.6651165236983549 Error targets Mean0.6651165236983549Error targets mode0    0.75\n",
            "dtype: float64\n",
            "Average squared residual 0.05456529364117749 count of extreme condition 123852 Error targets avg0.6490105125472337 Error targets Mean0.6490105125472337Error targets mode0    0.75\n",
            "dtype: float64\n",
            "Average squared residual 0.053623172753345896 count of extreme condition 76666 Error targets avg0.5868540161218794 Error targets Mean0.5868540161218794Error targets mode0    0.75\n",
            "dtype: float64\n",
            "Average squared residual 0.05285411218229658 count of extreme condition 54798 Error targets avg0.5217480564984124 Error targets Mean0.5217480564984124Error targets mode0    1.0\n",
            "dtype: float64\n",
            "Average squared residual 0.05222484409744088 count of extreme condition 51216 Error targets avg0.5057843252108716 Error targets Mean0.5057843252108716Error targets mode0    1.0\n",
            "dtype: float64\n",
            "Average squared residual 0.05171344049701986 count of extreme condition 50435 Error targets avg0.5020025775750967 Error targets Mean0.5020025775750967Error targets mode0    1.0\n",
            "dtype: float64\n",
            "Average squared residual 0.05129731646454873 count of extreme condition 50191 Error targets avg0.5007969556294953 Error targets Mean0.5007969556294953Error targets mode0    1.0\n",
            "dtype: float64\n",
            "Average squared residual 0.05095858424141503 count of extreme condition 50105 Error targets avg0.5003692246282806 Error targets Mean0.5003692246282806Error targets mode0    1.0\n",
            "dtype: float64\n",
            "Average squared residual 0.05068160092579603 count of extreme condition 50075 Error targets avg0.5002196704942586 Error targets Mean0.5002196704942586Error targets mode0    1.0\n",
            "dtype: float64\n",
            "Average squared residual 0.05045514132414675 count of extreme condition 50053 Error targets avg0.5001098835234651 Error targets Mean0.5001098835234651Error targets mode0    1.0\n",
            "dtype: float64\n",
            "Average squared residual 0.05026997147330064 count of extreme condition 50047 Error targets avg0.5000799248706216 Error targets Mean0.5000799248706216Error targets mode0    1.0\n",
            "dtype: float64\n",
            "Average squared residual 0.050119101174178324 count of extreme condition 50044 Error targets avg0.5000649428502918 Error targets Mean0.5000649428502918Error targets mode0    1.0\n",
            "dtype: float64\n",
            "Average squared residual 0.04999359405183766 count of extreme condition 50042 Error targets avg0.5000549538387754 Error targets Mean0.5000549538387754Error targets mode0    1.0\n",
            "dtype: float64\n",
            "Average squared residual 0.04989149813095971 count of extreme condition 50039 Error targets avg0.500039968824317 Error targets Mean0.500039968824317Error targets mode0    1.0\n",
            "dtype: float64\n",
            "Average squared residual 0.049806422376881945 count of extreme condition 50037 Error targets avg0.5000299778164159 Error targets Mean0.5000299778164159Error targets mode0    1.0\n",
            "dtype: float64\n",
            "Average squared residual 0.049734332970526714 count of extreme condition 50036 Error targets avg0.5000249820129506 Error targets Mean0.5000249820129506Error targets mode0    1.0\n",
            "dtype: float64\n",
            "Average squared residual 0.04967546806905155 count of extreme condition 50036 Error targets avg0.5000249820129506 Error targets Mean0.5000249820129506Error targets mode0    1.0\n",
            "dtype: float64\n",
            "Average squared residual 0.04962556568196132 count of extreme condition 50036 Error targets avg0.5000249820129506 Error targets Mean0.5000249820129506Error targets mode0    1.0\n",
            "dtype: float64\n",
            "Average squared residual 0.04958337369091261 count of extreme condition 50035 Error targets avg0.5000199860097931 Error targets Mean0.5000199860097931Error targets mode0    1.0\n",
            "dtype: float64\n",
            "Average squared residual 0.04954885208130163 count of extreme condition 50035 Error targets avg0.5000199860097931 Error targets Mean0.5000199860097931Error targets mode0    1.0\n",
            "dtype: float64\n",
            "Average squared residual 0.0495186710282201 count of extreme condition 50035 Error targets avg0.5000199860097931 Error targets Mean0.5000199860097931Error targets mode0    1.0\n",
            "dtype: float64\n",
            "Average squared residual 0.049492072017115384 count of extreme condition 50035 Error targets avg0.5000199860097931 Error targets Mean0.5000199860097931Error targets mode0    1.0\n",
            "dtype: float64\n",
            "Average squared residual 0.049468728506794894 count of extreme condition 50035 Error targets avg0.5000199860097931 Error targets Mean0.5000199860097931Error targets mode0    1.0\n",
            "dtype: float64\n",
            "Average squared residual 0.049448279489759085 count of extreme condition 50035 Error targets avg0.5000199860097931 Error targets Mean0.5000199860097931Error targets mode0    1.0\n",
            "dtype: float64\n",
            "Average squared residual 0.049431120407217956 count of extreme condition 50034 Error targets avg0.5000149898069313 Error targets Mean0.5000149898069313Error targets mode0    1.0\n",
            "dtype: float64\n",
            "Average squared residual 0.04941412864082765 count of extreme condition 50034 Error targets avg0.5000149898069313 Error targets Mean0.5000149898069313Error targets mode0    1.0\n",
            "dtype: float64\n",
            "Average squared residual 0.04939926262365989 count of extreme condition 50034 Error targets avg0.5000149898069313 Error targets Mean0.5000149898069313Error targets mode0    1.0\n",
            "dtype: float64\n",
            "Average squared residual 0.04938402226609236 count of extreme condition 50034 Error targets avg0.5000149898069313 Error targets Mean0.5000149898069313Error targets mode0    1.0\n",
            "dtype: float64\n",
            "Average squared residual 0.049371140297270535 count of extreme condition 50034 Error targets avg0.5000149898069313 Error targets Mean0.5000149898069313Error targets mode0    1.0\n",
            "dtype: float64\n",
            "Average squared residual 0.04936034733192468 count of extreme condition 50034 Error targets avg0.5000149898069313 Error targets Mean0.5000149898069313Error targets mode0    1.0\n",
            "dtype: float64\n",
            "Average squared residual 0.049349002663526 count of extreme condition 50034 Error targets avg0.5000149898069313 Error targets Mean0.5000149898069313Error targets mode0    1.0\n",
            "dtype: float64\n",
            "Average squared residual 0.049337857928486946 count of extreme condition 50034 Error targets avg0.5000149898069313 Error targets Mean0.5000149898069313Error targets mode0    1.0\n",
            "dtype: float64\n",
            "{'correlation': 0.020557192260545976, 'corr_sharpe': 0.7680188183496088, 'corr_mean': 0.021075241121083776, 'corr_std': 0.027441047820120135, 'max_drawdown': -0.0933748318360259, 'feature_exposure': 0.09172182510000831, 'max_feature_exposure': 0.26155793954762496}\n",
            "CPU times: user 2min 4s, sys: 679 ms, total: 2min 4s\n",
            "Wall time: 45 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RM107nhZF_ur",
        "outputId": "2ec75d34-88d0-46a3-e4ab-4561915a873f"
      },
      "source": [
        ""
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'corr_mean': 0.023158108168321624,\n",
              " 'corr_sharpe': 0.9080158599517091,\n",
              " 'corr_std': 0.025504078937071913,\n",
              " 'correlation': 0.022597734244065044,\n",
              " 'feature_exposure': 0.08754966605287076,\n",
              " 'max_drawdown': -0.05368360371583081,\n",
              " 'max_feature_exposure': 0.2569501926733141}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxjKwzKZLS1b",
        "outputId": "5135f80e-5e9f-444a-f3c4-c881d1e82c37"
      },
      "source": [
        "y_true = VALIDATION_DATA[TARGET]\n",
        "print(y_true.value_counts().to_dict())\n"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{0.5: 68954, 0.75: 27533, 0.25: 27531, 1.0: 6882, 0.0: 6879}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mf0BRd77fZMA"
      },
      "source": [
        "# Debugging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3BUVpxMwQza"
      },
      "source": [
        "def create_default_model():\n",
        "  \"\"\"\n",
        "  Returns the model used for debugging. This is the point of compaison everything needs to beat.\n",
        "  \"\"\"\n",
        "  default_params = {\n",
        "                'n_estimators': 100,\n",
        "                'objective': 'regression',\n",
        "                'boosting_type': 'gbdt',\n",
        "                'max_depth': 4,\n",
        "                'learning_rate': .02,\n",
        "                'feature_fraction': .25, \n",
        "                'seed': 3\n",
        "                  }\n",
        "  model = train_LGBMRegressor(params=default_params, train_data=TRAINING_DATA)\n",
        "  return model"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75ZV54AMzMiE",
        "outputId": "eeeff13c-d413-41cf-fd8d-9db141828e47"
      },
      "source": [
        "default_model = create_default_model()\n",
        "default_scores = create_score_summary(default_model)\n",
        "default_scores\n"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'corr_mean': 0.0206048232453805,\n",
              " 'corr_sharpe': 0.679690914670776,\n",
              " 'corr_std': 0.030314989947101058,\n",
              " 'correlation': 0.020095462039224157,\n",
              " 'feature_exposure': 0.10870392761087945,\n",
              " 'max_drawdown': -0.09063641195482552,\n",
              " 'max_feature_exposure': 0.31296726639023753}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    }
  ]
}