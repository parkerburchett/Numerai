{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7atJJ2fMiJ8"
      },
      "source": [
        "# Experimentation with different loss function and Evaluation metrics. \r\n",
        "\r\n",
        "\r\n",
        "Create a sub evaluation method to count the % of eras that you have >0 corr for. \r\n",
        "\r\n",
        "You might want  explor hyper parm for xgboost as well. and some other out of the box algos\r\n",
        "\r\n",
        "You might want a custom loss function of \"brair score\"  sum(true_outcome - prediction)^2 for all outcome, preiction in df. \r\n",
        "\r\n",
        "Put this in the validation scorring methods. add it to score summary\r\n",
        "\r\n",
        "You can do rank corrilation at each iteration.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "X4WFeO-ZMiKC"
      },
      "outputs": [],
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import os, sys\r\n",
        "import gc\r\n",
        "import pathlib\r\n",
        "import json\r\n",
        "import datetime\r\n",
        "from typing import List, NoReturn, Union, Tuple, Optional, Text, Generic, Callable, Dict\r\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, QuantileTransformer, minmax_scale\r\n",
        "from sklearn.model_selection import KFold, StratifiedKFold, TimeSeriesSplit\r\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, log_loss, mean_squared_error, mean_absolute_error, f1_score\r\n",
        "from scipy.stats import spearmanr # -P I think this is corr. \r\n",
        "import joblib\r\n",
        "import numerapi\r\n",
        "# model\r\n",
        "import lightgbm as lgb\r\n",
        "\r\n",
        "import operator\r\n",
        "\r\n",
        "# visualize\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import matplotlib.style as style\r\n",
        "from matplotlib import pyplot\r\n",
        "from matplotlib.ticker import ScalarFormatter\r\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Y0ELMvF79l3D"
      },
      "outputs": [],
      "source": [
        "on_colab = False\r\n",
        "if on_colab:\r\n",
        "    !pip install numerapi\r\n",
        "    from google.colab import drive\r\n",
        "    drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b364c12-MiKD"
      },
      "source": [
        "## Methods to Gather and Clean Incoming Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xmV-K6xHoqxW"
      },
      "outputs": [],
      "source": [
        "def create_global_variables()-> None:\r\n",
        "  \"\"\"\r\n",
        "    Create all global variables. \r\n",
        "    ROUND_NUMBER,FEATURES,TARGET,\r\n",
        "    TOURNAMENT_DATA,TRAINING_DATA,VALIDATION_DATA\r\n",
        "  \"\"\"\r\n",
        "  try:\r\n",
        "    if HAVE_GATHERED_DATA == FALSE:\r\n",
        "      print('getting training_data')\r\n",
        "      ping_training_data()\r\n",
        "      print('getting tournament data')\r\n",
        "      ping_tournament_data()\r\n",
        "      create_validation_data(df = TOURNAMENT_DATA)\r\n",
        "      print('created valid df')\r\n",
        "      create_global_constants()\r\n",
        "      drop_data_type_columns()\r\n",
        "      HAVE_GATHERED_DATA = True\r\n",
        "      print('finished gathering data')\r\n",
        "  except NameError:\r\n",
        "      print('getting training_data')\r\n",
        "      ping_training_data()\r\n",
        "      print('getting tournament data')\r\n",
        "      ping_tournament_data()\r\n",
        "      create_validation_data(df = TOURNAMENT_DATA)\r\n",
        "      print('created valid df')\r\n",
        "      create_global_constants()\r\n",
        "      drop_data_type_columns()\r\n",
        "      HAVE_GATHERED_DATA = True\r\n",
        "      print('finished gathering data')\r\n",
        "\r\n",
        "def drop_data_type_columns():\r\n",
        "  TRAINING_DATA.drop(columns=[\"data_type\"], inplace=True)\r\n",
        "  VALIDATION_DATA.drop(columns=[\"data_type\"], inplace=True) #\r\n",
        "  TOURNAMENT_DATA.drop(columns=[\"data_type\"], inplace=True)\r\n",
        "\r\n",
        "def ping_training_data():\r\n",
        "  global TRAINING_DATA\r\n",
        "  TRAINING_DATA = read_data('train')\r\n",
        "  \r\n",
        "def ping_tournament_data():\r\n",
        "  global TOURNAMENT_DATA\r\n",
        "  TOURNAMENT_DATA = read_data('tournament')\r\n",
        "\r\n",
        "def create_validation_data(df):\r\n",
        "  global VALIDATION_DATA\r\n",
        "  VALIDATION_DATA  = df[df[\"data_type\"] == \"validation\"].reset_index(drop = True)\r\n",
        "\r\n",
        "def cast_eras_as_int(x): \r\n",
        "    try:\r\n",
        "        return int(x[3:]) # strip the first 3 characters from each era\r\n",
        "    except:\r\n",
        "        return -99\r\n",
        "\r\n",
        "def read_data(data):\r\n",
        "    if data == 'train':\r\n",
        "        df = pd.read_csv('https://numerai-public-datasets.s3-us-west-2.amazonaws.com/latest_numerai_training_data.csv.xz')\r\n",
        "    elif data == 'tournament':\r\n",
        "        df = pd.read_csv('https://numerai-public-datasets.s3-us-west-2.amazonaws.com/latest_numerai_tournament_data.csv.xz')\r\n",
        "        \r\n",
        "    feature_cols = df.columns[df.columns.str.startswith('feature')]\r\n",
        "    mapping = {0.0 : 0, 0.25 : 1, 0.5 : 2, 0.75 : 3, 1.0 : 4}\r\n",
        "\r\n",
        "    for c in feature_cols:\r\n",
        "        df[c] = df[c].map(mapping).astype(np.uint8)\r\n",
        "        \r\n",
        "    df[\"era\"] = df[\"era\"].apply(cast_eras_as_int)\r\n",
        "    return df\r\n",
        "\r\n",
        "\r\n",
        "def create_global_constants() -> None:\r\n",
        "  global TARGET\r\n",
        "  TARGET = get_target_constant(TOURNAMENT_DATA)\r\n",
        "  global FEATURES\r\n",
        "  FEATURES = get_features_constant(TOURNAMENT_DATA)\r\n",
        "  napi = open_api_access()\r\n",
        "  global ROUND_NUMBER\r\n",
        "  ROUND_NUMBER = napi.get_current_round()\r\n",
        "\r\n",
        "\r\n",
        "def get_target_constant(tournament_data: pd.DataFrame):\r\n",
        "  return tournament_data.columns[tournament_data.columns.str.startswith('target')].values.tolist()[0]\r\n",
        "\r\n",
        "\r\n",
        "def get_features_constant(tournament_data: pd.DataFrame):\r\n",
        "  return [column_names for column_names in tournament_data.columns.values.tolist() if 'feature' in column_names]\r\n",
        "\r\n",
        "\r\n",
        "def load_api_creds_into_dict():\r\n",
        "  if on_colab:\r\n",
        "    creds = open('/content/drive/MyDrive/creds.json','r')\r\n",
        "  else:\r\n",
        "    creds = open('creds.json', 'r') \r\n",
        "  api_keys_dict = json.load(creds)\r\n",
        "  creds.close()\r\n",
        "  return api_keys_dict\r\n",
        "\r\n",
        "\r\n",
        "def open_api_access():\r\n",
        "  api_keys_dict = load_api_creds_into_dict()\r\n",
        "  my_secret_key = api_keys_dict['secret_key']\r\n",
        "  my_public_id = api_keys_dict['public_id']\r\n",
        "  napi = numerapi.NumerAPI(secret_key=my_secret_key, public_id=my_public_id)\r\n",
        "  return napi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBuYhojHFcQW"
      },
      "source": [
        "# Get the training and testing data and create the global variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "M3eMSLtXzspp",
        "outputId": "b8d8ce6c-bd92-4c27-a599-5bcc6d1b1a97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "getting training_data\n",
            "getting tournament data\n",
            "created valid df\n",
            "finished gathering data\n"
          ]
        }
      ],
      "source": [
        "create_global_variables() # works with python 64 bit conda\r\n",
        "HAVE_GATHERED_DATA = True \\\r\n",
        "# you really ought to rewrite this to be faster when you are on vs code on your laptop\r\n",
        "# make this work locally by just reading in the data from a .csv in your local drive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4khTMbONzW-"
      },
      "source": [
        "### ModelStats Object\n",
        "\n",
        "1. Stores the Trained Model\n",
        "2. Stores the Hyper Parameters\n",
        "3. Stores the Validation Scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6LCuE_km4Y_3"
      },
      "outputs": [],
      "source": [
        "if on_colab:\r\n",
        "  PATH_TO_SAVE_SCORES = '/content/drive/MyDrive/numerai_hyperparams_scores.csv'\r\n",
        "else:\r\n",
        "  PATH_TO_SAVE_SCORES = ''\r\n",
        "\r\n",
        "class ModelStats():\r\n",
        "  \"\"\"\r\n",
        "  An object that tracks Hyper Parameters, Time Costs and Scores. \r\n",
        "  \"\"\"\r\n",
        "  def __init__(self, model, scores:dict, total_time):\r\n",
        "        self.model = model \r\n",
        "        self.hyperparams = model.get_params() \r\n",
        "        self.scores = scores \r\n",
        "        self.total_time = total_time\r\n",
        "        self.params_scores_df = None \r\n",
        "\r\n",
        "\r\n",
        "  def create_params_scores_df(self):\r\n",
        "    \"\"\"\r\n",
        "    Create a DataFrame Representing the Hyper Parameters and Scores of this model.\r\n",
        "    \"\"\"\r\n",
        "    if self.params_scores_df == None:\r\n",
        "      all_stats_dict = {}\r\n",
        "      all_stats_dict['total_time'] = self.total_time\r\n",
        "      all_stats_dict['round_number'] = ROUND_NUMBER\r\n",
        "      all_stats_dict.update(self.hyperparams) # dict.update(dict) merges two dictionaries\r\n",
        "      all_stats_dict.update(self.scores)\r\n",
        "      DECIMALS = 4 \r\n",
        "      for key in all_stats_dict.keys():\r\n",
        "          try:\r\n",
        "            all_stats_dict[key] = [round(all_stats_dict[key],DECIMALS)]\r\n",
        "          except:\r\n",
        "            all_stats_dict[key] = [all_stats_dict[key]]\r\n",
        "\r\n",
        "      self.params_scores_df = pd.DataFrame.from_dict(all_stats_dict)\r\n",
        "\r\n",
        "  \r\n",
        "  def save_hyperparams_scores_to_google_drive_tabular(self)-> None:\r\n",
        "    \"\"\"\r\n",
        "        Appends this model's scores into your Google Drive with the other scores.\r\n",
        "    \"\"\"\r\n",
        "    self.create_params_scores_df()\r\n",
        "    # try to load that current df into memory\r\n",
        "    disk_df = pd.read_csv(PATH_TO_SAVE_SCORES)\r\n",
        "    #print(f'Read in new saved scores with {disk_df.shape} shape')\r\n",
        "    new_updated_disk_df = merge_dfs_horizontally(disk_df, self.params_scores_df)\r\n",
        "    #print(f'added next line of scores with {new_updated_disk_df.shape} shape')\r\n",
        "    new_updated_disk_df.to_csv(PATH_TO_SAVE_SCORES, index=False)\r\n",
        "    #print('Overwrote the new_updated_disk_df to your Google Drive')\r\n",
        "\r\n",
        "    try:\r\n",
        "      with open(PATH_TO_SAVE_SCORES, 'r') as scores_file:\r\n",
        "          lines = scores_file.readlines()\r\n",
        "          if len(lines) == 0:\r\n",
        "            print(\"the file does not exist. You are good to save your first score df\")\r\n",
        "    except:\r\n",
        "      self.params_scores_df.to_csv(PATH_TO_SAVE_SCORES, index=False)\r\n",
        "      # not exhaustively tested   \r\n",
        "           \r\n",
        "\r\n",
        "def merge_dfs_horizontally(df1 : pd.DataFrame, df2: pd.DataFrame)-> pd.DataFrame:\r\n",
        "  merged_df = pd.concat([df1, df2], axis=0)\r\n",
        "  return merged_df\r\n",
        "\r\n",
        "\r\n",
        "def train_LGBMRegressor(params: dict, train_data): \r\n",
        "  \"\"\"\r\n",
        "  Inputs: a dict of hyper paramaters for the model, \r\n",
        "  train_data: a pd.DataFrame of the Training Data\r\n",
        "\r\n",
        "  Returns a trained model\r\n",
        "  \"\"\"\r\n",
        "  model = lgb.LGBMRegressor(**params) \r\n",
        "  model.fit(train_data[FEATURES], train_data[TARGET])\r\n",
        "  return model\r\n",
        "\r\n",
        "def train_xgboost(train_data): \r\n",
        "  \"\"\"\r\n",
        "    Not deeply tested. Has much higher 10x time costs to train a single modle. \r\n",
        "    with these params it is low end but respectable. but takes about 2 hours to train via google colab\r\n",
        "  \"\"\"\r\n",
        "  model = xgb.XGBRegressor(max_depth=5, learning_rate=0.01, n_estimators=2000, n_jobs=-1, colsample_bytree=0.1)\r\n",
        "  model.fit(train_data[FEATURES], train_data[TARGET])\r\n",
        "  return model\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bWSBkk2_m3h"
      },
      "source": [
        "## Methods to Determine Validation Scores\r\n",
        "\r\n",
        "1. I did not write these. I added the English comments\r\n",
        "source https://www.kaggle.com/code1110/numerai-tournament"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "szXbJM0mMiKJ"
      },
      "outputs": [],
      "source": [
        "# naming conventions\r\n",
        "PREDICTION_NAME = 'prediction'\r\n",
        "TARGET_NAME = TARGET # 'target is the string named 'target'\r\n",
        "# EXAMPLE_PRED = 'example_prediction'\r\n",
        "\r\n",
        "# ---------------------------\r\n",
        "# Functions\r\n",
        "# ---------------------------\r\n",
        "def valid4score(valid : pd.DataFrame, pred : np.ndarray, load_example: bool=True, save : bool=False) -> pd.DataFrame:\r\n",
        "    \"\"\"\r\n",
        "    Generate new valid pandas dataframe for computing scores\r\n",
        "    \r\n",
        "    :INPUT:\r\n",
        "    - valid : pd.DataFrame extracted from tournament data (data_type='validation')\r\n",
        "    \r\n",
        "    \"\"\"\r\n",
        "    valid_df = valid.copy() # the validation dataframe you use this to test the CORR and other values\r\n",
        "\r\n",
        "    # Your model creates an array of floats [0,1] rank method converst them in a list of ints. \r\n",
        "\r\n",
        "    # your lis tof ints is then compared to their list of ints. \r\n",
        "    valid_df['prediction'] = pd.Series(pred).rank(pct=True, method=\"first\") # pred is the array of predictions your model creates for the set of validation vectors.  \r\n",
        "    # I am unsure if this preds is a float only only between 0,1,2,3,4. \r\n",
        "    valid_df.rename(columns={TARGET: 'target'}, inplace=True)\r\n",
        "    \r\n",
        "    # I don't know what the load example boolean is. I think you can use this to save predictions.\r\n",
        "    if load_example:\r\n",
        "        valid_df[EXAMPLE_PRED] = pd.read_csv(EXP_DIR + 'valid_df.csv')['prediction'].values\r\n",
        "    \r\n",
        "    if save==True:\r\n",
        "        valid_df.to_csv(OUTPUT_DIR + 'valid_df.csv', index=False)\r\n",
        "        print('Validation dataframe saved!')\r\n",
        "    \r\n",
        "    return valid_df\r\n",
        "\r\n",
        "def compute_corr(valid_df : pd.DataFrame):\r\n",
        "    \"\"\"\r\n",
        "    Compute rank correlation\r\n",
        "\r\n",
        "    THIS IS WHAT YOU ARE PRIMARILY PAID ON \r\n",
        "    \r\n",
        "    :INPUT:\r\n",
        "    - valid_df : pd.DataFrame where at least 2 columns ('prediction' & 'target') exist\r\n",
        "    \r\n",
        "    \"\"\"\r\n",
        "    # this uses Person Correilation. \r\n",
        "    # I You are paid on spearman corrilation. That is where the ratio of change is important not the raw amount of change\r\n",
        "    # see: https://support.minitab.com/en-us/minitab-express/1/help-and-how-to/modeling-statistics/regression/supporting-topics/basics/a-comparison-of-the-pearson-and-spearman-correlation-methods/\r\n",
        "    return np.corrcoef(valid_df[\"target\"], valid_df['prediction'])[0, 1] # valid_df['prediction'] is integer and ranks.\r\n",
        "\r\n",
        "def compute_max_drawdown(validation_correlations : pd.Series):\r\n",
        "    \"\"\"\r\n",
        "    Compute max drawdown\r\n",
        "    \r\n",
        "    :INPUT:\r\n",
        "    - validation_correaltions : pd.Series\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    rolling_max = (validation_correlations + 1).cumprod().rolling(window=100, min_periods=1).max()\r\n",
        "    daily_value = (validation_correlations + 1).cumprod()\r\n",
        "    max_drawdown = -(rolling_max - daily_value).max()\r\n",
        "    \r\n",
        "    return max_drawdown\r\n",
        "\r\n",
        "def compute_val_corr(valid_df : pd.DataFrame):\r\n",
        "    \"\"\"\r\n",
        "    Compute rank correlation for valid periods\r\n",
        "    \r\n",
        "    :INPUT:\r\n",
        "    - valid_df : pd.DataFrame where at least 2 columns ('prediction' & 'target') exist\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    # all validation\r\n",
        "    correlation = compute_corr(valid_df)\r\n",
        "    #print(\"rank corr = {:.4f}\".format(correlation))\r\n",
        "    return correlation\r\n",
        "    \r\n",
        "def compute_val_sharpe(valid_df : pd.DataFrame):\r\n",
        "    \"\"\"\r\n",
        "    Compute sharpe ratio for valid periods\r\n",
        "    \r\n",
        "    :INPUT:\r\n",
        "    - valid_df : pd.DataFrame where at least 2 columns ('prediction' & 'target') exist\r\n",
        "    \"\"\"\r\n",
        "    # all validation\r\n",
        "    d = valid_df.groupby('era')[['target', 'prediction']].corr().iloc[0::2,-1].reset_index()\r\n",
        "    me = d['prediction'].mean()\r\n",
        "    sd = d['prediction'].std()\r\n",
        "    max_drawdown = compute_max_drawdown(d['prediction'])\r\n",
        "    #print('sharpe ratio = {:.4f}, corr mean = {:.4f}, corr std = {:.4f}, max drawdown = {:.4f}'.format(me / sd, me, sd, max_drawdown))\r\n",
        "    \r\n",
        "    return me / sd, me, sd, max_drawdown\r\n",
        "    \r\n",
        "def feature_exposures(valid_df : pd.DataFrame):\r\n",
        "    \"\"\"\r\n",
        "    Compute feature exposure\r\n",
        "    \r\n",
        "    :INPUT:\r\n",
        "    - valid_df : pd.DataFrame where at least 2 columns ('prediction' & 'target') exist\r\n",
        "    \"\"\"\r\n",
        "    feature_names = [f for f in valid_df.columns\r\n",
        "                     if f.startswith(\"feature\")]\r\n",
        "    exposures = []\r\n",
        "    for f in feature_names:\r\n",
        "        fe = spearmanr(valid_df['prediction'], valid_df[f])[0]\r\n",
        "        exposures.append(fe)\r\n",
        "    return np.array(exposures)\r\n",
        "\r\n",
        "def max_feature_exposure(fe : np.ndarray):\r\n",
        "    return np.max(np.abs(fe))\r\n",
        "\r\n",
        "def feature_exposure(fe : np.ndarray):\r\n",
        "    return np.sqrt(np.mean(np.square(fe)))\r\n",
        "\r\n",
        "def compute_val_feature_exposure(valid_df : pd.DataFrame):\r\n",
        "    \"\"\"\r\n",
        "    Compute feature exposure for valid periods\r\n",
        "    \r\n",
        "    :INPUT:\r\n",
        "    - valid_df : pd.DataFrame where at least 2 columns ('prediction' & 'target') exist\r\n",
        "    \"\"\"\r\n",
        "    # all validation\r\n",
        "    fe = feature_exposures(valid_df)\r\n",
        "    fe1, fe2 = feature_exposure(fe), max_feature_exposure(fe)\r\n",
        "    #print('feature exposure = {:.4f}, max feature exposure = {:.4f}'.format(fe1, fe2))\r\n",
        "     \r\n",
        "    return fe1, fe2\r\n",
        "\r\n",
        "# to neutralize a column in a df by many other columns\r\n",
        "#         I have no idea what this method does. -P. need to read about it and write up a link to it. \r\n",
        "def neutralize(df, columns, by, proportion=1.0):\r\n",
        "    scores = df.loc[:, columns] # socres is all the rows and only certain columns\r\n",
        "    exposures = df[by].values\r\n",
        "\r\n",
        "    # constant column to make sure the series is completely neutral to exposures\r\n",
        "    exposures = np.hstack(\r\n",
        "        (exposures,\r\n",
        "         np.asarray(np.mean(scores)) * np.ones(len(exposures)).reshape(-1, 1)))\r\n",
        "\r\n",
        "    scores = scores - proportion * exposures.dot(\r\n",
        "        np.linalg.pinv(exposures).dot(scores))\r\n",
        "    return scores / scores.std()\r\n",
        "\r\n",
        "\r\n",
        "# to neutralize any series by any other series\r\n",
        "def neutralize_series(series, by, proportion=1.0):\r\n",
        "    scores = series.values.reshape(-1, 1)\r\n",
        "    exposures = by.values.reshape(-1, 1)\r\n",
        "\r\n",
        "    # this line makes series neutral to a constant column so that it's centered and for sure gets corr 0 with exposures\r\n",
        "    exposures = np.hstack(\r\n",
        "        (exposures,\r\n",
        "         np.array([np.mean(series)] * len(exposures)).reshape(-1, 1)))\r\n",
        "\r\n",
        "    correction = proportion * (exposures.dot(\r\n",
        "        np.linalg.lstsq(exposures, scores, rcond=None)[0]))\r\n",
        "    corrected_scores = scores - correction\r\n",
        "    neutralized = pd.Series(corrected_scores.ravel(), index=series.index)\r\n",
        "    return neutralized\r\n",
        "\r\n",
        "\r\n",
        "def unif(df):\r\n",
        "    x = (df.rank(method=\"first\") - 0.5) / len(df)\r\n",
        "    return pd.Series(x, index=df.index)\r\n",
        "\r\n",
        "def get_feature_neutral_mean(df):\r\n",
        "    feature_cols = [c for c in df.columns if c.startswith(\"feature\")]\r\n",
        "    df.loc[:, \"neutral_sub\"] = neutralize(df, [PREDICTION_NAME],\r\n",
        "                                          feature_cols)[PREDICTION_NAME]\r\n",
        "    scores = df.groupby(\"era\").apply(\r\n",
        "        lambda x: np.corrcoef(x[\"neutral_sub\"].rank(pct=True, method=\"first\"), x[TARGET_NAME])).mean()\r\n",
        "    return np.mean(scores)\r\n",
        "\r\n",
        "def compute_val_mmc(valid_df : pd.DataFrame):    \r\n",
        "    # MMC over validation\r\n",
        "    mmc_scores = []\r\n",
        "    corr_scores = []\r\n",
        "    for _, x in valid_df.groupby(\"era\"):\r\n",
        "        series = neutralize_series(pd.Series(unif(x[PREDICTION_NAME])),\r\n",
        "                                   pd.Series(unif(x[EXAMPLE_PRED])))\r\n",
        "        mmc_scores.append(np.cov(series, x[TARGET_NAME])[0, 1] / (0.29 ** 2)) # I have no idea what htis line does (0.29 ** 2)\r\n",
        "        corr_scores.append(np.corrcoef(unif(x[PREDICTION_NAME]).rank(pct=True, method=\"first\"), x[TARGET_NAME]))\r\n",
        "\r\n",
        "    val_mmc_mean = np.mean(mmc_scores)\r\n",
        "    val_mmc_std = np.std(mmc_scores)\r\n",
        "    val_mmc_sharpe = val_mmc_mean / val_mmc_std\r\n",
        "    corr_plus_mmcs = [c + m for c, m in zip(corr_scores, mmc_scores)]\r\n",
        "    corr_plus_mmc_sharpe = np.mean(corr_plus_mmcs) / np.std(corr_plus_mmcs)\r\n",
        "    corr_plus_mmc_mean = np.mean(corr_plus_mmcs)\r\n",
        "\r\n",
        "    #print(\"MMC Mean = {:.6f}, MMC Std = {:.6f}, CORR+MMC Sharpe = {:.4f}\".format(val_mmc_mean, val_mmc_std, corr_plus_mmc_sharpe))\r\n",
        "\r\n",
        "    # Check correlation with example predictions\r\n",
        "    corr_with_example_preds = np.corrcoef(valid_df[EXAMPLE_PRED].rank(pct=True, method=\"first\"),\r\n",
        "                                          valid_df[PREDICTION_NAME].rank(pct=True, method=\"first\"))[0, 1]\r\n",
        "    #print(\"Corr with example preds: {:.4f}\".format(corr_with_example_preds))\r\n",
        "    \r\n",
        "    return val_mmc_mean, val_mmc_std, corr_plus_mmc_sharpe, corr_with_example_preds\r\n",
        "\r\n",
        "\r\n",
        "# put compute Positve eras here\r\n",
        "\r\n",
        "\r\n",
        "# this is the main method. The rest are just called interanlly. \r\n",
        "def score_summary(valid_df : pd.DataFrame):\r\n",
        "    score_dict = {}\r\n",
        "    \r\n",
        "    try:\r\n",
        "        score_dict['correlation'] = compute_val_corr(valid_df)\r\n",
        "    except:\r\n",
        "        print('ERR: computing correlation')\r\n",
        "    try:\r\n",
        "        score_dict['corr_sharpe'], score_dict['corr_mean'], score_dict['corr_std'], score_dict['max_drawdown'] = compute_val_sharpe(valid_df)\r\n",
        "    except:\r\n",
        "        print('ERR: computing sharpe')\r\n",
        "    try:\r\n",
        "        score_dict['feature_exposure'], score_dict['max_feature_exposure'] = compute_val_feature_exposure(valid_df)\r\n",
        "    except:\r\n",
        "        print('ERR: computing feature exposure')\r\n",
        "    # try:\r\n",
        "    #     score_dict['mmc_mean'], score_dict['mmc_std'], score_dict['corr_mmc_sharpe'], score_dict['corr_with_example_xgb'] = compute_val_mmc(valid_df)\r\n",
        "    # except:\r\n",
        "    #     print('ERR: computing MMC')\r\n",
        "    \r\n",
        "    return score_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZlvD8U4bv072"
      },
      "outputs": [],
      "source": [
        "def look_at_best_models_so_far():\n",
        "  df = load_saved_params()\n",
        "  filter = df['correlation'] >.02\n",
        "  best = df[filter].sort_values(by ='correlation', ascending=False).head(20)\n",
        "  best.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lBD0oUmBWzx"
      },
      "source": [
        "# Command and Control methods for Exploring Hyper parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "80k6-VBtBWhf"
      },
      "outputs": [],
      "source": [
        "def train_validate_store(params:dict, train_data: pd.DataFrame, validation_data: pd.DataFrame):\r\n",
        "  \"\"\"\r\n",
        "    Create a LGBM model based on the hyper paramters in params trained on train_data.\r\n",
        "    Compute validation scores from the validation_data.\r\n",
        "    Append the hyperparams and scores to a .csv file in your Google Drive.\r\n",
        "    Silent: a boolean if you want to see a CORR score for this call\r\n",
        "  \"\"\"\r\n",
        "  start_time = datetime.datetime.now()\r\n",
        "  my_model = train_LGBMRegressor(params=params, train_data=train_data)\r\n",
        "  my_predictions = my_model.predict(validation_data[FEATURES])\r\n",
        "  valid_df = valid4score(validation_data, my_predictions, load_example=False, save=False)\r\n",
        "  my_scores = score_summary(valid_df)\r\n",
        "  my_total_time = (datetime.datetime.now() - start_time).total_seconds() \r\n",
        "  my_model_stats = ModelStats(model=my_model, scores=my_scores, total_time=my_total_time)\r\n",
        "  my_model_stats.save_hyperparams_scores_to_google_drive_tabular()\r\n",
        "  print(round(my_model_stats.scores['correlation'], 4), end=' ')\r\n",
        "  print(round(my_model_stats.scores['corr_sharpe'], 4))\r\n",
        "\r\n",
        "def load_saved_params():\r\n",
        "  return pd.read_csv(PATH_TO_SAVE_SCORES)\r\n",
        "\r\n",
        "\r\n",
        "def create_score_summary(model):\r\n",
        "  my_predictions = model.predict(VALIDATION_DATA[FEATURES])\r\n",
        "  valid_df = valid4score(VALIDATION_DATA, my_predictions, load_example=False, save=False)\r\n",
        "  return score_summary(valid_df)\r\n",
        "\r\n",
        "\r\n",
        "def generate_param_set():\r\n",
        "  \"\"\"\r\n",
        "  Create a set of hyper parameters to test on the validation data.  \r\n",
        "  Returns a list of dictionaries\r\n",
        "  \"\"\"\r\n",
        "  param_set=[]\r\n",
        "  for i in range(20):\r\n",
        "      for n_estimators in range(2800,4000,100):\r\n",
        "          param_set.append({\r\n",
        "                'n_estimators': n_estimators,\r\n",
        "                'objective': 'regression',\r\n",
        "                'boosting_type': 'gbdt',\r\n",
        "                'max_depth': 4,\r\n",
        "                'learning_rate': round(np.random.uniform(.02,.05),3),\r\n",
        "                'feature_fraction': round(np.random.uniform(0.1,.4),3), \r\n",
        "                'seed': 42 # exhaustive study has proven this to be the best possible seed. (joke)\r\n",
        "                  })\r\n",
        "  \r\n",
        "  return param_set\r\n",
        "\r\n",
        "\r\n",
        "def explore_lgbm():\r\n",
        "  \"\"\"\r\n",
        "    The main function to explore the hyper parameter space on the current data.\r\n",
        "  \"\"\"\r\n",
        "  param_set = generate_param_set()\r\n",
        "  for index, param in enumerate(param_set):\r\n",
        "    train_validate_store(param,TRAINING_DATA,VALIDATION_DATA)\r\n",
        "    print(f'Completed {index} models of {len(param_set)}')\r\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_x1Van5HwCHc"
      },
      "source": [
        "## Custom Loss functions\r\n",
        "\r\n",
        "Broad Goals:\r\n",
        "\r\n",
        "1. Increase Sharpe. (currently ~.9\r\n",
        "2. Decrease Validation CORR standard Deviation ( currently 0.0265)\r\n",
        "3. Decrease Feature exposure (currently .26 )\r\n",
        "\r\n",
        "Make it extra sensative for\r\n",
        "\r\n",
        "create a df from predictions and true value. \r\n",
        "\r\n",
        "Look at the top 5% of that data frame and get some Rank corr Error between them. \r\n",
        "\r\n",
        "You need a better understanding of how rank corr works.\r\n",
        "\r\n",
        "\r\n",
        "What an abstract class would look like\r\n",
        "\r\n",
        "abstract class custom_train_loss_eval_function(y_true: np.Array, y_pred: np.Array):\r\n",
        " my local copy of the notebook I am using as a tutorial https://colab.research.google.com/drive/1GZjZw3uJ3dyb_QhQZf0U7HjMKyznFp-B#scrollTo=JfAlK75Q4MlJ\r\n",
        "# it seems like there are 50k examples where the errror is always extreme You should figure out if there are any patterns there"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Ym0A8ZCa0Esr"
      },
      "outputs": [],
      "source": [
        "# source: https://towardsdatascience.com/custom-loss-functions-for-gradient-boosting-f79c1b40466d | April 2021\r\n",
        "# and https://github.com/manifoldai/mf-eng-public/blob/master/notebooks/custom_loss_lightgbm.ipynb | April 2021\r\n",
        "# This makes overestimates much more expensive than underestimates.\r\n",
        "# you should set up an inheritace strucure for the absract class of training_evaluators\r\n",
        "#I expect this to just make the model worse but is to make sure I can do it before testign more\r\n",
        "def custom_asymmetric_train(y_true, y_pred):\r\n",
        "  scaler = 1\r\n",
        "  residual = (y_true - y_pred).astype(\"float\")\r\n",
        "\r\n",
        "  # if np.random.randint(1,20) % 19 ==0: # only create a histogram on average every 20 rounds\r\n",
        "  #   plt.hist(minmax_scale(y_true), bins=20, alpha=0.7, label='TRUE_preds')\r\n",
        "  #   plt.hist(minmax_scale(y_pred), bins=20, alpha=0.3, label='my_preds')\r\n",
        "  #   print(np.corrcoef(y_true, y_pred)[0][1])\r\n",
        "  #   plt.show()\r\n",
        "  #   plt.clf()\r\n",
        "\r\n",
        "   # see what happens when you do hess is just an array of 1s  # hess can be a default array of 2\r\n",
        " \r\n",
        "  return grad, hess\r\n",
        "\r\n",
        "def custom_loss_1(y_true, y_pred):\r\n",
        "  \"\"\"\r\n",
        "      Need to find out how to pass the target and scaler as hyper parameters of this custom loss function\r\n",
        "      for now I am just \r\n",
        "  \"\"\"\r\n",
        "  target_to_weigh_more = 1\r\n",
        "  target_scaler = 2\r\n",
        "  residual = np.where(y_true == target_to_weigh_more, -1*target_scaler*target_scaler*(y_true - y_pred),(y_true - y_pred)).astype(float) # when target =0 scaler might need to be negative\r\n",
        "  grad = np.where(residual<0, -2*residual, -2*residual)\r\n",
        "  hess = np.where(residual<0, 2, 2) # #hess is just an array of 2s. hess =  derative of gradient with respect to residule\r\n",
        "\r\n",
        "  return grad, hess\r\n",
        "\r\n",
        "\r\n",
        "def custom_loss_0(y_true, y_pred):\r\n",
        "  \"\"\"\r\n",
        "      Need to find out how to pass the target and scaler as hyper parameters of this custom loss function\r\n",
        "      for now I am just \r\n",
        "  \"\"\"\r\n",
        "  target_to_weigh_more = 0\r\n",
        "  target_scaler = 2\r\n",
        "  residual = np.where(y_true == target_to_weigh_more, -1*target_scaler*(y_true - y_pred),(y_true - y_pred)).astype(float) # when target =0 scaler might need to be negative\r\n",
        "  grad = np.where(residual<0, -2*residual, -2*residual)\r\n",
        "  hess = np.where(residual<0, 2, 2) # #hess is just an array of 2s. hess =  derative of gradient with respect to residule\r\n",
        "\r\n",
        "  return grad, hess\r\n",
        "\r\n",
        "def custom_loss_5(y_true, y_pred):\r\n",
        "  \"\"\"\r\n",
        "      Need to find out how to pass the target and scaler as hyper parameters of this custom loss function\r\n",
        "      for now I am just \r\n",
        "  \"\"\"\r\n",
        "  target_to_weigh_more = .5\r\n",
        "  target_scaler = 2\r\n",
        "  residual = np.where(y_true == target_to_weigh_more, -1*target_scaler*(y_true - y_pred),(y_true - y_pred)).astype(float) # when target =0 scaler might need to be negative\r\n",
        "  grad = np.where(residual<0, -2*residual, -2*residual)\r\n",
        "  hess = np.where(residual<0, 2, 2) # #hess is just an array of 2s. hess =  derative of gradient with respect to residule\r\n",
        "\r\n",
        "  return grad, hess\r\n",
        "\r\n",
        "def create_default_model():\r\n",
        "  \"\"\"\r\n",
        "  Returns the model used for debugging. This is the point of compaison everything needs to beat.\r\n",
        "  \"\"\"\r\n",
        "  default_params = {\r\n",
        "                'n_estimators': 100,\r\n",
        "                'objective': 'regression',\r\n",
        "                'seed': 33\r\n",
        "                  }\r\n",
        "  model = train_LGBMRegressor(params=default_params, train_data=TRAINING_DATA)\r\n",
        "  return model\r\n",
        "\r\n",
        "\r\n",
        "def create_custom_loss_model(loss_function=custom_loss_0):\r\n",
        "  \"\"\"\r\n",
        "    Train a lightGBM model with a custom loss function\r\n",
        "    # figure out the syntax to pass this function another function in python\r\n",
        "  \"\"\"\r\n",
        "  custom_loss_model = lgb.LGBMRegressor(random_state=33)\r\n",
        "\r\n",
        "  custom_loss_model.set_params(**{'n_estimators':100,'objective': loss_function}) \r\n",
        "  custom_loss_model.fit(\r\n",
        "      TRAINING_DATA[FEATURES],\r\n",
        "      TRAINING_DATA[TARGET],\r\n",
        "  )\r\n",
        "  return custom_loss_model\r\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "# more validation metrics\r\n",
        "def compute_percent_postive_corr_eras(valid_df: pd.DataFrame) -> float:\r\n",
        "    \"\"\"\r\n",
        "        Compute the % of ears these predictions has postive corrilation for in the Validation Data\r\n",
        "        valid_df: pd.Dataframe with columns era, prediction, and target\r\n",
        "\r\n",
        "        returns: a float for the % of eras with positive corr\r\n",
        "    \"\"\"\r\n",
        "    num_total_eras = len(valid_df['era'].unique())\r\n",
        "    num_positive_eras = 0 \r\n",
        "    for era in list(valid_df['era'].unique()):\r\n",
        "        local_era_targets = np.array(valid_df[valid_df['era'] == era]['target'])\r\n",
        "        local_era_predictions = np.array(valid_df[valid_df['era'] == era]['prediction'])\r\n",
        "        era_corr = np.corrcoef(local_era_targets, local_era_predictions)[0][1]\r\n",
        "        if era_corr >0:\r\n",
        "            num_positive_eras +=1\r\n",
        "    return num_positive_eras / num_total_eras\r\n",
        "\r\n",
        "\r\n",
        "def get_average_for_subset_target(valid_df, target_value) -> dict:\r\n",
        "    \"\"\"\r\n",
        "        Get the averge predicted value for the subset of targets target_value.\r\n",
        "\r\n",
        "        eg target_value =.5 will get the average prediction for all the known targets where the true target is .5\r\n",
        "    \"\"\"\r\n",
        "    valid_df_only_certain_targets = valid_df[valid_df['target'] == target_value][['target', 'prediction']]\r\n",
        "    average_predicted_value =  np.average(valid_df_only_certain_targets['prediction'])\r\n",
        "    return {f'target_average_predicted_value_{target_value}': np.round(average_predicted_value,4)}\r\n",
        "    \r\n",
        "\r\n",
        "def create_average_target_dict(valid_df) -> dict:\r\n",
        "    \"\"\"\r\n",
        "        Create a dictionary of the target_### : average residule on the validation data\r\n",
        "            \r\n",
        "            example:\r\n",
        "            {'target_0': 0.49989702428757804,\r\n",
        "            'target_0.25': 0.24883607036483255,\r\n",
        "            'target_0.5': -0.007334194185297394,\r\n",
        "            'target_0.75': -0.23571927817417498,\r\n",
        "            'target_1': -0.4788167061124526}\r\n",
        "    \"\"\"\r\n",
        "    target_average_prediction_dict = {}\r\n",
        "    for target in [0,.25,.5,.75,1]:\r\n",
        "        target_average_prediction_dict.update(get_average_for_subset_target(valid_df, target_value=target))\r\n",
        "    return target_average_prediction_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mf0BRd77fZMA"
      },
      "source": [
        "# Debugging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "custom_model_bias_towards_0 = create_custom_loss_model(loss_function=custom_loss_0)\r\n",
        "custom_model_bias_towards_1 = create_custom_loss_model(loss_function=custom_loss_1)\r\n",
        "custom_model_bias_towards_5 = create_custom_loss_model(loss_function=custom_loss_5)\r\n",
        "\r\n",
        "default_model = create_default_model()\r\n",
        "# this cell takes about 90 seconds\r\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "bias_0_valid_df = valid4score(VALIDATION_DATA, custom_model_bias_towards_0.predict(VALIDATION_DATA[FEATURES]), load_example=False, save=False)\r\n",
        "bias_0_scores = score_summary(bias_0_valid_df)\r\n",
        "\r\n",
        "bias_1_valid_df = valid4score(VALIDATION_DATA, custom_model_bias_towards_1.predict(VALIDATION_DATA[FEATURES]), load_example=False, save=False)\r\n",
        "bias_1_scores = score_summary(bias_1_valid_df)\r\n",
        "\r\n",
        "bias_5_valid_df = valid4score(VALIDATION_DATA, custom_model_bias_towards_5.predict(VALIDATION_DATA[FEATURES]), load_example=False, save=False)\r\n",
        "bias_5_scores = score_summary(bias_5_valid_df)\r\n",
        "\r\n",
        "default_valid_df = valid4score(VALIDATION_DATA, default_model.predict(VALIDATION_DATA[FEATURES]), load_example=False, save=False)\r\n",
        "no_bias_scores = score_summary(default_valid_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bias 0 {'correlation': 0.0011544628306690632, 'corr_sharpe': 0.08555215226451973, 'corr_mean': 0.0012102861562157121, 'corr_std': 0.014146764566174953, 'max_drawdown': -0.039629067402218165, 'feature_exposure': 0.1900874994617239, 'max_feature_exposure': 0.8897494754548462}\n",
            "bias 1 {'correlation': 0.0007164402571104154, 'corr_sharpe': 0.03859597028952235, 'corr_mean': 0.0006168931470605618, 'corr_std': 0.01598335635645439, 'max_drawdown': -0.057996785233258796, 'feature_exposure': 0.08305040713481965, 'max_feature_exposure': 0.3064544100714022}\n",
            "bias .5 {'correlation': 0.001557417314575799, 'corr_sharpe': 0.12237092166665317, 'corr_mean': 0.0016500468678291506, 'corr_std': 0.013483978426868371, 'max_drawdown': -0.036179542628768546, 'feature_exposure': 0.1749340990508152, 'max_feature_exposure': 0.946232131290998}\n",
            "No bias {'correlation': 0.022597732406895794, 'corr_sharpe': 0.9080157514385578, 'corr_mean': 0.023158106294257446, 'corr_std': 0.02550407992104581, 'max_drawdown': -0.05368361010493472, 'feature_exposure': 0.08754966659880963, 'max_feature_exposure': 0.25695019525373824}\n"
          ]
        }
      ],
      "source": [
        "print(f'bias 0 {bias_0_scores}')\r\n",
        "print(f'bias 1 {bias_1_scores}')\r\n",
        "print(f'bias .5 {bias_5_scores}')\r\n",
        "print(f'No bias {no_bias_scores}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bais 0\n",
            "('target_average_predicted_value_0', 0.7287)\n",
            "('target_average_predicted_value_0.25', 0.5325)\n",
            "('target_average_predicted_value_0.5', 0.4288)\n",
            "('target_average_predicted_value_0.75', 0.5294)\n",
            "('target_average_predicted_value_1', 0.7376)\n",
            "bais 1\n",
            "('target_average_predicted_value_0', 0.4228)\n",
            "('target_average_predicted_value_0.25', 0.4897)\n",
            "('target_average_predicted_value_0.5', 0.5226)\n",
            "('target_average_predicted_value_0.75', 0.4938)\n",
            "('target_average_predicted_value_1', 0.4166)\n",
            "bais .5\n",
            "('target_average_predicted_value_0', 0.7209)\n",
            "('target_average_predicted_value_0.25', 0.5317)\n",
            "('target_average_predicted_value_0.5', 0.4305)\n",
            "('target_average_predicted_value_0.75', 0.53)\n",
            "('target_average_predicted_value_1', 0.7282)\n",
            "No bias\n",
            "('target_average_predicted_value_0', 0.4925)\n",
            "('target_average_predicted_value_0.25', 0.4956)\n",
            "('target_average_predicted_value_0.5', 0.4957)\n",
            "('target_average_predicted_value_0.75', 0.5133)\n",
            "('target_average_predicted_value_1', 0.5153)\n"
          ]
        }
      ],
      "source": [
        "print('bais 0')\r\n",
        "for i in create_average_target_dict(bias_0_valid_df).items(): \r\n",
        "    print(i)\r\n",
        "\r\n",
        "\r\n",
        "print('bais 1')\r\n",
        "for i in create_average_target_dict(bias_1_valid_df).items(): # when you bias by 0 you get 1 and 0 approachin 0\r\n",
        "    print(i)\r\n",
        "\r\n",
        "\r\n",
        "print('bais .5')\r\n",
        "for i in create_average_target_dict(bias_5_valid_df).items(): # when you bias by 0 you get 1 and 0 approachin 0\r\n",
        "    print(i)\r\n",
        "\r\n",
        "print('No bias')\r\n",
        "for i in create_average_target_dict(default_valid_df).items(): # when you bias by 0 you get 1 and 0 approachin 0\r\n",
        "    print(i)\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Conclusions.\r\n",
        "\r\n",
        "\r\n",
        "When you change the function that determines the residules it does does not exagerate the averge score : target relationship. \r\n",
        "\r\n",
        "The realtionship should be linear. It is bizzare that it makes the predicted values on unseed validation data into a parabola.  Honestly I have no idea why it is doing that.\r\n",
        "\r\n",
        "I want to see what happens when you bias on a differen value such as .25 and .75 and .5.\r\n",
        "\r\n",
        "I have no idea what is going on here but it is surprising and interesting. I want to look at it more closely. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "b364c12-MiKD"
      ],
      "machine_shape": "hm",
      "name": "Refactored Max Datapoints.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.8.3 64-bit (conda)",
      "name": "python383jvsc74a57bd05f55334c0ec2b6b265735cb6e4e448fe59a4ec2646b0eb29ab8c6a2d945064a9"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}