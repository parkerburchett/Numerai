{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Refactored Max Datapoints.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Y4khTMbONzW-",
        "7bWSBkk2_m3h"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parkerburchett/Numerai/blob/Refactor-Max-DataPoints/Refactored_Max_Datapoints.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7atJJ2fMiJ8"
      },
      "source": [
        "# This notebook explores the Hyper Parameter Space of the lightgbm library\n",
        "\n",
        "\n",
        "You might want to find out how corrilataed different seeds of the optimal hyper params are. Then submit 4 versions of it, that are the most un  corrilated. but 3 nmr on each of them\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0iP-UgesMiKB",
        "outputId": "fcfeae6e-501b-4563-bd8e-559e15f49e66"
      },
      "source": [
        "!pip install numerapi\n",
        "import numerapi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting numerapi\n",
            "  Downloading https://files.pythonhosted.org/packages/fd/96/ebdbaff5a2fef49b212e4f40634166f59e45462a768c0136d148f00255c5/numerapi-2.4.5-py3-none-any.whl\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from numerapi) (7.1.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from numerapi) (2.8.1)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from numerapi) (2018.9)\n",
            "Requirement already satisfied: tqdm>=4.29.1 in /usr/local/lib/python3.7/dist-packages (from numerapi) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from numerapi) (2.23.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil->numerapi) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->numerapi) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->numerapi) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->numerapi) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->numerapi) (1.24.3)\n",
            "Installing collected packages: numerapi\n",
            "Successfully installed numerapi-2.4.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4WFeO-ZMiKC"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os, sys\n",
        "import gc\n",
        "import pathlib\n",
        "import json\n",
        "import datetime\n",
        "from typing import List, NoReturn, Union, Tuple, Optional, Text, Generic, Callable, Dict\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, QuantileTransformer\n",
        "from sklearn.model_selection import KFold, StratifiedKFold, TimeSeriesSplit\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, log_loss, mean_squared_error, mean_absolute_error, f1_score\n",
        "from scipy.stats import spearmanr # -P I think this is corr. \n",
        "import joblib\n",
        "\n",
        "# model\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "import operator\n",
        "\n",
        "# visualize\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.style as style\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot\n",
        "from matplotlib.ticker import ScalarFormatter\n",
        "sns.set_context(\"talk\")\n",
        "style.use('seaborn-colorblind')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0ELMvF79l3D"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b364c12-MiKD"
      },
      "source": [
        "## Methods to Gather and Clean Incoming Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmV-K6xHoqxW"
      },
      "source": [
        "def create_global_variables()-> None:\n",
        "  \"\"\"\n",
        "    Create all global variables. \n",
        "    ROUND_NUMBER,FEATURES,TARGET,\n",
        "    TOURNAMENT_DATA,TRAINING_DATA,VALIDATION_DATA\n",
        "  \"\"\"\n",
        "  try:\n",
        "    if HAVE_GATHERED_DATA == FALSE:\n",
        "      ping_training_data()\n",
        "      ping_tournament_data()\n",
        "      create_validation_data(df = TOURNAMENT_DATA)\n",
        "\n",
        "      create_global_constants()\n",
        "      drop_data_type_columns()\n",
        "      HAVE_GATHERED_DATA = True\n",
        "  except NameError:\n",
        "      ping_training_data()\n",
        "      ping_tournament_data()\n",
        "      create_validation_data(df = TOURNAMENT_DATA)\n",
        "      \n",
        "      create_global_constants()\n",
        "      drop_data_type_columns()\n",
        "      HAVE_GATHERED_DATA = True\n",
        "\n",
        "def drop_data_type_columns():\n",
        "  TRAINING_DATA.drop(columns=[\"data_type\"], inplace=True)\n",
        "  VALIDATION_DATA.drop(columns=[\"data_type\"], inplace=True) #\n",
        "  TOURNAMENT_DATA.drop(columns=[\"data_type\"], inplace=True)\n",
        "\n",
        "def ping_training_data():\n",
        "  global TRAINING_DATA\n",
        "  TRAINING_DATA = read_data('train')\n",
        "  \n",
        "def ping_tournament_data():\n",
        "  global TOURNAMENT_DATA\n",
        "  TOURNAMENT_DATA = read_data('tournament')\n",
        "\n",
        "def create_validation_data(df):\n",
        "  global VALIDATION_DATA\n",
        "  VALIDATION_DATA  = df[df[\"data_type\"] == \"validation\"].reset_index(drop = True)\n",
        "\n",
        "def cast_eras_as_int(x): \n",
        "    try:\n",
        "        return int(x[3:]) # strip the first 3 characters from each era\n",
        "    except:\n",
        "        return -99\n",
        "\n",
        "def read_data(data):\n",
        "    if data == 'train':\n",
        "        df = pd.read_csv('https://numerai-public-datasets.s3-us-west-2.amazonaws.com/latest_numerai_training_data.csv.xz')\n",
        "    elif data == 'tournament':\n",
        "        df = pd.read_csv('https://numerai-public-datasets.s3-us-west-2.amazonaws.com/latest_numerai_tournament_data.csv.xz')\n",
        "        \n",
        "    feature_cols = df.columns[df.columns.str.startswith('feature')]\n",
        "    mapping = {0.0 : 0, 0.25 : 1, 0.5 : 2, 0.75 : 3, 1.0 : 4}\n",
        "\n",
        "    for c in feature_cols:\n",
        "        df[c] = df[c].map(mapping).astype(np.uint8)\n",
        "        \n",
        "    df[\"era\"] = df[\"era\"].apply(cast_eras_as_int)\n",
        "    return df\n",
        "\n",
        "\n",
        "def create_global_constants() -> None:\n",
        "  global TARGET\n",
        "  TARGET = get_target_constant(TOURNAMENT_DATA)\n",
        "  global FEATURES\n",
        "  FEATURES = get_features_constant(TOURNAMENT_DATA)\n",
        "  napi = open_api_access()\n",
        "  global ROUND_NUMBER\n",
        "  ROUND_NUMBER = napi.get_current_round()\n",
        "\n",
        "\n",
        "def get_target_constant(tournament_data: pd.DataFrame):\n",
        "  return tournament_data.columns[tournament_data.columns.str.startswith('target')].values.tolist()[0]\n",
        "\n",
        "\n",
        "def get_features_constant(tournament_data: pd.DataFrame):\n",
        "  return [column_names for column_names in tournament_data.columns.values.tolist() if 'feature' in column_names]\n",
        "\n",
        "\n",
        "\n",
        "def load_api_creds_into_dict():\n",
        "  creds  = open('/content/drive/MyDrive/creds.json','r') \n",
        "  api_keys_dict = json.load(creds)\n",
        "  creds.close()\n",
        "  return api_keys_dict\n",
        "\n",
        "\n",
        "def open_api_access():\n",
        "    api_keys_dict = load_api_creds_into_dict()\n",
        "    my_secret_key = api_keys_dict['secret_key']\n",
        "    my_public_id = api_keys_dict['public_id']\n",
        "    napi = numerapi.NumerAPI(secret_key=my_secret_key, public_id=my_public_id)\n",
        "    return napi\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "M3eMSLtXzspp",
        "outputId": "478aa79d-1117-416d-cf66-4e1e179d8a2d"
      },
      "source": [
        "%%time\n",
        "create_global_variables()\n",
        "HAVE_GATHERED_DATA =True\n",
        "\n",
        "\n",
        "## the first time you run this in a session it should take 20 minutes. \n",
        "## the second time it should be near instant."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 5min 12s, sys: 11.5 s, total: 5min 23s\n",
            "Wall time: 5min 28s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "uosAd8sp-UPV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4khTMbONzW-"
      },
      "source": [
        "### ModelStats Object\n",
        "\n",
        "1. Stores the Trained Model\n",
        "2. Stores the Hyper Parameters\n",
        "3. Stores the Validation Scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6LCuE_km4Y_3"
      },
      "source": [
        "class ModelStats():\n",
        "  \"\"\"\n",
        "  An object that tracks Hyper Parameters, Time Costs and Scores. \n",
        "  \"\"\"\n",
        "  def __init__(self, model, scores:dict, total_time):\n",
        "        self.model = model \n",
        "        self.hyperparams = model.get_params() \n",
        "        self.scores = scores \n",
        "        self.total_time = total_time\n",
        "        self.all_stats_dict = None \n",
        "\n",
        "\n",
        "  def create_all_stats_dict(self):\n",
        "    \"\"\"\n",
        "    Create a single dictionary tracking all the relevent statistics.\n",
        "    \"\"\"\n",
        "    if self.all_stats_dict == None:\n",
        "      \n",
        "      all_stats_dict = {}\n",
        "      all_stats_dict['total_time'] = self.total_time\n",
        "      all_stats_dict['round_number'] = ROUND_NUMBER\n",
        "      all_stats_dict.update(self.hyperparams) # dict.update(dict) merges two dictionaries\n",
        "      all_stats_dict.update(self.scores)\n",
        "\n",
        "      DECIMALS = 4 \n",
        "\n",
        "      for key in all_stats_dict.keys():\n",
        "          try:\n",
        "            all_stats_dict[key] = [round(all_stats_dict[key],DECIMALS)]\n",
        "          except:\n",
        "            all_stats_dict[key] = [all_stats_dict[key]]\n",
        "\n",
        "      self.all_stats_dict = all_stats_dict\n",
        "\n",
        "\n",
        "  def headlines(self):\n",
        "    \"\"\"\n",
        "    # Get a subset of scores that are the high level summary of the model\n",
        "    \"\"\"\n",
        "    self.create_all_stats_dict()\n",
        "    summary_dict = {}\n",
        "    summary_dict['correlation'] = self.all_stats_dict['correlation']\n",
        "    summary_dict['corr_sharpe'] = self.all_stats_dict['corr_sharpe']\n",
        "    summary_dict['max_depth'] = self.all_stats_dict['max_depth']\n",
        "    summary_dict['n_estimators'] = self.all_stats_dict['n_estimators']\n",
        "    summary_dict['total_time'] = self.all_stats_dict['total_time']  \n",
        "    summary_dict['num_leaves'] = self.all_stats_dict['num_leaves']\n",
        "    summary_dict['learning_rate'] = self.all_stats_dict['learning_rate']\n",
        "    return summary_dict\n",
        "  \n",
        "\n",
        "  def save_all_dict_to_disk(self):\n",
        "    \"\"\"\n",
        "        Check to see if you already have scores saved. if yes. add these scores. \n",
        "        else create a new file to save scores\n",
        "    \"\"\"\n",
        "    print('here')\n",
        "    self.create_all_stats_dict()\n",
        "    PATH_TO_SAVE_SCORES = '/content/drive/MyDrive/numerai_hyperparams_scores.csv'\n",
        "    all_stats_df = pd.DataFrame.from_dict(my_model_stats.all_stats_dict, orient='index')\n",
        "    print('1')\n",
        "\n",
        "    if file_can_be_converted_to_df(PATH_TO_SAVE_SCORES):\n",
        "      print('2')\n",
        "      scores_already_saved_df = pd.read_csv(PATH_TO_SAVE_SCORES)\n",
        "      print('5')\n",
        "      scores_already_saved_df = pd.concat([scores_already_saved_df, all_stats_df], axis=1)\n",
        "      scores_already_saved_df.to_csv(PATH_TO_SAVE_SCORES, index=False)\n",
        "      print('4')\n",
        "\n",
        "    else:\n",
        "      with open(PATH_TO_SAVE_SCORES,'x'):\n",
        "        pass\n",
        "      all_stats_df.to_csv(PATH_TO_SAVE_SCORES, index=False)\n",
        "\n",
        "def file_can_be_converted_to_df(path: str)-> bool:\n",
        "  try:\n",
        "    df = pd.read_csv(path)\n",
        "    return True\n",
        "  except:\n",
        "    return False\n",
        "\n",
        "\n",
        "def train_LGBMRegressor(params: dict, train_data): # there is not really a clear cell to put this method\n",
        "  \"\"\"\n",
        "  Inputs: a dict of hyper paramaters for the model, \n",
        "  train_data: a pd.DataFrame of the Tiuraining Data\n",
        "\n",
        "  Returns a trained model based on the parmas\n",
        "  \"\"\"\n",
        "  model = lgb.LGBMRegressor(**params) \n",
        "  model.fit(train_data[FEATURES], train_data[TARGET])\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bWSBkk2_m3h"
      },
      "source": [
        "#### Methods to Determine Validation Scores\n",
        "\n",
        "1. I did not write these. I added the English comments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "szXbJM0mMiKJ"
      },
      "source": [
        "# naming conventions\n",
        "PREDICTION_NAME = 'prediction'\n",
        "TARGET_NAME = TARGET # 'target is the string named 'target'\n",
        "# EXAMPLE_PRED = 'example_prediction'\n",
        "\n",
        "# ---------------------------\n",
        "# Functions\n",
        "# ---------------------------\n",
        "def valid4score(valid : pd.DataFrame, pred : np.ndarray, load_example: bool=True, save : bool=False) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Generate new valid pandas dataframe for computing scores\n",
        "    \n",
        "    :INPUT:\n",
        "    - valid : pd.DataFrame extracted from tournament data (data_type='validation')\n",
        "    \n",
        "    \"\"\"\n",
        "    valid_df = valid.copy() # the validation dataframe you use this to test the CORR and other values\n",
        "\n",
        "    # Your model creates an array of floats [0,1] rank method converst them in a list of ints. \n",
        "\n",
        "    # your lis tof ints is then compared to their list of ints. \n",
        "    valid_df['prediction'] = pd.Series(pred).rank(pct=True, method=\"first\") # pred is the array of predictions your model creates for the set of validation vectors.  \n",
        "    # I am unsure if this preds is a float only only between 0,1,2,3,4. \n",
        "    valid_df.rename(columns={TARGET: 'target'}, inplace=True)\n",
        "    \n",
        "    # I don't know what the load example boolean is. I think you can use this to save predictions.\n",
        "    if load_example:\n",
        "        valid_df[EXAMPLE_PRED] = pd.read_csv(EXP_DIR + 'valid_df.csv')['prediction'].values\n",
        "    \n",
        "    if save==True:\n",
        "        valid_df.to_csv(OUTPUT_DIR + 'valid_df.csv', index=False)\n",
        "        print('Validation dataframe saved!')\n",
        "    \n",
        "    return valid_df\n",
        "\n",
        "def compute_corr(valid_df : pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Compute rank correlation\n",
        "\n",
        "    THIS IS WHAT YOU ARE PRIMARILY PAID ON \n",
        "    \n",
        "    :INPUT:\n",
        "    - valid_df : pd.DataFrame where at least 2 columns ('prediction' & 'target') exist\n",
        "    \n",
        "    \"\"\"\n",
        "    # this uses Person Correilation. \n",
        "    # I You are paid on spearman corrilation. That is where the ratio of change is important not the raw amount of change\n",
        "    # see: https://support.minitab.com/en-us/minitab-express/1/help-and-how-to/modeling-statistics/regression/supporting-topics/basics/a-comparison-of-the-pearson-and-spearman-correlation-methods/\n",
        "    return np.corrcoef(valid_df[\"target\"], valid_df['prediction'])[0, 1]\n",
        "\n",
        "def compute_max_drawdown(validation_correlations : pd.Series):\n",
        "    \"\"\"\n",
        "    Compute max drawdown\n",
        "    \n",
        "    :INPUT:\n",
        "    - validation_correaltions : pd.Series\n",
        "    \"\"\"\n",
        "    \n",
        "    rolling_max = (validation_correlations + 1).cumprod().rolling(window=100, min_periods=1).max()\n",
        "    daily_value = (validation_correlations + 1).cumprod()\n",
        "    max_drawdown = -(rolling_max - daily_value).max()\n",
        "    \n",
        "    return max_drawdown\n",
        "\n",
        "def compute_val_corr(valid_df : pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Compute rank correlation for valid periods\n",
        "    \n",
        "    :INPUT:\n",
        "    - valid_df : pd.DataFrame where at least 2 columns ('prediction' & 'target') exist\n",
        "    \"\"\"\n",
        "    \n",
        "    # all validation\n",
        "    correlation = compute_corr(valid_df)\n",
        "    #print(\"rank corr = {:.4f}\".format(correlation))\n",
        "    return correlation\n",
        "    \n",
        "def compute_val_sharpe(valid_df : pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Compute sharpe ratio for valid periods\n",
        "    \n",
        "    :INPUT:\n",
        "    - valid_df : pd.DataFrame where at least 2 columns ('prediction' & 'target') exist\n",
        "    \"\"\"\n",
        "    # all validation\n",
        "    d = valid_df.groupby('era')[['target', 'prediction']].corr().iloc[0::2,-1].reset_index()\n",
        "    me = d['prediction'].mean()\n",
        "    sd = d['prediction'].std()\n",
        "    max_drawdown = compute_max_drawdown(d['prediction'])\n",
        "    #print('sharpe ratio = {:.4f}, corr mean = {:.4f}, corr std = {:.4f}, max drawdown = {:.4f}'.format(me / sd, me, sd, max_drawdown))\n",
        "    \n",
        "    return me / sd, me, sd, max_drawdown\n",
        "    \n",
        "def feature_exposures(valid_df : pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Compute feature exposure\n",
        "    \n",
        "    :INPUT:\n",
        "    - valid_df : pd.DataFrame where at least 2 columns ('prediction' & 'target') exist\n",
        "    \"\"\"\n",
        "    feature_names = [f for f in valid_df.columns\n",
        "                     if f.startswith(\"feature\")]\n",
        "    exposures = []\n",
        "    for f in feature_names:\n",
        "        fe = spearmanr(valid_df['prediction'], valid_df[f])[0]\n",
        "        exposures.append(fe)\n",
        "    return np.array(exposures)\n",
        "\n",
        "def max_feature_exposure(fe : np.ndarray):\n",
        "    return np.max(np.abs(fe))\n",
        "\n",
        "def feature_exposure(fe : np.ndarray):\n",
        "    return np.sqrt(np.mean(np.square(fe)))\n",
        "\n",
        "def compute_val_feature_exposure(valid_df : pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Compute feature exposure for valid periods\n",
        "    \n",
        "    :INPUT:\n",
        "    - valid_df : pd.DataFrame where at least 2 columns ('prediction' & 'target') exist\n",
        "    \"\"\"\n",
        "    # all validation\n",
        "    fe = feature_exposures(valid_df)\n",
        "    fe1, fe2 = feature_exposure(fe), max_feature_exposure(fe)\n",
        "    #print('feature exposure = {:.4f}, max feature exposure = {:.4f}'.format(fe1, fe2))\n",
        "     \n",
        "    return fe1, fe2\n",
        "\n",
        "# to neutralize a column in a df by many other columns\n",
        "#         I have no idea what this method does. -P. need to read about it and write up a link to it. \n",
        "def neutralize(df, columns, by, proportion=1.0):\n",
        "    scores = df.loc[:, columns]\n",
        "    exposures = df[by].values\n",
        "\n",
        "    # constant column to make sure the series is completely neutral to exposures\n",
        "    exposures = np.hstack(\n",
        "        (exposures,\n",
        "         np.asarray(np.mean(scores)) * np.ones(len(exposures)).reshape(-1, 1)))\n",
        "\n",
        "    scores = scores - proportion * exposures.dot(\n",
        "        np.linalg.pinv(exposures).dot(scores))\n",
        "    return scores / scores.std()\n",
        "\n",
        "\n",
        "# to neutralize any series by any other series\n",
        "def neutralize_series(series, by, proportion=1.0):\n",
        "    scores = series.values.reshape(-1, 1)\n",
        "    exposures = by.values.reshape(-1, 1)\n",
        "\n",
        "    # this line makes series neutral to a constant column so that it's centered and for sure gets corr 0 with exposures\n",
        "    exposures = np.hstack(\n",
        "        (exposures,\n",
        "         np.array([np.mean(series)] * len(exposures)).reshape(-1, 1)))\n",
        "\n",
        "    correction = proportion * (exposures.dot(\n",
        "        np.linalg.lstsq(exposures, scores, rcond=None)[0]))\n",
        "    corrected_scores = scores - correction\n",
        "    neutralized = pd.Series(corrected_scores.ravel(), index=series.index)\n",
        "    return neutralized\n",
        "\n",
        "\n",
        "def unif(df):\n",
        "    x = (df.rank(method=\"first\") - 0.5) / len(df)\n",
        "    return pd.Series(x, index=df.index)\n",
        "\n",
        "def get_feature_neutral_mean(df):\n",
        "    feature_cols = [c for c in df.columns if c.startswith(\"feature\")]\n",
        "    df.loc[:, \"neutral_sub\"] = neutralize(df, [PREDICTION_NAME],\n",
        "                                          feature_cols)[PREDICTION_NAME]\n",
        "    scores = df.groupby(\"era\").apply(\n",
        "        lambda x: np.corrcoef(x[\"neutral_sub\"].rank(pct=True, method=\"first\"), x[TARGET_NAME])).mean()\n",
        "    return np.mean(scores)\n",
        "\n",
        "def compute_val_mmc(valid_df : pd.DataFrame):    \n",
        "    # MMC over validation\n",
        "    mmc_scores = []\n",
        "    corr_scores = []\n",
        "    for _, x in valid_df.groupby(\"era\"):\n",
        "        series = neutralize_series(pd.Series(unif(x[PREDICTION_NAME])),\n",
        "                                   pd.Series(unif(x[EXAMPLE_PRED])))\n",
        "        mmc_scores.append(np.cov(series, x[TARGET_NAME])[0, 1] / (0.29 ** 2)) # I have no idea what htis line does (0.29 ** 2)\n",
        "        corr_scores.append(np.corrcoef(unif(x[PREDICTION_NAME]).rank(pct=True, method=\"first\"), x[TARGET_NAME]))\n",
        "\n",
        "    val_mmc_mean = np.mean(mmc_scores)\n",
        "    val_mmc_std = np.std(mmc_scores)\n",
        "    val_mmc_sharpe = val_mmc_mean / val_mmc_std\n",
        "    corr_plus_mmcs = [c + m for c, m in zip(corr_scores, mmc_scores)]\n",
        "    corr_plus_mmc_sharpe = np.mean(corr_plus_mmcs) / np.std(corr_plus_mmcs)\n",
        "    corr_plus_mmc_mean = np.mean(corr_plus_mmcs)\n",
        "\n",
        "    #print(\"MMC Mean = {:.6f}, MMC Std = {:.6f}, CORR+MMC Sharpe = {:.4f}\".format(val_mmc_mean, val_mmc_std, corr_plus_mmc_sharpe))\n",
        "\n",
        "    # Check correlation with example predictions\n",
        "    corr_with_example_preds = np.corrcoef(valid_df[EXAMPLE_PRED].rank(pct=True, method=\"first\"),\n",
        "                                          valid_df[PREDICTION_NAME].rank(pct=True, method=\"first\"))[0, 1]\n",
        "    #print(\"Corr with example preds: {:.4f}\".format(corr_with_example_preds))\n",
        "    \n",
        "    return val_mmc_mean, val_mmc_std, corr_plus_mmc_sharpe, corr_with_example_preds\n",
        "\n",
        "\n",
        "# this is the main method. The rest are just called interanlly. \n",
        "def score_summary(valid_df : pd.DataFrame):\n",
        "    score_dict = {}\n",
        "    \n",
        "    try:\n",
        "        score_dict['correlation'] = compute_val_corr(valid_df)\n",
        "    except:\n",
        "        print('ERR: computing correlation')\n",
        "    try:\n",
        "        score_dict['corr_sharpe'], score_dict['corr_mean'], score_dict['corr_std'], score_dict['max_drawdown'] = compute_val_sharpe(valid_df)\n",
        "    except:\n",
        "        print('ERR: computing sharpe')\n",
        "    try:\n",
        "        score_dict['feature_exposure'], score_dict['max_feature_exposure'] = compute_val_feature_exposure(valid_df)\n",
        "    except:\n",
        "        print('ERR: computing feature exposure')\n",
        "    # try:\n",
        "    #     score_dict['mmc_mean'], score_dict['mmc_std'], score_dict['corr_mmc_sharpe'], score_dict['corr_with_example_xgb'] = compute_val_mmc(valid_df)\n",
        "    # except:\n",
        "    #     print('ERR: computing MMC')\n",
        "    \n",
        "    return score_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lBD0oUmBWzx"
      },
      "source": [
        "### Main to train and track time, hyper parmas and scores."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "80k6-VBtBWhf"
      },
      "source": [
        "def train_validate_store(params, train_data, validation_data, features):\n",
        "  \"\"\"\n",
        "  Train a model. Get Validation Scores. Create ModelStats Object. Save that model Stats object to your Google Drive\n",
        "  \"\"\"\n",
        "  start_time = datetime.datetime.now()\n",
        "  my_model = train_LGBMRegressor(params=params, train_data=train_data)\n",
        "  my_predictions = my_model.predict(validation_data[features])\n",
        "  valid_df = valid4score(validation_data, my_predictions, load_example=False, save=False)\n",
        "  my_scores = score_summary(valid_df)\n",
        "  total_time = (datetime.datetime.now() - start_time).total_seconds() \n",
        "  my_model_stats = ModelStats(model=my_model, scores=my_scores, total_time=total_time)\n",
        "  my_model_stats.create_all_stats_dict()\n",
        "  return my_model_stats\n",
        "\n",
        "def create_DataFrame_from_ModelStats(modelStats_object: ModelStats)-> pd.DataFrame:\n",
        "  df= pd.DataFrame.from_dict(modelStats_object.all_stats_dict)\n",
        "  return df\n",
        "\n",
        "def merge_df_vertically(old_stats_df : pd.DataFrame, new_stats_df: pd.DataFrame)-> pd.DataFrame:\n",
        "  merged_df = pd.concat([old_stats_df, new_stats_df], axis=0)\n",
        "  return merged_df\n",
        "              "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mf0BRd77fZMA"
      },
      "source": [
        "# Debugging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQM5saclfXq6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26dc6375-6dce-4ae9-92f0-ad38831e2aad"
      },
      "source": [
        "%%time\n",
        "param_set=[]\n",
        "for depth in range(3,5):\n",
        "  for learning_rate in range(22,42,2):\n",
        "    for feature_fraction in range(95,120,5):\n",
        "      param_set.append({\n",
        "            'n_estimators': 3000,\n",
        "            'objective': 'regression',\n",
        "            'boosting_type': 'gbdt',\n",
        "            'max_depth': depth,\n",
        "            'learning_rate': learning_rate / 1000,\n",
        "            'feature_fraction': feature_fraction/ 1000, \n",
        "            'seed': 42 \n",
        "              })\n",
        "\n",
        "record_df = None\n",
        "\n",
        "for param in param_set:\n",
        "  my_model_stats = train_validate_store(param, \n",
        "                       train_data=TRAINING_DATA, \n",
        "                       validation_data = VALIDATION_DATA, \n",
        "                       features=FEATURES)\n",
        "  if type(record_df) == None:\n",
        "    record_df = create_DataFrame_from_ModelStats(my_model_stats) # also include a try pd.load(csv, index =false)\n",
        "  else:\n",
        "    new_df = create_DataFrame_from_ModelStats(my_model_stats)\n",
        "    record_df = merge_df_vertically(record_df,new_df)\n",
        "    # overwrite the record_df saved in drive\n",
        "  record_df.to_csv(f'/content/drive/MyDrive/{ROUND_NUMBER}scores.csv', index=False)\n",
        "  \n",
        "  print(record_df.shape) # might also want to print the corr socres"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-17 18:15:51,899 INFO numexpr.utils: NumExpr defaulting to 4 threads.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(1, 31)\n",
            "(2, 31)\n",
            "(3, 31)\n",
            "(4, 31)\n",
            "(5, 31)\n",
            "(6, 31)\n",
            "(7, 31)\n",
            "(8, 31)\n",
            "(9, 31)\n",
            "(10, 31)\n",
            "(11, 31)\n",
            "(12, 31)\n",
            "(13, 31)\n",
            "(14, 31)\n",
            "(15, 31)\n",
            "(16, 31)\n",
            "(17, 31)\n",
            "(18, 31)\n",
            "(19, 31)\n",
            "(20, 31)\n",
            "(21, 31)\n",
            "(22, 31)\n",
            "(23, 31)\n",
            "(24, 31)\n",
            "(25, 31)\n",
            "(26, 31)\n",
            "(27, 31)\n",
            "(28, 31)\n",
            "(29, 31)\n",
            "(30, 31)\n",
            "(31, 31)\n",
            "(32, 31)\n",
            "(33, 31)\n",
            "(34, 31)\n",
            "(35, 31)\n",
            "(36, 31)\n",
            "(37, 31)\n",
            "(38, 31)\n",
            "(39, 31)\n",
            "(40, 31)\n",
            "(41, 31)\n",
            "(42, 31)\n",
            "(43, 31)\n",
            "(44, 31)\n",
            "(45, 31)\n",
            "(46, 31)\n",
            "(47, 31)\n",
            "(48, 31)\n",
            "(49, 31)\n",
            "(50, 31)\n",
            "(51, 31)\n",
            "(52, 31)\n",
            "(53, 31)\n",
            "(54, 31)\n",
            "(55, 31)\n",
            "(56, 31)\n",
            "(57, 31)\n",
            "(58, 31)\n",
            "(59, 31)\n",
            "(60, 31)\n",
            "(61, 31)\n",
            "(62, 31)\n",
            "(63, 31)\n",
            "(64, 31)\n",
            "(65, 31)\n",
            "(66, 31)\n",
            "(67, 31)\n",
            "(68, 31)\n",
            "(69, 31)\n",
            "(70, 31)\n",
            "(71, 31)\n",
            "(72, 31)\n",
            "(73, 31)\n",
            "(74, 31)\n",
            "(75, 31)\n",
            "(76, 31)\n",
            "(77, 31)\n",
            "(78, 31)\n",
            "(79, 31)\n",
            "(80, 31)\n",
            "(81, 31)\n",
            "(82, 31)\n",
            "(83, 31)\n",
            "(84, 31)\n",
            "(85, 31)\n",
            "(86, 31)\n",
            "(87, 31)\n",
            "(88, 31)\n",
            "(89, 31)\n",
            "(90, 31)\n",
            "(91, 31)\n",
            "(92, 31)\n",
            "(93, 31)\n",
            "(94, 31)\n",
            "(95, 31)\n",
            "(96, 31)\n",
            "(97, 31)\n",
            "(98, 31)\n",
            "(99, 31)\n",
            "(100, 31)\n",
            "CPU times: user 12h 54min 42s, sys: 1min 5s, total: 12h 55min 48s\n",
            "Wall time: 3h 33min 52s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZHh0Tsw4CcF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9865fb9c-c735-455a-b601-f0e01a340eca"
      },
      "source": [
        "record_df['correlation']"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    0.0243\n",
              "0    0.0240\n",
              "0    0.0244\n",
              "0    0.0238\n",
              "0    0.0229\n",
              "      ...  \n",
              "0    0.0232\n",
              "0    0.0223\n",
              "0    0.0220\n",
              "0    0.0231\n",
              "0    0.0231\n",
              "Name: correlation, Length: 100, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2tMd34z-F2r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9d74061-149a-433a-df58-d38255db8a4f"
      },
      "source": [
        "def load_saved_params():\n",
        "  df = pd.read_csv(f'/content/drive/MyDrive/{ROUND_NUMBER}scores.csv')\n",
        "  return df\n",
        "\n",
        "\n",
        "df = load_saved_params()\n",
        "df.columns"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['total_time', 'round_number', 'boosting_type', 'class_weight',\n",
              "       'colsample_bytree', 'importance_type', 'learning_rate', 'max_depth',\n",
              "       'min_child_samples', 'min_child_weight', 'min_split_gain',\n",
              "       'n_estimators', 'n_jobs', 'num_leaves', 'objective', 'random_state',\n",
              "       'reg_alpha', 'reg_lambda', 'silent', 'subsample', 'subsample_for_bin',\n",
              "       'subsample_freq', 'feature_fraction', 'seed', 'correlation',\n",
              "       'corr_sharpe', 'corr_mean', 'corr_std', 'max_drawdown',\n",
              "       'feature_exposure', 'max_feature_exposure'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtnkbbrr---F",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        },
        "outputId": "a7b07255-0ac1-4089-880e-04358c2fd8ac"
      },
      "source": [
        "import statsmodels.api as sm\n",
        "\n",
        "def compute_multiple_regression(\n",
        "        df,\n",
        "        indepenent_variables=['max_depth', 'learning_rate', 'feature_fraction'],\n",
        "        dependent_variable='correlation'):\n",
        "    x = df[indepenent_variables]\n",
        "    y = df[dependent_variable]\n",
        "    model = sm.OLS(y, x)\n",
        "    results = model.fit()\n",
        "    return results\n",
        "\n",
        "res = compute_multiple_regression(df)\n",
        "print(res.summary()\n",
        ")\n",
        "\n",
        "df[df.correlation == np.max(df.correlation)]\n",
        "\n",
        "\n",
        "# it looks like the best for round 260 is seed=42, freature, fraction  =.095; n_estimators =3000, depth =4, learnign rate ="
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                 OLS Regression Results                                \n",
            "=======================================================================================\n",
            "Dep. Variable:            correlation   R-squared (uncentered):                   0.995\n",
            "Model:                            OLS   Adj. R-squared (uncentered):              0.995\n",
            "Method:                 Least Squares   F-statistic:                              6766.\n",
            "Date:                Sat, 17 Apr 2021   Prob (F-statistic):                   1.75e-112\n",
            "Time:                        22:04:45   Log-Likelihood:                          498.71\n",
            "No. Observations:                 100   AIC:                                     -991.4\n",
            "Df Residuals:                      97   BIC:                                     -983.6\n",
            "Df Model:                           3                                                  \n",
            "Covariance Type:            nonrobust                                                  \n",
            "====================================================================================\n",
            "                       coef    std err          t      P>|t|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------------\n",
            "max_depth            0.0012      0.000      3.825      0.000       0.001       0.002\n",
            "learning_rate        0.0302      0.028      1.089      0.279      -0.025       0.085\n",
            "feature_fraction     0.1791      0.012     14.697      0.000       0.155       0.203\n",
            "==============================================================================\n",
            "Omnibus:                        7.630   Durbin-Watson:                   1.391\n",
            "Prob(Omnibus):                  0.022   Jarque-Bera (JB):                3.085\n",
            "Skew:                           0.014   Prob(JB):                        0.214\n",
            "Kurtosis:                       2.140   Cond. No.                         604.\n",
            "==============================================================================\n",
            "\n",
            "Warnings:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>total_time</th>\n",
              "      <th>round_number</th>\n",
              "      <th>boosting_type</th>\n",
              "      <th>class_weight</th>\n",
              "      <th>colsample_bytree</th>\n",
              "      <th>importance_type</th>\n",
              "      <th>learning_rate</th>\n",
              "      <th>max_depth</th>\n",
              "      <th>min_child_samples</th>\n",
              "      <th>min_child_weight</th>\n",
              "      <th>min_split_gain</th>\n",
              "      <th>n_estimators</th>\n",
              "      <th>n_jobs</th>\n",
              "      <th>num_leaves</th>\n",
              "      <th>objective</th>\n",
              "      <th>random_state</th>\n",
              "      <th>reg_alpha</th>\n",
              "      <th>reg_lambda</th>\n",
              "      <th>silent</th>\n",
              "      <th>subsample</th>\n",
              "      <th>subsample_for_bin</th>\n",
              "      <th>subsample_freq</th>\n",
              "      <th>feature_fraction</th>\n",
              "      <th>seed</th>\n",
              "      <th>correlation</th>\n",
              "      <th>corr_sharpe</th>\n",
              "      <th>corr_mean</th>\n",
              "      <th>corr_std</th>\n",
              "      <th>max_drawdown</th>\n",
              "      <th>feature_exposure</th>\n",
              "      <th>max_feature_exposure</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>65</th>\n",
              "      <td>136.8332</td>\n",
              "      <td>260</td>\n",
              "      <td>gbdt</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>split</td>\n",
              "      <td>0.028</td>\n",
              "      <td>4</td>\n",
              "      <td>20</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3000</td>\n",
              "      <td>-1</td>\n",
              "      <td>31</td>\n",
              "      <td>regression</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>200000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.095</td>\n",
              "      <td>42</td>\n",
              "      <td>0.0259</td>\n",
              "      <td>0.9844</td>\n",
              "      <td>0.0264</td>\n",
              "      <td>0.0268</td>\n",
              "      <td>-0.0595</td>\n",
              "      <td>0.0716</td>\n",
              "      <td>0.2648</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    total_time  round_number  ... feature_exposure  max_feature_exposure\n",
              "65    136.8332           260  ...           0.0716                0.2648\n",
              "\n",
              "[1 rows x 31 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 788
        },
        "id": "_e9W1f6F2mBy",
        "outputId": "29349ac4-70bd-4fd5-899d-9af3d1c93746"
      },
      "source": [
        "df[df.correlation > 0.024300]"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>total_time</th>\n",
              "      <th>round_number</th>\n",
              "      <th>boosting_type</th>\n",
              "      <th>class_weight</th>\n",
              "      <th>colsample_bytree</th>\n",
              "      <th>importance_type</th>\n",
              "      <th>learning_rate</th>\n",
              "      <th>max_depth</th>\n",
              "      <th>min_child_samples</th>\n",
              "      <th>min_child_weight</th>\n",
              "      <th>min_split_gain</th>\n",
              "      <th>n_estimators</th>\n",
              "      <th>n_jobs</th>\n",
              "      <th>num_leaves</th>\n",
              "      <th>objective</th>\n",
              "      <th>random_state</th>\n",
              "      <th>reg_alpha</th>\n",
              "      <th>reg_lambda</th>\n",
              "      <th>silent</th>\n",
              "      <th>subsample</th>\n",
              "      <th>subsample_for_bin</th>\n",
              "      <th>subsample_freq</th>\n",
              "      <th>feature_fraction</th>\n",
              "      <th>seed</th>\n",
              "      <th>correlation</th>\n",
              "      <th>corr_sharpe</th>\n",
              "      <th>corr_mean</th>\n",
              "      <th>corr_std</th>\n",
              "      <th>max_drawdown</th>\n",
              "      <th>feature_exposure</th>\n",
              "      <th>max_feature_exposure</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>117.0181</td>\n",
              "      <td>260</td>\n",
              "      <td>gbdt</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>split</td>\n",
              "      <td>0.022</td>\n",
              "      <td>3</td>\n",
              "      <td>20</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3000</td>\n",
              "      <td>-1</td>\n",
              "      <td>31</td>\n",
              "      <td>regression</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>200000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.105</td>\n",
              "      <td>42</td>\n",
              "      <td>0.0244</td>\n",
              "      <td>0.9195</td>\n",
              "      <td>0.0248</td>\n",
              "      <td>0.0270</td>\n",
              "      <td>-0.0733</td>\n",
              "      <td>0.0838</td>\n",
              "      <td>0.2535</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>116.1439</td>\n",
              "      <td>260</td>\n",
              "      <td>gbdt</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>split</td>\n",
              "      <td>0.024</td>\n",
              "      <td>3</td>\n",
              "      <td>20</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3000</td>\n",
              "      <td>-1</td>\n",
              "      <td>31</td>\n",
              "      <td>regression</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>200000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.100</td>\n",
              "      <td>42</td>\n",
              "      <td>0.0248</td>\n",
              "      <td>0.9059</td>\n",
              "      <td>0.0253</td>\n",
              "      <td>0.0279</td>\n",
              "      <td>-0.0807</td>\n",
              "      <td>0.0817</td>\n",
              "      <td>0.2522</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>122.2655</td>\n",
              "      <td>260</td>\n",
              "      <td>gbdt</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>split</td>\n",
              "      <td>0.024</td>\n",
              "      <td>3</td>\n",
              "      <td>20</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3000</td>\n",
              "      <td>-1</td>\n",
              "      <td>31</td>\n",
              "      <td>regression</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>200000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.110</td>\n",
              "      <td>42</td>\n",
              "      <td>0.0245</td>\n",
              "      <td>0.9186</td>\n",
              "      <td>0.0250</td>\n",
              "      <td>0.0272</td>\n",
              "      <td>-0.0643</td>\n",
              "      <td>0.0815</td>\n",
              "      <td>0.2533</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>118.0555</td>\n",
              "      <td>260</td>\n",
              "      <td>gbdt</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>split</td>\n",
              "      <td>0.026</td>\n",
              "      <td>3</td>\n",
              "      <td>20</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3000</td>\n",
              "      <td>-1</td>\n",
              "      <td>31</td>\n",
              "      <td>regression</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>200000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.110</td>\n",
              "      <td>42</td>\n",
              "      <td>0.0244</td>\n",
              "      <td>0.8999</td>\n",
              "      <td>0.0248</td>\n",
              "      <td>0.0276</td>\n",
              "      <td>-0.0740</td>\n",
              "      <td>0.0803</td>\n",
              "      <td>0.2539</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>113.8561</td>\n",
              "      <td>260</td>\n",
              "      <td>gbdt</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>split</td>\n",
              "      <td>0.028</td>\n",
              "      <td>3</td>\n",
              "      <td>20</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3000</td>\n",
              "      <td>-1</td>\n",
              "      <td>31</td>\n",
              "      <td>regression</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>200000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.100</td>\n",
              "      <td>42</td>\n",
              "      <td>0.0245</td>\n",
              "      <td>0.9491</td>\n",
              "      <td>0.0249</td>\n",
              "      <td>0.0262</td>\n",
              "      <td>-0.0624</td>\n",
              "      <td>0.0801</td>\n",
              "      <td>0.2509</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>122.7018</td>\n",
              "      <td>260</td>\n",
              "      <td>gbdt</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>split</td>\n",
              "      <td>0.034</td>\n",
              "      <td>3</td>\n",
              "      <td>20</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3000</td>\n",
              "      <td>-1</td>\n",
              "      <td>31</td>\n",
              "      <td>regression</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>200000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.115</td>\n",
              "      <td>42</td>\n",
              "      <td>0.0246</td>\n",
              "      <td>1.0015</td>\n",
              "      <td>0.0250</td>\n",
              "      <td>0.0250</td>\n",
              "      <td>-0.0453</td>\n",
              "      <td>0.0762</td>\n",
              "      <td>0.2441</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>109.8449</td>\n",
              "      <td>260</td>\n",
              "      <td>gbdt</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>split</td>\n",
              "      <td>0.036</td>\n",
              "      <td>3</td>\n",
              "      <td>20</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3000</td>\n",
              "      <td>-1</td>\n",
              "      <td>31</td>\n",
              "      <td>regression</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>200000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.105</td>\n",
              "      <td>42</td>\n",
              "      <td>0.0245</td>\n",
              "      <td>0.9402</td>\n",
              "      <td>0.0250</td>\n",
              "      <td>0.0266</td>\n",
              "      <td>-0.0464</td>\n",
              "      <td>0.0748</td>\n",
              "      <td>0.2498</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>117.1597</td>\n",
              "      <td>260</td>\n",
              "      <td>gbdt</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>split</td>\n",
              "      <td>0.036</td>\n",
              "      <td>3</td>\n",
              "      <td>20</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3000</td>\n",
              "      <td>-1</td>\n",
              "      <td>31</td>\n",
              "      <td>regression</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>200000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.110</td>\n",
              "      <td>42</td>\n",
              "      <td>0.0245</td>\n",
              "      <td>0.9372</td>\n",
              "      <td>0.0250</td>\n",
              "      <td>0.0266</td>\n",
              "      <td>-0.0616</td>\n",
              "      <td>0.0753</td>\n",
              "      <td>0.2475</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>117.5454</td>\n",
              "      <td>260</td>\n",
              "      <td>gbdt</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>split</td>\n",
              "      <td>0.036</td>\n",
              "      <td>3</td>\n",
              "      <td>20</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3000</td>\n",
              "      <td>-1</td>\n",
              "      <td>31</td>\n",
              "      <td>regression</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>200000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.115</td>\n",
              "      <td>42</td>\n",
              "      <td>0.0245</td>\n",
              "      <td>0.9365</td>\n",
              "      <td>0.0250</td>\n",
              "      <td>0.0267</td>\n",
              "      <td>-0.0600</td>\n",
              "      <td>0.0751</td>\n",
              "      <td>0.2488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>116.6783</td>\n",
              "      <td>260</td>\n",
              "      <td>gbdt</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>split</td>\n",
              "      <td>0.038</td>\n",
              "      <td>3</td>\n",
              "      <td>20</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3000</td>\n",
              "      <td>-1</td>\n",
              "      <td>31</td>\n",
              "      <td>regression</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>200000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.115</td>\n",
              "      <td>42</td>\n",
              "      <td>0.0245</td>\n",
              "      <td>0.9518</td>\n",
              "      <td>0.0249</td>\n",
              "      <td>0.0262</td>\n",
              "      <td>-0.0459</td>\n",
              "      <td>0.0738</td>\n",
              "      <td>0.2512</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>111.2904</td>\n",
              "      <td>260</td>\n",
              "      <td>gbdt</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>split</td>\n",
              "      <td>0.040</td>\n",
              "      <td>3</td>\n",
              "      <td>20</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3000</td>\n",
              "      <td>-1</td>\n",
              "      <td>31</td>\n",
              "      <td>regression</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>200000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.105</td>\n",
              "      <td>42</td>\n",
              "      <td>0.0244</td>\n",
              "      <td>0.9468</td>\n",
              "      <td>0.0248</td>\n",
              "      <td>0.0262</td>\n",
              "      <td>-0.0487</td>\n",
              "      <td>0.0735</td>\n",
              "      <td>0.2476</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>129.0989</td>\n",
              "      <td>260</td>\n",
              "      <td>gbdt</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>split</td>\n",
              "      <td>0.022</td>\n",
              "      <td>4</td>\n",
              "      <td>20</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3000</td>\n",
              "      <td>-1</td>\n",
              "      <td>31</td>\n",
              "      <td>regression</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>200000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.095</td>\n",
              "      <td>42</td>\n",
              "      <td>0.0250</td>\n",
              "      <td>0.9240</td>\n",
              "      <td>0.0254</td>\n",
              "      <td>0.0275</td>\n",
              "      <td>-0.0648</td>\n",
              "      <td>0.0751</td>\n",
              "      <td>0.2619</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>134.2505</td>\n",
              "      <td>260</td>\n",
              "      <td>gbdt</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>split</td>\n",
              "      <td>0.022</td>\n",
              "      <td>4</td>\n",
              "      <td>20</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3000</td>\n",
              "      <td>-1</td>\n",
              "      <td>31</td>\n",
              "      <td>regression</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>200000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.100</td>\n",
              "      <td>42</td>\n",
              "      <td>0.0246</td>\n",
              "      <td>0.9283</td>\n",
              "      <td>0.0250</td>\n",
              "      <td>0.0270</td>\n",
              "      <td>-0.0570</td>\n",
              "      <td>0.0753</td>\n",
              "      <td>0.2612</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>137.2006</td>\n",
              "      <td>260</td>\n",
              "      <td>gbdt</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>split</td>\n",
              "      <td>0.022</td>\n",
              "      <td>4</td>\n",
              "      <td>20</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3000</td>\n",
              "      <td>-1</td>\n",
              "      <td>31</td>\n",
              "      <td>regression</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>200000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.105</td>\n",
              "      <td>42</td>\n",
              "      <td>0.0248</td>\n",
              "      <td>0.9413</td>\n",
              "      <td>0.0253</td>\n",
              "      <td>0.0269</td>\n",
              "      <td>-0.0523</td>\n",
              "      <td>0.0753</td>\n",
              "      <td>0.2604</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>150.6690</td>\n",
              "      <td>260</td>\n",
              "      <td>gbdt</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>split</td>\n",
              "      <td>0.022</td>\n",
              "      <td>4</td>\n",
              "      <td>20</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3000</td>\n",
              "      <td>-1</td>\n",
              "      <td>31</td>\n",
              "      <td>regression</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>200000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.115</td>\n",
              "      <td>42</td>\n",
              "      <td>0.0251</td>\n",
              "      <td>0.9914</td>\n",
              "      <td>0.0256</td>\n",
              "      <td>0.0258</td>\n",
              "      <td>-0.0443</td>\n",
              "      <td>0.0748</td>\n",
              "      <td>0.2646</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>137.1859</td>\n",
              "      <td>260</td>\n",
              "      <td>gbdt</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>split</td>\n",
              "      <td>0.024</td>\n",
              "      <td>4</td>\n",
              "      <td>20</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3000</td>\n",
              "      <td>-1</td>\n",
              "      <td>31</td>\n",
              "      <td>regression</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>200000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.095</td>\n",
              "      <td>42</td>\n",
              "      <td>0.0244</td>\n",
              "      <td>0.9554</td>\n",
              "      <td>0.0248</td>\n",
              "      <td>0.0260</td>\n",
              "      <td>-0.0605</td>\n",
              "      <td>0.0742</td>\n",
              "      <td>0.2642</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>144.5336</td>\n",
              "      <td>260</td>\n",
              "      <td>gbdt</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>split</td>\n",
              "      <td>0.024</td>\n",
              "      <td>4</td>\n",
              "      <td>20</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3000</td>\n",
              "      <td>-1</td>\n",
              "      <td>31</td>\n",
              "      <td>regression</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>200000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.105</td>\n",
              "      <td>42</td>\n",
              "      <td>0.0249</td>\n",
              "      <td>0.9712</td>\n",
              "      <td>0.0253</td>\n",
              "      <td>0.0261</td>\n",
              "      <td>-0.0642</td>\n",
              "      <td>0.0739</td>\n",
              "      <td>0.2641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>150.9035</td>\n",
              "      <td>260</td>\n",
              "      <td>gbdt</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>split</td>\n",
              "      <td>0.024</td>\n",
              "      <td>4</td>\n",
              "      <td>20</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3000</td>\n",
              "      <td>-1</td>\n",
              "      <td>31</td>\n",
              "      <td>regression</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>200000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.115</td>\n",
              "      <td>42</td>\n",
              "      <td>0.0244</td>\n",
              "      <td>0.9865</td>\n",
              "      <td>0.0249</td>\n",
              "      <td>0.0252</td>\n",
              "      <td>-0.0481</td>\n",
              "      <td>0.0735</td>\n",
              "      <td>0.2641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>148.2997</td>\n",
              "      <td>260</td>\n",
              "      <td>gbdt</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>split</td>\n",
              "      <td>0.026</td>\n",
              "      <td>4</td>\n",
              "      <td>20</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3000</td>\n",
              "      <td>-1</td>\n",
              "      <td>31</td>\n",
              "      <td>regression</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>200000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.110</td>\n",
              "      <td>42</td>\n",
              "      <td>0.0254</td>\n",
              "      <td>0.9507</td>\n",
              "      <td>0.0258</td>\n",
              "      <td>0.0271</td>\n",
              "      <td>-0.0636</td>\n",
              "      <td>0.0726</td>\n",
              "      <td>0.2659</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65</th>\n",
              "      <td>136.8332</td>\n",
              "      <td>260</td>\n",
              "      <td>gbdt</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>split</td>\n",
              "      <td>0.028</td>\n",
              "      <td>4</td>\n",
              "      <td>20</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3000</td>\n",
              "      <td>-1</td>\n",
              "      <td>31</td>\n",
              "      <td>regression</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>200000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.095</td>\n",
              "      <td>42</td>\n",
              "      <td>0.0259</td>\n",
              "      <td>0.9844</td>\n",
              "      <td>0.0264</td>\n",
              "      <td>0.0268</td>\n",
              "      <td>-0.0595</td>\n",
              "      <td>0.0716</td>\n",
              "      <td>0.2648</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69</th>\n",
              "      <td>152.9809</td>\n",
              "      <td>260</td>\n",
              "      <td>gbdt</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>split</td>\n",
              "      <td>0.028</td>\n",
              "      <td>4</td>\n",
              "      <td>20</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3000</td>\n",
              "      <td>-1</td>\n",
              "      <td>31</td>\n",
              "      <td>regression</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>200000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.115</td>\n",
              "      <td>42</td>\n",
              "      <td>0.0252</td>\n",
              "      <td>0.9955</td>\n",
              "      <td>0.0256</td>\n",
              "      <td>0.0257</td>\n",
              "      <td>-0.0430</td>\n",
              "      <td>0.0708</td>\n",
              "      <td>0.2623</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>133.1882</td>\n",
              "      <td>260</td>\n",
              "      <td>gbdt</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>split</td>\n",
              "      <td>0.030</td>\n",
              "      <td>4</td>\n",
              "      <td>20</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3000</td>\n",
              "      <td>-1</td>\n",
              "      <td>31</td>\n",
              "      <td>regression</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>200000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.095</td>\n",
              "      <td>42</td>\n",
              "      <td>0.0249</td>\n",
              "      <td>0.9644</td>\n",
              "      <td>0.0254</td>\n",
              "      <td>0.0263</td>\n",
              "      <td>-0.0502</td>\n",
              "      <td>0.0705</td>\n",
              "      <td>0.2647</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>137.7592</td>\n",
              "      <td>260</td>\n",
              "      <td>gbdt</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>split</td>\n",
              "      <td>0.032</td>\n",
              "      <td>4</td>\n",
              "      <td>20</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3000</td>\n",
              "      <td>-1</td>\n",
              "      <td>31</td>\n",
              "      <td>regression</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>200000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.095</td>\n",
              "      <td>42</td>\n",
              "      <td>0.0250</td>\n",
              "      <td>1.0095</td>\n",
              "      <td>0.0254</td>\n",
              "      <td>0.0252</td>\n",
              "      <td>-0.0442</td>\n",
              "      <td>0.0695</td>\n",
              "      <td>0.2625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>129.3303</td>\n",
              "      <td>260</td>\n",
              "      <td>gbdt</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>split</td>\n",
              "      <td>0.034</td>\n",
              "      <td>4</td>\n",
              "      <td>20</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3000</td>\n",
              "      <td>-1</td>\n",
              "      <td>31</td>\n",
              "      <td>regression</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>200000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.095</td>\n",
              "      <td>42</td>\n",
              "      <td>0.0249</td>\n",
              "      <td>1.0552</td>\n",
              "      <td>0.0252</td>\n",
              "      <td>0.0239</td>\n",
              "      <td>-0.0378</td>\n",
              "      <td>0.0687</td>\n",
              "      <td>0.2609</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    total_time  round_number  ... feature_exposure  max_feature_exposure\n",
              "2     117.0181           260  ...           0.0838                0.2535\n",
              "6     116.1439           260  ...           0.0817                0.2522\n",
              "8     122.2655           260  ...           0.0815                0.2533\n",
              "13    118.0555           260  ...           0.0803                0.2539\n",
              "16    113.8561           260  ...           0.0801                0.2509\n",
              "34    122.7018           260  ...           0.0762                0.2441\n",
              "37    109.8449           260  ...           0.0748                0.2498\n",
              "38    117.1597           260  ...           0.0753                0.2475\n",
              "39    117.5454           260  ...           0.0751                0.2488\n",
              "44    116.6783           260  ...           0.0738                0.2512\n",
              "47    111.2904           260  ...           0.0735                0.2476\n",
              "50    129.0989           260  ...           0.0751                0.2619\n",
              "51    134.2505           260  ...           0.0753                0.2612\n",
              "52    137.2006           260  ...           0.0753                0.2604\n",
              "54    150.6690           260  ...           0.0748                0.2646\n",
              "55    137.1859           260  ...           0.0742                0.2642\n",
              "57    144.5336           260  ...           0.0739                0.2641\n",
              "59    150.9035           260  ...           0.0735                0.2641\n",
              "63    148.2997           260  ...           0.0726                0.2659\n",
              "65    136.8332           260  ...           0.0716                0.2648\n",
              "69    152.9809           260  ...           0.0708                0.2623\n",
              "70    133.1882           260  ...           0.0705                0.2647\n",
              "75    137.7592           260  ...           0.0695                0.2625\n",
              "80    129.3303           260  ...           0.0687                0.2609\n",
              "\n",
              "[24 rows x 31 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0rc20lf4tFH"
      },
      "source": [
        "# # create histogram of corrlation scores by random seed. \n",
        "# final_scores = record_df['correlation']\n",
        "# f = final_scores.to_numpy()\n",
        "# std_dev_of_corr = round(np.std(f),4)\n",
        "# avg_corr = round(np.average(f),4)\n",
        "\n",
        "# sharpe_of_corr = round(avg_corr / std_dev_of_corr,4).\n",
        "\n",
        "# # interestingly these scores are very tightly coupled.  random seed does not tend to have much influcence\n",
        "# print(f'The average corr was {avg_corr} with a Standard Deviation of {std_dev_of_corr} with a sharpe of {sharpe_of_corr}')"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsRJSpJNMiKQ"
      },
      "source": [
        "# Submit to Numerai\n",
        "\n",
        "I don't think I should submit anything to Numerai here. \n",
        "\n",
        "1. Create a prediction list.\n",
        "2. Link those predictions with the tournment data\n",
        "3. Write the id, prediction to a csv file.\n",
        "4. Use numerai wrapper to submit that .csv file as your current model. \n",
        "5. This submits for MRQUANTSALOT and TUTMODEL\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSo8yDCxTBB1"
      },
      "source": [
        "### Methods to handle submission"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EoYEwqg4MiKQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79c73527-89ce-4058-934d-1b6267440a58"
      },
      "source": [
        "def load_api_creds_into_dict(): # works\n",
        "  \"\"\"\n",
        "    Read creds.json and return a dictionary of your API keys.\n",
        "  \"\"\"\n",
        "  creds  = open('creds.json','r') # refactor this to point at your google drive. \n",
        "  api_keys_dict = json.load(creds)\n",
        "  creds.close()\n",
        "  return api_keys_dict\n",
        "\n",
        "\n",
        "def open_api_access(): # works\n",
        "    \"\"\"\n",
        "    Read in my private key from creds.json and return the numerai api wrapper\n",
        "    \"\"\"\n",
        "    api_keys_dict = load_api_creds_into_dict()\n",
        "    my_secret_key = api_keys_dict['secret_key']\n",
        "    my_public_id = api_keys_dict['public_id']\n",
        "    napi = numerapi.NumerAPI(secret_key=my_secret_key, public_id=my_public_id)\n",
        "    return napi\n",
        "\n",
        "\n",
        "def create_id_prediction_df(tournament_data: pd.DataFrame, model_predictions : np.ndarray): # works\n",
        "    \"\"\"\n",
        "    Create a dataframe that looks like \n",
        "    id,prediction\n",
        "    asdfads,.5429\n",
        "    asdfaddsss,.5051\n",
        "    ...\n",
        "    \"\"\"\n",
        "    predictions_df = tournament_data[\"id\"].to_frame() # get all the Ids and cast them to a Dataframe\n",
        "    predictions_df[PREDICTION_NAME] = model_predictions #add your predictions to the id frame\n",
        "    return predictions_df # data frame of id, prediction\n",
        "\n",
        "\n",
        "def write_predictions_to_file(prediction_df: pd.DataFrame): # works\n",
        "    my_file_name = 'myPredictions.csv'\n",
        "    try:\n",
        "      out_location = open(my_file_name, 'x')\n",
        "    except:\n",
        "      out_location = open(my_file_name, 'w')\n",
        "\n",
        "    prediction_df.to_csv(out_location, index=False)\n",
        "    out_location.close()\n",
        "    return my_file_name \n",
        "\n",
        "\n",
        "def run_model_and_create_prediction_file(model_object, tournament_data: pd.DataFrame, features: list):\n",
        "  \"\"\"\n",
        "    This stitches everything together.\n",
        "\n",
        "    Pass it a trained model and the tournament data set, the list of feature columns\n",
        "    1. Does preditions\n",
        "    2. write the predictions to a file.\n",
        "    3. returns the name fo the file where my predictions are saved data is saved\n",
        "  \"\"\"\n",
        "  model_predictions = model_object.predict(tournament_data[features])\n",
        "  prediction_df = create_id_prediction_df(tournament_data,model_predictions)\n",
        "  file_with_predictions = write_predictions_to_file(prediction_df)\n",
        "  return file_with_predictions\n",
        "\n",
        "\n",
        "def submit_predictions_to_numerai(filename_of_predictions, sumbit_model_id):\n",
        "    napi = open_api_access() # open a connection to the numerai API with your creds.json file\n",
        "    submission_id = napi.upload_predictions(filename_of_predictions, model_id=sumbit_model_id)\n",
        "    print(f'You successfully submitted for {sumbit_model_id}')\n",
        "    print(type(submission_id))\n",
        "    return submission_id\n",
        "\n",
        "print('your helper methods work correctly')\n",
        "    "
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "your helper methods work correctly\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}