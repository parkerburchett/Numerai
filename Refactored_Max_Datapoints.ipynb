{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Refactored Max Datapoints.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "7bWSBkk2_m3h",
        "z4vmuIio762C",
        "t3RY5VyX8C1w"
      ],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7atJJ2fMiJ8"
      },
      "source": [
        "# This notebook explores the Hyper Parameter Space of the lightgbm library\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0iP-UgesMiKB",
        "outputId": "ba2163eb-3160-4c52-ab9f-f1504ed50a6a"
      },
      "source": [
        "!pip install numerapi\n",
        "import numerapi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting numerapi\n",
            "  Downloading https://files.pythonhosted.org/packages/fd/96/ebdbaff5a2fef49b212e4f40634166f59e45462a768c0136d148f00255c5/numerapi-2.4.5-py3-none-any.whl\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from numerapi) (2.8.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from numerapi) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from numerapi) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.29.1 in /usr/local/lib/python3.7/dist-packages (from numerapi) (4.41.1)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from numerapi) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil->numerapi) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->numerapi) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->numerapi) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->numerapi) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->numerapi) (3.0.4)\n",
            "Installing collected packages: numerapi\n",
            "Successfully installed numerapi-2.4.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4WFeO-ZMiKC"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os, sys\n",
        "import gc\n",
        "import pathlib\n",
        "import json\n",
        "import datetime\n",
        "from typing import List, NoReturn, Union, Tuple, Optional, Text, Generic, Callable, Dict\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, QuantileTransformer\n",
        "from sklearn.model_selection import KFold, StratifiedKFold, TimeSeriesSplit\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, log_loss, mean_squared_error, mean_absolute_error, f1_score\n",
        "from scipy.stats import spearmanr # -P I think this is corr. \n",
        "import joblib\n",
        "\n",
        "# model\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "import operator\n",
        "\n",
        "# visualize\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.style as style\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot\n",
        "from matplotlib.ticker import ScalarFormatter\n",
        "sns.set_context(\"talk\")\n",
        "style.use('seaborn-colorblind')\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0ELMvF79l3D",
        "outputId": "107f34c8-4051-4a9b-85c9-5d1fa035af86"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b364c12-MiKD"
      },
      "source": [
        "## Methods to Gather and Clean Incoming Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mm1stk1pMiKD"
      },
      "source": [
        ""
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6qoKn7W4Za_"
      },
      "source": [
        "### Load the Data from Numerai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "strAzqc-MiKE"
      },
      "source": [
        "### Extract the Validation from the tournament_data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmV-K6xHoqxW"
      },
      "source": [
        "def create_global_variables()-> None:\n",
        "  \"\"\"\n",
        "    Create all global variables. \n",
        "    ROUND_NUMBER,FEATURES,TARGET,\n",
        "    TOURNAMENT_DATA,TRAINING_DATA,VALIDATION_DATA\n",
        "  \"\"\"\n",
        "  try:\n",
        "    if HAVE_GATHERED_DATA == FALSE:\n",
        "      create_global_constants()\n",
        "      ping_training_data()\n",
        "      TOURNAMENT_DATA = ping_tournament_data()\n",
        "      create_validation_data(df = TOURNAMENT_DATA)\n",
        "      drop_data_type_columns()\n",
        "      HAVE_GATHERED_DATA = True\n",
        "  except NameError:\n",
        "    create_global_constants()\n",
        "    ping_training_data()\n",
        "    TOURNAMENT_DATA = ping_tournament_data()\n",
        "    create_validation_data(df = TOURNAMENT_DATA)\n",
        "    drop_data_type_columns()\n",
        "    HAVE_GATHERED_DATA = True\n",
        "\n",
        "def drop_data_type_columns():\n",
        "  TRAINING_DATA.drop(columns=[\"data_type\"], inplace=True)\n",
        "  VALIDATION_DATA.drop(columns=[\"data_type\"], inplace=True) #\n",
        "  TOURNAMENT_DATA.drop(columns=[\"data_type\"], inplace=True)\n",
        "\n",
        "def ping_training_data():\n",
        "  global TRAINING_DATA\n",
        "  TRAINING_DATA = read_data('train')\n",
        "  \n",
        "def ping_tournament_data():\n",
        "  global TOURNAMENT_DATA\n",
        "  TOURNAMENT_DATA = read_data('tournament')\n",
        "  return TOURNAMENT_DATA\n",
        "\n",
        "def create_validation_data(df):\n",
        "  global VALIDATION_DATA\n",
        "  VALIDATION_DATA  = df[df[\"data_type\"] == \"validation\"].reset_index(drop = True)\n",
        "\n",
        "def cast_eras_as_int(x): \n",
        "    try:\n",
        "        return int(x[3:]) # strip the first 3 characters from each era\n",
        "    except:\n",
        "        return -99\n",
        "\n",
        "def read_data(data):\n",
        "    if data == 'train':\n",
        "        df = pd.read_csv('https://numerai-public-datasets.s3-us-west-2.amazonaws.com/latest_numerai_training_data.csv.xz')\n",
        "    elif data == 'tournament':\n",
        "        df = pd.read_csv('https://numerai-public-datasets.s3-us-west-2.amazonaws.com/latest_numerai_tournament_data.csv.xz')\n",
        "        \n",
        "    feature_cols = df.columns[df.columns.str.startswith('feature')]\n",
        "    mapping = {0.0 : 0, 0.25 : 1, 0.5 : 2, 0.75 : 3, 1.0 : 4}\n",
        "\n",
        "    for c in feature_cols:\n",
        "        df[c] = df[c].map(mapping).astype(np.uint8)\n",
        "        \n",
        "    df[\"era\"] = df[\"era\"].apply(cast_eras_as_int)\n",
        "    return df\n",
        "\n",
        "\n",
        "def create_global_constants() -> None:\n",
        "  global TARGET\n",
        "  TARGET = get_target_constant(tournament_data)\n",
        "  global FEATURES\n",
        "  FEATURES = get_features_constant(tournament_data)\n",
        "  napi = open_api_access()\n",
        "  global ROUND_NUMBER\n",
        "  ROUND_NUMBER = napi.get_current_round()\n",
        "\n",
        "\n",
        "def get_target_constant(tournament_data: pd.DataFrame):\n",
        "  return tournament_data.columns[tournament_data.columns.str.startswith('target')].values.tolist()[0]\n",
        "\n",
        "\n",
        "def get_features_constant(tournament_data: pd.DataFrame):\n",
        "  return [column_names for column_names in tournament_data.columns.values.tolist() if 'feature' in column_names]\n",
        "\n",
        "\n",
        "\n",
        "def load_api_creds_into_dict():\n",
        "  creds  = open('/content/drive/MyDrive/creds.json','r') \n",
        "  api_keys_dict = json.load(creds)\n",
        "  creds.close()\n",
        "  return api_keys_dict\n",
        "\n",
        "\n",
        "def open_api_access():\n",
        "    api_keys_dict = load_api_creds_into_dict()\n",
        "    my_secret_key = api_keys_dict['secret_key']\n",
        "    my_public_id = api_keys_dict['public_id']\n",
        "    napi = numerapi.NumerAPI(secret_key=my_secret_key, public_id=my_public_id)\n",
        "    return napi\n",
        "\n",
        "\n"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3eMSLtXzspp",
        "outputId": "5fa37ce6-f4ed-4ea1-9cbf-534c5acaea5e"
      },
      "source": [
        "%%time\n",
        "create_global_variables()\n",
        "\n",
        "## the first time you run this in a session it should take 20 minutes. \n",
        "## the second time it should be near instant."
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 6min 3s, sys: 26.7 s, total: 6min 29s\n",
            "Wall time: 6min 36s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "uosAd8sp-UPV",
        "outputId": "c2eab651-0a89-4eb8-aa26-814ea8d57489"
      },
      "source": [
        "HAVE_GATHERED_DATA"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-9d60da9e7ba0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mHAVE_GATHERED_DATA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'HAVE_GATHERED_DATA' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4khTMbONzW-"
      },
      "source": [
        "### ModelStats Object\n",
        "\n",
        "1. Stores the Trained Model\n",
        "2. Stores the Hyper Parameters\n",
        "3. Stores the Validation Scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LCuE_km4Y_3"
      },
      "source": [
        "class ModelStats():\n",
        "  \"\"\"\n",
        "  An object that tracks Hyper Parameters, Time Costs and Scores. \n",
        "  \"\"\"\n",
        "  def __init__(self, model, scores:dict, total_time):\n",
        "        self.model = model \n",
        "        self.hyperparams = model.get_params() \n",
        "        self.scores = scores \n",
        "        self.total_time = total_time\n",
        "        self.all_stats_dict = None \n",
        "\n",
        "\n",
        "  def create_all_stats_dict(self):\n",
        "    \"\"\"\n",
        "    Create a single dictionary tracking all the relevent statistics.\n",
        "    \"\"\"\n",
        "    if self.all_stats_dict == None:\n",
        "      \n",
        "      all_stats_dict = {}\n",
        "      all_stats_dict['total_time'] = self.total_time\n",
        "      all_stats_dict['round_number'] = ROUND_NUMBER\n",
        "      all_stats_dict.update(self.hyperparams) # dict.update(dict) merges two dictionaries\n",
        "      all_stats_dict.update(self.scores)\n",
        "\n",
        "      DECIMALS = 4 \n",
        "\n",
        "      for key in all_stats_dict.keys():\n",
        "          try:\n",
        "            all_stats_dict[key] = round(all_stats_dict[key],DECIMALS)\n",
        "          except:\n",
        "            all_stats_dict[key] = all_stats_dict[key]\n",
        "\n",
        "      self.all_stats_dict = all_stats_dict\n",
        "\n",
        "\n",
        "  def headlines(self):\n",
        "    \"\"\"\n",
        "    # Get a subset of scores that are the high level summary of the model\n",
        "    \"\"\"\n",
        "    self.create_all_stats_dict()\n",
        "    summary_dict = {}\n",
        "    summary_dict['correlation'] = self.all_stats_dict['correlation']\n",
        "    summary_dict['corr_sharpe'] = self.all_stats_dict['corr_sharpe']\n",
        "    summary_dict['max_depth'] = self.all_stats_dict['max_depth']\n",
        "    summary_dict['n_estimators'] = self.all_stats_dict['n_estimators']\n",
        "    summary_dict['total_time'] = self.all_stats_dict['total_time']  \n",
        "    summary_dict['num_leaves'] = self.all_stats_dict['num_leaves']\n",
        "    summary_dict['learning_rate'] = self.all_stats_dict['learning_rate']\n",
        "    return summary_dict\n",
        "  \n",
        "\n",
        "  def save_all_dict_to_disk(self):\n",
        "    \"\"\"\n",
        "        Check to see if you already have scores saved. if yes. add these scores. \n",
        "        else create a new file to save scores\n",
        "    \"\"\"\n",
        "    print('here')\n",
        "    self.create_all_stats_dict()\n",
        "    PATH_TO_SAVE_SCORES = '/content/drive/MyDrive/numerai_hyperparams_scores.csv'\n",
        "    all_stats_df = pd.DataFrame.from_dict(my_model_stats.all_stats_dict, orient='index')\n",
        "    print('1')\n",
        "\n",
        "    if file_can_be_converted_to_df(PATH_TO_SAVE_SCORES):\n",
        "      print('2')\n",
        "      scores_already_saved_df = pd.read_csv(PATH_TO_SAVE_SCORES)\n",
        "      print('5')\n",
        "      scores_already_saved_df = pd.concat([scores_already_saved_df, all_stats_df], axis=1)\n",
        "      scores_already_saved_df.to_csv(PATH_TO_SAVE_SCORES, index=False)\n",
        "      print('4')\n",
        "\n",
        "    else:\n",
        "      with open(PATH_TO_SAVE_SCORES,'x'):\n",
        "        pass\n",
        "      all_stats_df.to_csv(PATH_TO_SAVE_SCORES, index=False)\n",
        "\n",
        "def file_can_be_converted_to_df(path: str)-> bool:\n",
        "  try:\n",
        "    df = pd.read_csv(path)\n",
        "    return True\n",
        "  except:\n",
        "    return False\n",
        "\n",
        "\n",
        "def train_LGBMRegressor(params: dict, train_data): # there is not really a clear cell to put this method\n",
        "  \"\"\"\n",
        "  Inputs: a dict of hyper paramaters for the model, \n",
        "  train_data: a pd.DataFrame of the Tiuraining Data\n",
        "\n",
        "  Returns a trained model based on the parmas\n",
        "  \"\"\"\n",
        "  model = lgb.LGBMRegressor(**params) \n",
        "  model.fit(train_data[FEATURES], train_data[TARGET])\n",
        "  return model"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bWSBkk2_m3h"
      },
      "source": [
        "#### Methods to Determine Validation Scores\n",
        "\n",
        "1. I did not write these. I added the English comments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szXbJM0mMiKJ"
      },
      "source": [
        "# naming conventions\n",
        "PREDICTION_NAME = 'prediction'\n",
        "TARGET_NAME = TARGET # 'target is the string named 'target'\n",
        "# EXAMPLE_PRED = 'example_prediction'\n",
        "\n",
        "# ---------------------------\n",
        "# Functions\n",
        "# ---------------------------\n",
        "def valid4score(valid : pd.DataFrame, pred : np.ndarray, load_example: bool=True, save : bool=False) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Generate new valid pandas dataframe for computing scores\n",
        "    \n",
        "    :INPUT:\n",
        "    - valid : pd.DataFrame extracted from tournament data (data_type='validation')\n",
        "    \n",
        "    \"\"\"\n",
        "    valid_df = valid.copy() # the validation dataframe you use this to test the CORR and other values\n",
        "\n",
        "    # Your model creates an array of floats [0,1] rank method converst them in a list of ints. \n",
        "\n",
        "    # your lis tof ints is then compared to their list of ints. \n",
        "    valid_df['prediction'] = pd.Series(pred).rank(pct=True, method=\"first\") # pred is the array of predictions your model creates for the set of validation vectors.  \n",
        "    # I am unsure if this preds is a float only only between 0,1,2,3,4. \n",
        "    valid_df.rename(columns={TARGET: 'target'}, inplace=True)\n",
        "    \n",
        "    # I don't know what the load example boolean is. I think you can use this to save predictions.\n",
        "    if load_example:\n",
        "        valid_df[EXAMPLE_PRED] = pd.read_csv(EXP_DIR + 'valid_df.csv')['prediction'].values\n",
        "    \n",
        "    if save==True:\n",
        "        valid_df.to_csv(OUTPUT_DIR + 'valid_df.csv', index=False)\n",
        "        print('Validation dataframe saved!')\n",
        "    \n",
        "    return valid_df\n",
        "\n",
        "def compute_corr(valid_df : pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Compute rank correlation\n",
        "\n",
        "    THIS IS WHAT YOU ARE PRIMARILY PAID ON \n",
        "    \n",
        "    :INPUT:\n",
        "    - valid_df : pd.DataFrame where at least 2 columns ('prediction' & 'target') exist\n",
        "    \n",
        "    \"\"\"\n",
        "    # this uses Person Correilation. \n",
        "    # I You are paid on spearman corrilation. That is where the ratio of change is important not the raw amount of change\n",
        "    # see: https://support.minitab.com/en-us/minitab-express/1/help-and-how-to/modeling-statistics/regression/supporting-topics/basics/a-comparison-of-the-pearson-and-spearman-correlation-methods/\n",
        "    return np.corrcoef(valid_df[\"target\"], valid_df['prediction'])[0, 1]\n",
        "\n",
        "def compute_max_drawdown(validation_correlations : pd.Series):\n",
        "    \"\"\"\n",
        "    Compute max drawdown\n",
        "    \n",
        "    :INPUT:\n",
        "    - validation_correaltions : pd.Series\n",
        "    \"\"\"\n",
        "    \n",
        "    rolling_max = (validation_correlations + 1).cumprod().rolling(window=100, min_periods=1).max()\n",
        "    daily_value = (validation_correlations + 1).cumprod()\n",
        "    max_drawdown = -(rolling_max - daily_value).max()\n",
        "    \n",
        "    return max_drawdown\n",
        "\n",
        "def compute_val_corr(valid_df : pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Compute rank correlation for valid periods\n",
        "    \n",
        "    :INPUT:\n",
        "    - valid_df : pd.DataFrame where at least 2 columns ('prediction' & 'target') exist\n",
        "    \"\"\"\n",
        "    \n",
        "    # all validation\n",
        "    correlation = compute_corr(valid_df)\n",
        "    #print(\"rank corr = {:.4f}\".format(correlation))\n",
        "    return correlation\n",
        "    \n",
        "def compute_val_sharpe(valid_df : pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Compute sharpe ratio for valid periods\n",
        "    \n",
        "    :INPUT:\n",
        "    - valid_df : pd.DataFrame where at least 2 columns ('prediction' & 'target') exist\n",
        "    \"\"\"\n",
        "    # all validation\n",
        "    d = valid_df.groupby('era')[['target', 'prediction']].corr().iloc[0::2,-1].reset_index()\n",
        "    me = d['prediction'].mean()\n",
        "    sd = d['prediction'].std()\n",
        "    max_drawdown = compute_max_drawdown(d['prediction'])\n",
        "    #print('sharpe ratio = {:.4f}, corr mean = {:.4f}, corr std = {:.4f}, max drawdown = {:.4f}'.format(me / sd, me, sd, max_drawdown))\n",
        "    \n",
        "    return me / sd, me, sd, max_drawdown\n",
        "    \n",
        "def feature_exposures(valid_df : pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Compute feature exposure\n",
        "    \n",
        "    :INPUT:\n",
        "    - valid_df : pd.DataFrame where at least 2 columns ('prediction' & 'target') exist\n",
        "    \"\"\"\n",
        "    feature_names = [f for f in valid_df.columns\n",
        "                     if f.startswith(\"feature\")]\n",
        "    exposures = []\n",
        "    for f in feature_names:\n",
        "        fe = spearmanr(valid_df['prediction'], valid_df[f])[0]\n",
        "        exposures.append(fe)\n",
        "    return np.array(exposures)\n",
        "\n",
        "def max_feature_exposure(fe : np.ndarray):\n",
        "    return np.max(np.abs(fe))\n",
        "\n",
        "def feature_exposure(fe : np.ndarray):\n",
        "    return np.sqrt(np.mean(np.square(fe)))\n",
        "\n",
        "def compute_val_feature_exposure(valid_df : pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Compute feature exposure for valid periods\n",
        "    \n",
        "    :INPUT:\n",
        "    - valid_df : pd.DataFrame where at least 2 columns ('prediction' & 'target') exist\n",
        "    \"\"\"\n",
        "    # all validation\n",
        "    fe = feature_exposures(valid_df)\n",
        "    fe1, fe2 = feature_exposure(fe), max_feature_exposure(fe)\n",
        "    #print('feature exposure = {:.4f}, max feature exposure = {:.4f}'.format(fe1, fe2))\n",
        "     \n",
        "    return fe1, fe2\n",
        "\n",
        "# to neutralize a column in a df by many other columns\n",
        "#         I have no idea what this method does. -P. need to read about it and write up a link to it. \n",
        "def neutralize(df, columns, by, proportion=1.0):\n",
        "    scores = df.loc[:, columns]\n",
        "    exposures = df[by].values\n",
        "\n",
        "    # constant column to make sure the series is completely neutral to exposures\n",
        "    exposures = np.hstack(\n",
        "        (exposures,\n",
        "         np.asarray(np.mean(scores)) * np.ones(len(exposures)).reshape(-1, 1)))\n",
        "\n",
        "    scores = scores - proportion * exposures.dot(\n",
        "        np.linalg.pinv(exposures).dot(scores))\n",
        "    return scores / scores.std()\n",
        "\n",
        "\n",
        "# to neutralize any series by any other series\n",
        "def neutralize_series(series, by, proportion=1.0):\n",
        "    scores = series.values.reshape(-1, 1)\n",
        "    exposures = by.values.reshape(-1, 1)\n",
        "\n",
        "    # this line makes series neutral to a constant column so that it's centered and for sure gets corr 0 with exposures\n",
        "    exposures = np.hstack(\n",
        "        (exposures,\n",
        "         np.array([np.mean(series)] * len(exposures)).reshape(-1, 1)))\n",
        "\n",
        "    correction = proportion * (exposures.dot(\n",
        "        np.linalg.lstsq(exposures, scores, rcond=None)[0]))\n",
        "    corrected_scores = scores - correction\n",
        "    neutralized = pd.Series(corrected_scores.ravel(), index=series.index)\n",
        "    return neutralized\n",
        "\n",
        "\n",
        "def unif(df):\n",
        "    x = (df.rank(method=\"first\") - 0.5) / len(df)\n",
        "    return pd.Series(x, index=df.index)\n",
        "\n",
        "def get_feature_neutral_mean(df):\n",
        "    feature_cols = [c for c in df.columns if c.startswith(\"feature\")]\n",
        "    df.loc[:, \"neutral_sub\"] = neutralize(df, [PREDICTION_NAME],\n",
        "                                          feature_cols)[PREDICTION_NAME]\n",
        "    scores = df.groupby(\"era\").apply(\n",
        "        lambda x: np.corrcoef(x[\"neutral_sub\"].rank(pct=True, method=\"first\"), x[TARGET_NAME])).mean()\n",
        "    return np.mean(scores)\n",
        "\n",
        "def compute_val_mmc(valid_df : pd.DataFrame):    \n",
        "    # MMC over validation\n",
        "    mmc_scores = []\n",
        "    corr_scores = []\n",
        "    for _, x in valid_df.groupby(\"era\"):\n",
        "        series = neutralize_series(pd.Series(unif(x[PREDICTION_NAME])),\n",
        "                                   pd.Series(unif(x[EXAMPLE_PRED])))\n",
        "        mmc_scores.append(np.cov(series, x[TARGET_NAME])[0, 1] / (0.29 ** 2)) # I have no idea what htis line does (0.29 ** 2)\n",
        "        corr_scores.append(np.corrcoef(unif(x[PREDICTION_NAME]).rank(pct=True, method=\"first\"), x[TARGET_NAME]))\n",
        "\n",
        "    val_mmc_mean = np.mean(mmc_scores)\n",
        "    val_mmc_std = np.std(mmc_scores)\n",
        "    val_mmc_sharpe = val_mmc_mean / val_mmc_std\n",
        "    corr_plus_mmcs = [c + m for c, m in zip(corr_scores, mmc_scores)]\n",
        "    corr_plus_mmc_sharpe = np.mean(corr_plus_mmcs) / np.std(corr_plus_mmcs)\n",
        "    corr_plus_mmc_mean = np.mean(corr_plus_mmcs)\n",
        "\n",
        "    #print(\"MMC Mean = {:.6f}, MMC Std = {:.6f}, CORR+MMC Sharpe = {:.4f}\".format(val_mmc_mean, val_mmc_std, corr_plus_mmc_sharpe))\n",
        "\n",
        "    # Check correlation with example predictions\n",
        "    corr_with_example_preds = np.corrcoef(valid_df[EXAMPLE_PRED].rank(pct=True, method=\"first\"),\n",
        "                                          valid_df[PREDICTION_NAME].rank(pct=True, method=\"first\"))[0, 1]\n",
        "    #print(\"Corr with example preds: {:.4f}\".format(corr_with_example_preds))\n",
        "    \n",
        "    return val_mmc_mean, val_mmc_std, corr_plus_mmc_sharpe, corr_with_example_preds\n",
        "\n",
        "\n",
        "# this is the main method. The rest are just called interanlly. \n",
        "def score_summary(valid_df : pd.DataFrame):\n",
        "    score_dict = {}\n",
        "    \n",
        "    try:\n",
        "        score_dict['correlation'] = compute_val_corr(valid_df)\n",
        "    except:\n",
        "        print('ERR: computing correlation')\n",
        "    try:\n",
        "        score_dict['corr_sharpe'], score_dict['corr_mean'], score_dict['corr_std'], score_dict['max_drawdown'] = compute_val_sharpe(valid_df)\n",
        "    except:\n",
        "        print('ERR: computing sharpe')\n",
        "    try:\n",
        "        score_dict['feature_exposure'], score_dict['max_feature_exposure'] = compute_val_feature_exposure(valid_df)\n",
        "    except:\n",
        "        print('ERR: computing feature exposure')\n",
        "    # try:\n",
        "    #     score_dict['mmc_mean'], score_dict['mmc_std'], score_dict['corr_mmc_sharpe'], score_dict['corr_with_example_xgb'] = compute_val_mmc(valid_df)\n",
        "    # except:\n",
        "    #     print('ERR: computing MMC')\n",
        "    \n",
        "    return score_dict"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lBD0oUmBWzx"
      },
      "source": [
        "### Main to train and track time, hyper parmas and scores."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80k6-VBtBWhf"
      },
      "source": [
        "def train_validate_store(params, train_data, validation_data, features):\n",
        "  \"\"\"\n",
        "  Train a model. Get Validation Scores. Create ModelStats Object. Save that model Stats object to your Google Drive\n",
        "  \"\"\"\n",
        "  start_time = datetime.datetime.now()\n",
        "  my_model = train_LGBMRegressor(params=params, train_data=train_data)\n",
        "  my_predictions = my_model.predict(validation_data[features])\n",
        "  valid_df = valid4score(validation_data, my_predictions, load_example=False, save=False)\n",
        "  my_scores = score_summary(valid_df)\n",
        "  total_time = (datetime.datetime.now() - start_time).total_seconds() \n",
        "  my_model_stats = ModelStats(model=my_model, scores=my_scores, total_time=total_time)\n",
        "  my_model_stats.save_all_dict_to_disk() # broken\n",
        "\n",
        "\n",
        "              "
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgZxBNciA61U"
      },
      "source": [
        "## Debugging\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "3v9zDxaDA55-",
        "outputId": "166c4945-a901-4ddb-f6a0-60b32a051610"
      },
      "source": [
        "param = {\n",
        "            'n_estimators': 100,\n",
        "            'objective': 'regression',\n",
        "            'boosting_type': 'gbdt',\n",
        "            'max_depth': 4,  \n",
        "            'learning_rate': .032, \n",
        "            'feature_fraction': .095, \n",
        "            'seed': 1 \n",
        "          }\n",
        "my_model = train_LGBMRegressor(param, train_data=TRAINING_DATA)\n",
        "my_predictions = my_model.predict(VALIDATION_DATA[FEATURES])\n",
        "valid_df = valid4score(VALIDATION_DATA, my_predictions, load_example=False, save=False)\n",
        "my_scores = score_summary(valid_df)\n",
        "my_model_stats = ModelStats(model=my_model, scores=my_scores, total_time=10)"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-91-9a33100d3d47>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0;34m'seed'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m           }\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmy_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_LGBMRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTRAINING_DATA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mmy_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVALIDATION_DATA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mFEATURES\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mvalid_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid4score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVALIDATION_DATA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_example\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-88-1343cb18b81b>\u001b[0m in \u001b[0;36mtrain_LGBMRegressor\u001b[0;34m(params, train_data)\u001b[0m\n\u001b[1;32m     90\u001b[0m   \"\"\"\n\u001b[1;32m     91\u001b[0m   \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLGBMRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mFEATURES\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTARGET\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[1;32m    683\u001b[0m                                        \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m                                        \u001b[0mcategorical_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m                                        callbacks=callbacks)\n\u001b[0m\u001b[1;32m    686\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[1;32m    542\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m                               \u001b[0mcategorical_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 544\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevals_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# construct booster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m         \u001b[0mbooster\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBooster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_valid_contain_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_train_data_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, train_set, model_file, silent)\u001b[0m\n\u001b[1;32m   1550\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1551\u001b[0m             _safe_call(_LIB.LGBM_BoosterCreate(\n\u001b[0;32m-> 1552\u001b[0;31m                 \u001b[0mtrain_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1553\u001b[0m                 \u001b[0mc_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1554\u001b[0m                 ctypes.byref(self.handle)))\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mconstruct\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    999\u001b[0m                                 \u001b[0minit_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predictor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1000\u001b[0m                                 \u001b[0msilent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msilent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1001\u001b[0;31m                                 categorical_feature=self.categorical_feature, params=self.params)\n\u001b[0m\u001b[1;32m   1002\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfree_raw_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1003\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m(self, data, label, reference, weight, group, init_score, predictor, silent, feature_name, categorical_feature, params)\u001b[0m\n\u001b[1;32m    789\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init_from_csc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 791\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init_from_np2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    792\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init_from_list_np2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36m__init_from_np2d\u001b[0;34m(self, mat, params_str, ref_dataset)\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mc_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m             \u001b[0mref_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m             ctypes.byref(self.handle)))\n\u001b[0m\u001b[1;32m    856\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mf0BRd77fZMA"
      },
      "source": [
        "# Testing some theories. \n",
        "\n",
        "Lets keep all but 1 variable constant and then create a Dataframe of all the scores for that changing variable.\n",
        "\n",
        "\n",
        "To start set n_estiamtors =1000 and try all the leaves between 2, and 32. \n",
        "\n",
        "save results to a dataframe.\n",
        "\n",
        "You need to let this run for like 30 minutes. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "The validation df might be a constant You might need to only make it once at set up and then just reference it each time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQM5saclfXq6",
        "outputId": "e161df9e-ee51-4696-c3f2-2e28966233f6"
      },
      "source": [
        "%%time\n",
        "param_set = [{\n",
        "            'n_estimators': 50,\n",
        "            'objective': 'regression',\n",
        "            'boosting_type': 'gbdt',\n",
        "            'max_depth': 4, # best max depth is 4\n",
        "            'learning_rate': .032,#.032 is best with sharpe >1 and depth =4\n",
        "            'feature_fraction': .095, # MIGHT BE WRONG. NEED TO DOUBLE CHECK. \n",
        "            'seed': i \n",
        "              } for i in range(1,2)] \n",
        "\n",
        "\n",
        "# broken\n",
        "for param in param_set:\n",
        "  train_validate_store(param, \n",
        "                       train_data=TRAINING_DATA, \n",
        "                       validation_data = VALIDATION_DATA, \n",
        "                       features=FEATURES)\n",
        "\n"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "here\n",
            "1\n",
            "2\n",
            "5\n",
            "4\n",
            "CPU times: user 38.7 s, sys: 1.69 s, total: 40.4 s\n",
            "Wall time: 23.1 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTW50gG3SJRq"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "tkNvSL6xMKut",
        "outputId": "1f6824b6-c6f3-446e-907b-64c2cec7e973"
      },
      "source": [
        "df = scores_already_saved_df = pd.read_csv('/content/drive/MyDrive/numerai_hyperparams_scores.csv')\n",
        "df"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>0.1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>10</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>259</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>gbdt</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>NaN</td>\n",
              "      <td>200000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61</th>\n",
              "      <td>NaN</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>62 rows  2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       0     0.1\n",
              "0     10     NaN\n",
              "1    259     NaN\n",
              "2   gbdt     NaN\n",
              "3    NaN     NaN\n",
              "4    1.0     NaN\n",
              "..   ...     ...\n",
              "57   NaN       1\n",
              "58   NaN     1.0\n",
              "59   NaN  200000\n",
              "60   NaN       0\n",
              "61   NaN      10\n",
              "\n",
              "[62 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSh7xmz7lrLk"
      },
      "source": [
        "y = all_scores_df.correlation\n",
        "x = all_scores_df.max_depth\n",
        "plt.plot(x,y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsRJSpJNMiKQ"
      },
      "source": [
        "# Submit to Numerai\n",
        "\n",
        "\n",
        "1. Create a prediction list.\n",
        "2. Link those predictions with the tournment data\n",
        "3. Write the id, prediction to a csv file.\n",
        "4. Use numerai wrapper to submit that .csv file as your current model. \n",
        "5. This submits for MRQUANTSALOT and TUTMODEL\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSo8yDCxTBB1"
      },
      "source": [
        "### Methods to handle submission"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EoYEwqg4MiKQ"
      },
      "source": [
        "def load_api_creds_into_dict(): # works\n",
        "  \"\"\"\n",
        "    Read creds.json and return a dictionary of your API keys.\n",
        "  \"\"\"\n",
        "  creds  = open('creds.json','r') # refactor this to point at your google drive. \n",
        "  api_keys_dict = json.load(creds)\n",
        "  creds.close()\n",
        "  return api_keys_dict\n",
        "\n",
        "\n",
        "def open_api_access(): # works\n",
        "    \"\"\"\n",
        "    Read in my private key from creds.json and return the numerai api wrapper\n",
        "    \"\"\"\n",
        "    api_keys_dict = load_api_creds_into_dict()\n",
        "    my_secret_key = api_keys_dict['secret_key']\n",
        "    my_public_id = api_keys_dict['public_id']\n",
        "    napi = numerapi.NumerAPI(secret_key=my_secret_key, public_id=my_public_id)\n",
        "    return napi\n",
        "\n",
        "\n",
        "def create_id_prediction_df(tournament_data: pd.DataFrame, model_predictions : np.ndarray): # works\n",
        "    \"\"\"\n",
        "    Create a dataframe that looks like \n",
        "    id,prediction\n",
        "    asdfads,.5429\n",
        "    asdfaddsss,.5051\n",
        "    ...\n",
        "    \"\"\"\n",
        "    predictions_df = tournament_data[\"id\"].to_frame() # get all the Ids and cast them to a Dataframe\n",
        "    predictions_df[PREDICTION_NAME] = model_predictions #add your predictions to the id frame\n",
        "    return predictions_df # data frame of id, prediction\n",
        "\n",
        "\n",
        "def write_predictions_to_file(prediction_df: pd.DataFrame): # works\n",
        "    my_file_name = 'myPredictions.csv'\n",
        "    try:\n",
        "      out_location = open(my_file_name, 'x')\n",
        "    except:\n",
        "      out_location = open(my_file_name, 'w')\n",
        "\n",
        "    prediction_df.to_csv(out_location, index=False)\n",
        "    out_location.close()\n",
        "    return my_file_name \n",
        "\n",
        "\n",
        "def run_model_and_create_prediction_file(model_object, tournament_data: pd.DataFrame, features: list):\n",
        "  \"\"\"\n",
        "    This stitches everything together.\n",
        "\n",
        "    Pass it a trained model and the tournament data set, the list of feature columns\n",
        "    1. Does preditions\n",
        "    2. write the predictions to a file.\n",
        "    3. returns the name fo the file where my predictions are saved data is saved\n",
        "  \"\"\"\n",
        "  model_predictions = model_object.predict(tournament_data[features])\n",
        "  prediction_df = create_id_prediction_df(tournament_data,model_predictions)\n",
        "  file_with_predictions = write_predictions_to_file(prediction_df)\n",
        "  return file_with_predictions\n",
        "\n",
        "\n",
        "def submit_predictions_to_numerai(filename_of_predictions, sumbit_model_id):\n",
        "    napi = open_api_access() # open a connection to the numerai API with your creds.json file\n",
        "    submission_id = napi.upload_predictions(filename_of_predictions, model_id=sumbit_model_id)\n",
        "    print(f'You successfully submitted for {sumbit_model_id}')\n",
        "    print(type(submission_id))\n",
        "    return submission_id\n",
        "\n",
        "print('your helper methods work correctly')\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54YIKJia7TNa"
      },
      "source": [
        "api_keys_dict = load_api_creds_into_dict()\n",
        "mrquantsalot_model_id = api_keys_dict['mr_quants_model_id']\n",
        "tut_model_model_id = api_keys_dict['tutmodel_model_id']\n",
        "PREDICTION_NAME = \"prediction\" # this is the header of the csv file you are creating\n",
        "OUTPUT_DIR = '' # just the root of your local folder in this instance of google colab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4vmuIio762C"
      },
      "source": [
        "### Run and submit MRQUANTSALOT\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8FKXKURMiKR"
      },
      "source": [
        "%%time\n",
        "if want_to_submit:\n",
        "  mr_quants_file_with_predictions = run_model_and_create_prediction_file(mr_quants_model, tournament_data, features)\n",
        "  numerai_submission_id_mrQ = submit_predictions_to_numerai(mr_quants_file_with_predictions, mrquantsalot_model_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3RY5VyX8C1w"
      },
      "source": [
        "### Run and submit TUT_MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ygq0jiVE8Cg0"
      },
      "source": [
        "%%time\n",
        "if want_to_submit:\n",
        "  tut_model_file_with_predictions = run_model_and_create_prediction_file(tut_model, tournament_data, features)\n",
        "  numerai_submission_id_tut = submit_predictions_to_numerai(tut_model_file_with_predictions, tut_model_model_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3aOmG--eQjt"
      },
      "source": [
        "## The params I am using now. No clear reason for these over another model. \n",
        "\n",
        "Default num_leaves =31\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rROXr1jBOvrg",
        "outputId": "524a2ccd-d024-4ffd-e8cd-89a309e13cb9"
      },
      "source": [
        "param_set = [{\n",
        "            'n_estimators': 500*i,\n",
        "            'objective': 'regression',\n",
        "            'boosting_type': 'gbdt',\n",
        "            'max_depth': 4, # max depth of each random forest\n",
        "            'learning_rate': 0.013, # the size of the step between trees\n",
        "            'feature_fraction': 0.095, # what % of features each tree will consider. you should root this at sqrt(310)/ 310 ~0.058\n",
        "            'seed': 42 # a random seed.\n",
        "            }for i in range(30,60,5)]\n",
        "\n",
        "# i expect overfitting as the n_estimatros gets larger so the sharpe and correlatino scores will go down after this point. \n",
        "\n",
        "# so far is matching thesroy. \n",
        "\n",
        "#~ 5k estimators seems about right.\n",
        "\n",
        "\n",
        "# tut_parmas = {\n",
        "#             'n_estimators': 3000,\n",
        "#             'objective': 'regression',\n",
        "#             'boosting_type': 'gbdt',\n",
        "#             'max_depth': 4,\n",
        "#             'learning_rate': 0.013, \n",
        "#             'feature_fraction': 0.095,\n",
        "#             'seed': 42\n",
        "#             } \n",
        "\n",
        "\n",
        "# this version of the param gets CORR = .0255 and a sharpe of .97\n",
        "for param in param_set: # this is fully parralizeable \n",
        "  model_stats =  train_validate_store(param,\n",
        "                             train_data=train_data,\n",
        "                             validation_data=valid,\n",
        "                             features=FEATURES)\n",
        "  headlines = model_stats.headlines() # this also gets creates the all_stats_dict\n",
        "  print(model_stats.all_stats_dict)\n",
        "  all_scores.append(model_stats.all_stats_dict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'total_time': 255.1076, 'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.013, 'max_depth': 4, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 5500, 'n_jobs': -1, 'num_leaves': 31, 'objective': 'regression', 'random_state': None, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'silent': 1, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'feature_fraction': 0.095, 'seed': 42, 'correlation': 0.0255, 'corr_sharpe': 0.9774, 'corr_mean': 0.0259, 'corr_std': 0.0265, 'max_drawdown': -0.0573, 'feature_exposure': 0.0747, 'max_feature_exposure': 0.2652}\n",
            "{'total_time': 359.9538, 'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.013, 'max_depth': 4, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 8000, 'n_jobs': -1, 'num_leaves': 31, 'objective': 'regression', 'random_state': None, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'silent': 1, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'feature_fraction': 0.095, 'seed': 42, 'correlation': 0.0243, 'corr_sharpe': 0.9753, 'corr_mean': 0.0247, 'corr_std': 0.0253, 'max_drawdown': -0.046, 'feature_exposure': 0.0702, 'max_feature_exposure': 0.2697}\n",
            "{'total_time': 479.5853, 'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.013, 'max_depth': 4, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 10500, 'n_jobs': -1, 'num_leaves': 31, 'objective': 'regression', 'random_state': None, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'silent': 1, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'feature_fraction': 0.095, 'seed': 42, 'correlation': 0.0233, 'corr_sharpe': 0.9334, 'corr_mean': 0.0238, 'corr_std': 0.0255, 'max_drawdown': -0.048, 'feature_exposure': 0.067, 'max_feature_exposure': 0.2685}\n",
            "{'total_time': 590.3642, 'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.013, 'max_depth': 4, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 13000, 'n_jobs': -1, 'num_leaves': 31, 'objective': 'regression', 'random_state': None, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'silent': 1, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'feature_fraction': 0.095, 'seed': 42, 'correlation': 0.0226, 'corr_sharpe': 0.8951, 'corr_mean': 0.023, 'corr_std': 0.0257, 'max_drawdown': -0.0515, 'feature_exposure': 0.0656, 'max_feature_exposure': 0.2727}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}