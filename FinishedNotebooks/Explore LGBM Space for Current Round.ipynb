{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7atJJ2fMiJ8"
      },
      "source": [
        "# This notebook explores the Hyper Parameter Space of the lightgbm library\n",
        "\n",
        "\n",
        "You might want to find out how corrilataed different seeds of the optimal hyper params are. Then submit 4 versions of it, that are the most un  corrilated. but 3 nmr on each of them\n",
        "\n",
        "\n",
        "Create a sub evaluation method to count the % of eras that you have >0 corr for. \n",
        "\n",
        "You might want  explor hyper parm for xgboost as well. and some other out of the box algos\n",
        "\n",
        "You might want a custom loss function of \"brair score\"  sum(true_outcome - prediction)^2 for all outcome, preiction in df. \n",
        "\n",
        "Put this in the validation scorring methods. add it to score summary\n",
        "\n",
        "You can do rank corrilation at each iteration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0iP-UgesMiKB",
        "outputId": "aaf361da-4d89-4e18-9e1f-267d57354623"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting numerapi\n",
            "  Downloading https://files.pythonhosted.org/packages/fd/96/ebdbaff5a2fef49b212e4f40634166f59e45462a768c0136d148f00255c5/numerapi-2.4.5-py3-none-any.whl\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from numerapi) (2018.9)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from numerapi) (2.23.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from numerapi) (2.8.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from numerapi) (7.1.2)\n",
            "Requirement already satisfied: tqdm>=4.29.1 in /usr/local/lib/python3.7/dist-packages (from numerapi) (4.41.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->numerapi) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->numerapi) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->numerapi) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->numerapi) (2.10)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil->numerapi) (1.15.0)\n",
            "Installing collected packages: numerapi\n",
            "Successfully installed numerapi-2.4.5\n"
          ]
        }
      ],
      "source": [
        "!pip install numerapi\r\n",
        "import numerapi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4WFeO-ZMiKC"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os, sys\n",
        "import gc\n",
        "import pathlib\n",
        "import json\n",
        "import datetime\n",
        "from typing import List, NoReturn, Union, Tuple, Optional, Text, Generic, Callable, Dict\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, QuantileTransformer\n",
        "from sklearn.model_selection import KFold, StratifiedKFold, TimeSeriesSplit\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, log_loss, mean_squared_error, mean_absolute_error, f1_score\n",
        "from scipy.stats import spearmanr # -P I think this is corr. \n",
        "import joblib\n",
        "\n",
        "# model\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "import operator\n",
        "\n",
        "# visualize\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.style as style\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot\n",
        "from matplotlib.ticker import ScalarFormatter\n",
        "sns.set_context(\"talk\")\n",
        "style.use('seaborn-colorblind')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0ELMvF79l3D"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b364c12-MiKD"
      },
      "source": [
        "## Methods to Gather and Clean Incoming Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmV-K6xHoqxW"
      },
      "outputs": [],
      "source": [
        "def create_global_variables()-> None:\n",
        "  \"\"\"\n",
        "    Create all global variables. \n",
        "    ROUND_NUMBER,FEATURES,TARGET,\n",
        "    TOURNAMENT_DATA,TRAINING_DATA,VALIDATION_DATA\n",
        "  \"\"\"\n",
        "  try:\n",
        "    if HAVE_GATHERED_DATA == FALSE:\n",
        "      ping_training_data()\n",
        "      ping_tournament_data()\n",
        "      create_validation_data(df = TOURNAMENT_DATA)\n",
        "\n",
        "      create_global_constants()\n",
        "      drop_data_type_columns()\n",
        "      HAVE_GATHERED_DATA = True\n",
        "  except NameError:\n",
        "      ping_training_data()\n",
        "      ping_tournament_data()\n",
        "      create_validation_data(df = TOURNAMENT_DATA)\n",
        "      \n",
        "      create_global_constants()\n",
        "      drop_data_type_columns()\n",
        "      HAVE_GATHERED_DATA = True\n",
        "\n",
        "def drop_data_type_columns():\n",
        "  TRAINING_DATA.drop(columns=[\"data_type\"], inplace=True)\n",
        "  VALIDATION_DATA.drop(columns=[\"data_type\"], inplace=True) #\n",
        "  TOURNAMENT_DATA.drop(columns=[\"data_type\"], inplace=True)\n",
        "\n",
        "def ping_training_data():\n",
        "  global TRAINING_DATA\n",
        "  TRAINING_DATA = read_data('train')\n",
        "  \n",
        "def ping_tournament_data():\n",
        "  global TOURNAMENT_DATA\n",
        "  TOURNAMENT_DATA = read_data('tournament')\n",
        "\n",
        "def create_validation_data(df):\n",
        "  global VALIDATION_DATA\n",
        "  VALIDATION_DATA  = df[df[\"data_type\"] == \"validation\"].reset_index(drop = True)\n",
        "\n",
        "def cast_eras_as_int(x): \n",
        "    try:\n",
        "        return int(x[3:]) # strip the first 3 characters from each era\n",
        "    except:\n",
        "        return -99\n",
        "\n",
        "def read_data(data):\n",
        "    if data == 'train':\n",
        "        df = pd.read_csv('https://numerai-public-datasets.s3-us-west-2.amazonaws.com/latest_numerai_training_data.csv.xz')\n",
        "    elif data == 'tournament':\n",
        "        df = pd.read_csv('https://numerai-public-datasets.s3-us-west-2.amazonaws.com/latest_numerai_tournament_data.csv.xz')\n",
        "        \n",
        "    feature_cols = df.columns[df.columns.str.startswith('feature')]\n",
        "    mapping = {0.0 : 0, 0.25 : 1, 0.5 : 2, 0.75 : 3, 1.0 : 4}\n",
        "\n",
        "    for c in feature_cols:\n",
        "        df[c] = df[c].map(mapping).astype(np.uint8)\n",
        "        \n",
        "    df[\"era\"] = df[\"era\"].apply(cast_eras_as_int)\n",
        "    return df\n",
        "\n",
        "\n",
        "def create_global_constants() -> None:\n",
        "  global TARGET\n",
        "  TARGET = get_target_constant(TOURNAMENT_DATA)\n",
        "  global FEATURES\n",
        "  FEATURES = get_features_constant(TOURNAMENT_DATA)\n",
        "  napi = open_api_access()\n",
        "  global ROUND_NUMBER\n",
        "  ROUND_NUMBER = napi.get_current_round()\n",
        "\n",
        "\n",
        "def get_target_constant(tournament_data: pd.DataFrame):\n",
        "  return tournament_data.columns[tournament_data.columns.str.startswith('target')].values.tolist()[0]\n",
        "\n",
        "\n",
        "def get_features_constant(tournament_data: pd.DataFrame):\n",
        "  return [column_names for column_names in tournament_data.columns.values.tolist() if 'feature' in column_names]\n",
        "\n",
        "\n",
        "\n",
        "def load_api_creds_into_dict():\n",
        "  creds  = open('/content/drive/MyDrive/creds.json','r') \n",
        "  api_keys_dict = json.load(creds)\n",
        "  creds.close()\n",
        "  return api_keys_dict\n",
        "\n",
        "\n",
        "def open_api_access():\n",
        "    api_keys_dict = load_api_creds_into_dict()\n",
        "    my_secret_key = api_keys_dict['secret_key']\n",
        "    my_public_id = api_keys_dict['public_id']\n",
        "    napi = numerapi.NumerAPI(secret_key=my_secret_key, public_id=my_public_id)\n",
        "    return napi\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBuYhojHFcQW"
      },
      "source": [
        "# Get the training and testing data and create the global variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "M3eMSLtXzspp",
        "outputId": "b8d8ce6c-bd92-4c27-a599-5bcc6d1b1a97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 5min 4s, sys: 11.2 s, total: 5min 15s\n",
            "Wall time: 5min 30s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "create_global_variables()\n",
        "HAVE_GATHERED_DATA = True\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4khTMbONzW-"
      },
      "source": [
        "### ModelStats Object\n",
        "\n",
        "1. Stores the Trained Model\n",
        "2. Stores the Hyper Parameters\n",
        "3. Stores the Validation Scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LCuE_km4Y_3"
      },
      "outputs": [],
      "source": [
        "PATH_TO_SAVE_SCORES = '/content/drive/MyDrive/numerai_hyperparams_scores.csv' # change this when you get different models.\r\n",
        "class ModelStats():\r\n",
        "  \"\"\"\r\n",
        "  An object that tracks Hyper Parameters, Time Costs and Scores. \r\n",
        "  \"\"\"\r\n",
        "  def __init__(self, model, scores:dict, total_time):\r\n",
        "        self.model = model \r\n",
        "        self.hyperparams = model.get_params() \r\n",
        "        self.scores = scores \r\n",
        "        self.total_time = total_time\r\n",
        "        self.params_scores_df = None \r\n",
        "\r\n",
        "\r\n",
        "  def create_params_scores_df(self):\r\n",
        "    \"\"\"\r\n",
        "    Create a DataFrame Representing the Hyper Parameters and Scores of this model.\r\n",
        "    \"\"\"\r\n",
        "    if self.params_scores_df == None:\r\n",
        "      all_stats_dict = {}\r\n",
        "      all_stats_dict['total_time'] = self.total_time\r\n",
        "      all_stats_dict['round_number'] = ROUND_NUMBER\r\n",
        "      all_stats_dict.update(self.hyperparams) # dict.update(dict) merges two dictionaries\r\n",
        "      all_stats_dict.update(self.scores)\r\n",
        "      DECIMALS = 4 \r\n",
        "      for key in all_stats_dict.keys():\r\n",
        "          try:\r\n",
        "            all_stats_dict[key] = [round(all_stats_dict[key],DECIMALS)]\r\n",
        "          except:\r\n",
        "            all_stats_dict[key] = [all_stats_dict[key]]\r\n",
        "\r\n",
        "      self.params_scores_df = pd.DataFrame.from_dict(all_stats_dict)\r\n",
        "\r\n",
        "  \r\n",
        "  def save_hyperparams_scores_to_google_drive_tabular(self)-> None:\r\n",
        "    \"\"\"\r\n",
        "        Appends this model's scores into your Google Drive with the other scores.\r\n",
        "    \"\"\"\r\n",
        "    self.create_params_scores_df()\r\n",
        "    # try to load that current df into memory\r\n",
        "    disk_df = pd.read_csv(PATH_TO_SAVE_SCORES)\r\n",
        "    #print(f'Read in new saved scores with {disk_df.shape} shape')\r\n",
        "    new_updated_disk_df = merge_dfs_horizontally(disk_df, self.params_scores_df)\r\n",
        "    #print(f'added next line of scores with {new_updated_disk_df.shape} shape')\r\n",
        "    new_updated_disk_df.to_csv(PATH_TO_SAVE_SCORES, index=False)\r\n",
        "    #print('Overwrote the new_updated_disk_df to your Google Drive')\r\n",
        "\r\n",
        "    try:\r\n",
        "      with open(PATH_TO_SAVE_SCORES, 'r') as scores_file:\r\n",
        "          lines = scores_file.readlines()\r\n",
        "          if len(lines) == 0:\r\n",
        "            print(\"the file does not exist. You are good to save your first score df\")\r\n",
        "    except:\r\n",
        "      self.params_scores_df.to_csv(PATH_TO_SAVE_SCORES, index=False)\r\n",
        "      # not exhaustively tested   \r\n",
        "           \r\n",
        "\r\n",
        "def merge_dfs_horizontally(df1 : pd.DataFrame, df2: pd.DataFrame)-> pd.DataFrame:\r\n",
        "  merged_df = pd.concat([df1, df2], axis=0)\r\n",
        "  return merged_df\r\n",
        "\r\n",
        "\r\n",
        "def train_LGBMRegressor(params: dict, train_data): \r\n",
        "  \"\"\"\r\n",
        "  Inputs: a dict of hyper paramaters for the model, \r\n",
        "  train_data: a pd.DataFrame of the Training Data\r\n",
        "\r\n",
        "  Returns a trained model\r\n",
        "  \"\"\"\r\n",
        "  model = lgb.LGBMRegressor(**params) \r\n",
        "  model.fit(train_data[FEATURES], train_data[TARGET])\r\n",
        "  return model\r\n",
        "\r\n",
        "def train_xgboost(train_data): \r\n",
        "  \"\"\"\r\n",
        "    Not deeply tested. Has much higher 10x time costs to train a single modle. \r\n",
        "    with these params it is low end but respectable. but takes about 2 hours to train via google colab\r\n",
        "  \"\"\"\r\n",
        "  model = xgb.XGBRegressor(max_depth=5, learning_rate=0.01, n_estimators=2000, n_jobs=-1, colsample_bytree=0.1)\r\n",
        "  model.fit(train_data[FEATURES], train_data[TARGET])\r\n",
        "  return model\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bWSBkk2_m3h"
      },
      "source": [
        "#### Methods to Determine Validation Scores\n",
        "\n",
        "1. I did not write these. I added the English comments\n",
        "source https://www.kaggle.com/code1110/numerai-tournament"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "szXbJM0mMiKJ"
      },
      "outputs": [],
      "source": [
        "# naming conventions\n",
        "PREDICTION_NAME = 'prediction'\n",
        "TARGET_NAME = TARGET # 'target is the string named 'target'\n",
        "# EXAMPLE_PRED = 'example_prediction'\n",
        "\n",
        "# ---------------------------\n",
        "# Functions\n",
        "# ---------------------------\n",
        "def valid4score(valid : pd.DataFrame, pred : np.ndarray, load_example: bool=True, save : bool=False) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Generate new valid pandas dataframe for computing scores\n",
        "    \n",
        "    :INPUT:\n",
        "    - valid : pd.DataFrame extracted from tournament data (data_type='validation')\n",
        "    \n",
        "    \"\"\"\n",
        "    valid_df = valid.copy() # the validation dataframe you use this to test the CORR and other values\n",
        "\n",
        "    # Your model creates an array of floats [0,1] rank method converst them in a list of ints. \n",
        "\n",
        "    # your lis tof ints is then compared to their list of ints. \n",
        "    valid_df['prediction'] = pd.Series(pred).rank(pct=True, method=\"first\") # pred is the array of predictions your model creates for the set of validation vectors.  \n",
        "    # I am unsure if this preds is a float only only between 0,1,2,3,4. \n",
        "    valid_df.rename(columns={TARGET: 'target'}, inplace=True)\n",
        "    \n",
        "    # I don't know what the load example boolean is. I think you can use this to save predictions.\n",
        "    if load_example:\n",
        "        valid_df[EXAMPLE_PRED] = pd.read_csv(EXP_DIR + 'valid_df.csv')['prediction'].values\n",
        "    \n",
        "    if save==True:\n",
        "        valid_df.to_csv(OUTPUT_DIR + 'valid_df.csv', index=False)\n",
        "        print('Validation dataframe saved!')\n",
        "    \n",
        "    return valid_df\n",
        "\n",
        "def compute_corr(valid_df : pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Compute rank correlation\n",
        "\n",
        "    THIS IS WHAT YOU ARE PRIMARILY PAID ON \n",
        "    \n",
        "    :INPUT:\n",
        "    - valid_df : pd.DataFrame where at least 2 columns ('prediction' & 'target') exist\n",
        "    \n",
        "    \"\"\"\n",
        "    # this uses Person Correilation. \n",
        "    # I You are paid on spearman corrilation. That is where the ratio of change is important not the raw amount of change\n",
        "    # see: https://support.minitab.com/en-us/minitab-express/1/help-and-how-to/modeling-statistics/regression/supporting-topics/basics/a-comparison-of-the-pearson-and-spearman-correlation-methods/\n",
        "    return np.corrcoef(valid_df[\"target\"], valid_df['prediction'])[0, 1]\n",
        "\n",
        "def compute_max_drawdown(validation_correlations : pd.Series):\n",
        "    \"\"\"\n",
        "    Compute max drawdown\n",
        "    \n",
        "    :INPUT:\n",
        "    - validation_correaltions : pd.Series\n",
        "    \"\"\"\n",
        "    \n",
        "    rolling_max = (validation_correlations + 1).cumprod().rolling(window=100, min_periods=1).max()\n",
        "    daily_value = (validation_correlations + 1).cumprod()\n",
        "    max_drawdown = -(rolling_max - daily_value).max()\n",
        "    \n",
        "    return max_drawdown\n",
        "\n",
        "def compute_val_corr(valid_df : pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Compute rank correlation for valid periods\n",
        "    \n",
        "    :INPUT:\n",
        "    - valid_df : pd.DataFrame where at least 2 columns ('prediction' & 'target') exist\n",
        "    \"\"\"\n",
        "    \n",
        "    # all validation\n",
        "    correlation = compute_corr(valid_df)\n",
        "    #print(\"rank corr = {:.4f}\".format(correlation))\n",
        "    return correlation\n",
        "    \n",
        "def compute_val_sharpe(valid_df : pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Compute sharpe ratio for valid periods\n",
        "    \n",
        "    :INPUT:\n",
        "    - valid_df : pd.DataFrame where at least 2 columns ('prediction' & 'target') exist\n",
        "    \"\"\"\n",
        "    # all validation\n",
        "    d = valid_df.groupby('era')[['target', 'prediction']].corr().iloc[0::2,-1].reset_index()\n",
        "    me = d['prediction'].mean()\n",
        "    sd = d['prediction'].std()\n",
        "    max_drawdown = compute_max_drawdown(d['prediction'])\n",
        "    #print('sharpe ratio = {:.4f}, corr mean = {:.4f}, corr std = {:.4f}, max drawdown = {:.4f}'.format(me / sd, me, sd, max_drawdown))\n",
        "    \n",
        "    return me / sd, me, sd, max_drawdown\n",
        "    \n",
        "def feature_exposures(valid_df : pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Compute feature exposure\n",
        "    \n",
        "    :INPUT:\n",
        "    - valid_df : pd.DataFrame where at least 2 columns ('prediction' & 'target') exist\n",
        "    \"\"\"\n",
        "    feature_names = [f for f in valid_df.columns\n",
        "                     if f.startswith(\"feature\")]\n",
        "    exposures = []\n",
        "    for f in feature_names:\n",
        "        fe = spearmanr(valid_df['prediction'], valid_df[f])[0]\n",
        "        exposures.append(fe)\n",
        "    return np.array(exposures)\n",
        "\n",
        "def max_feature_exposure(fe : np.ndarray):\n",
        "    return np.max(np.abs(fe))\n",
        "\n",
        "def feature_exposure(fe : np.ndarray):\n",
        "    return np.sqrt(np.mean(np.square(fe)))\n",
        "\n",
        "def compute_val_feature_exposure(valid_df : pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Compute feature exposure for valid periods\n",
        "    \n",
        "    :INPUT:\n",
        "    - valid_df : pd.DataFrame where at least 2 columns ('prediction' & 'target') exist\n",
        "    \"\"\"\n",
        "    # all validation\n",
        "    fe = feature_exposures(valid_df)\n",
        "    fe1, fe2 = feature_exposure(fe), max_feature_exposure(fe)\n",
        "    #print('feature exposure = {:.4f}, max feature exposure = {:.4f}'.format(fe1, fe2))\n",
        "     \n",
        "    return fe1, fe2\n",
        "\n",
        "# to neutralize a column in a df by many other columns\n",
        "#         I have no idea what this method does. -P. need to read about it and write up a link to it. \n",
        "def neutralize(df, columns, by, proportion=1.0):\n",
        "    scores = df.loc[:, columns]\n",
        "    exposures = df[by].values\n",
        "\n",
        "    # constant column to make sure the series is completely neutral to exposures\n",
        "    exposures = np.hstack(\n",
        "        (exposures,\n",
        "         np.asarray(np.mean(scores)) * np.ones(len(exposures)).reshape(-1, 1)))\n",
        "\n",
        "    scores = scores - proportion * exposures.dot(\n",
        "        np.linalg.pinv(exposures).dot(scores))\n",
        "    return scores / scores.std()\n",
        "\n",
        "\n",
        "# to neutralize any series by any other series\n",
        "def neutralize_series(series, by, proportion=1.0):\n",
        "    scores = series.values.reshape(-1, 1)\n",
        "    exposures = by.values.reshape(-1, 1)\n",
        "\n",
        "    # this line makes series neutral to a constant column so that it's centered and for sure gets corr 0 with exposures\n",
        "    exposures = np.hstack(\n",
        "        (exposures,\n",
        "         np.array([np.mean(series)] * len(exposures)).reshape(-1, 1)))\n",
        "\n",
        "    correction = proportion * (exposures.dot(\n",
        "        np.linalg.lstsq(exposures, scores, rcond=None)[0]))\n",
        "    corrected_scores = scores - correction\n",
        "    neutralized = pd.Series(corrected_scores.ravel(), index=series.index)\n",
        "    return neutralized\n",
        "\n",
        "\n",
        "def unif(df):\n",
        "    x = (df.rank(method=\"first\") - 0.5) / len(df)\n",
        "    return pd.Series(x, index=df.index)\n",
        "\n",
        "def get_feature_neutral_mean(df):\n",
        "    feature_cols = [c for c in df.columns if c.startswith(\"feature\")]\n",
        "    df.loc[:, \"neutral_sub\"] = neutralize(df, [PREDICTION_NAME],\n",
        "                                          feature_cols)[PREDICTION_NAME]\n",
        "    scores = df.groupby(\"era\").apply(\n",
        "        lambda x: np.corrcoef(x[\"neutral_sub\"].rank(pct=True, method=\"first\"), x[TARGET_NAME])).mean()\n",
        "    return np.mean(scores)\n",
        "\n",
        "def compute_val_mmc(valid_df : pd.DataFrame):    \n",
        "    # MMC over validation\n",
        "    mmc_scores = []\n",
        "    corr_scores = []\n",
        "    for _, x in valid_df.groupby(\"era\"):\n",
        "        series = neutralize_series(pd.Series(unif(x[PREDICTION_NAME])),\n",
        "                                   pd.Series(unif(x[EXAMPLE_PRED])))\n",
        "        mmc_scores.append(np.cov(series, x[TARGET_NAME])[0, 1] / (0.29 ** 2)) # I have no idea what htis line does (0.29 ** 2)\n",
        "        corr_scores.append(np.corrcoef(unif(x[PREDICTION_NAME]).rank(pct=True, method=\"first\"), x[TARGET_NAME]))\n",
        "\n",
        "    val_mmc_mean = np.mean(mmc_scores)\n",
        "    val_mmc_std = np.std(mmc_scores)\n",
        "    val_mmc_sharpe = val_mmc_mean / val_mmc_std\n",
        "    corr_plus_mmcs = [c + m for c, m in zip(corr_scores, mmc_scores)]\n",
        "    corr_plus_mmc_sharpe = np.mean(corr_plus_mmcs) / np.std(corr_plus_mmcs)\n",
        "    corr_plus_mmc_mean = np.mean(corr_plus_mmcs)\n",
        "\n",
        "    #print(\"MMC Mean = {:.6f}, MMC Std = {:.6f}, CORR+MMC Sharpe = {:.4f}\".format(val_mmc_mean, val_mmc_std, corr_plus_mmc_sharpe))\n",
        "\n",
        "    # Check correlation with example predictions\n",
        "    corr_with_example_preds = np.corrcoef(valid_df[EXAMPLE_PRED].rank(pct=True, method=\"first\"),\n",
        "                                          valid_df[PREDICTION_NAME].rank(pct=True, method=\"first\"))[0, 1]\n",
        "    #print(\"Corr with example preds: {:.4f}\".format(corr_with_example_preds))\n",
        "    \n",
        "    return val_mmc_mean, val_mmc_std, corr_plus_mmc_sharpe, corr_with_example_preds\n",
        "\n",
        "\n",
        "# this is the main method. The rest are just called interanlly. \n",
        "def score_summary(valid_df : pd.DataFrame):\n",
        "    score_dict = {}\n",
        "    \n",
        "    try:\n",
        "        score_dict['correlation'] = compute_val_corr(valid_df)\n",
        "    except:\n",
        "        print('ERR: computing correlation')\n",
        "    try:\n",
        "        score_dict['corr_sharpe'], score_dict['corr_mean'], score_dict['corr_std'], score_dict['max_drawdown'] = compute_val_sharpe(valid_df)\n",
        "    except:\n",
        "        print('ERR: computing sharpe')\n",
        "    try:\n",
        "        score_dict['feature_exposure'], score_dict['max_feature_exposure'] = compute_val_feature_exposure(valid_df)\n",
        "    except:\n",
        "        print('ERR: computing feature exposure')\n",
        "    # try:\n",
        "    #     score_dict['mmc_mean'], score_dict['mmc_std'], score_dict['corr_mmc_sharpe'], score_dict['corr_with_example_xgb'] = compute_val_mmc(valid_df)\n",
        "    # except:\n",
        "    #     print('ERR: computing MMC')\n",
        "    \n",
        "    return score_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZlvD8U4bv072"
      },
      "outputs": [],
      "source": [
        "def look_at_best_models_so_far():\n",
        "  df = load_saved_params()\n",
        "  filter = df['correlation'] >.02\n",
        "  best = df[filter].sort_values(by ='correlation', ascending=False).head(20)\n",
        "  best.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lBD0oUmBWzx"
      },
      "source": [
        "### Command and cotrol methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80k6-VBtBWhf"
      },
      "outputs": [],
      "source": [
        "def train_validate_store(params:dict, train_data: pd.DataFrame, validation_data: pd.DataFrame):\r\n",
        "  \"\"\"\r\n",
        "    Create a LGBM model based on the hyper paramters in params trained on train_data.\r\n",
        "    Compute validation scores from the validation_data.\r\n",
        "    Append the hyperparams and scores to a .csv file in your Google Drive.\r\n",
        "    Silent: a boolean if you want to see a CORR score for this call\r\n",
        "  \"\"\"\r\n",
        "  start_time = datetime.datetime.now()\r\n",
        "  my_model = train_LGBMRegressor(params=params, train_data=train_data)\r\n",
        "  my_predictions = my_model.predict(validation_data[FEATURES])\r\n",
        "  valid_df = valid4score(validation_data, my_predictions, load_example=False, save=False)\r\n",
        "  my_scores = score_summary(valid_df)\r\n",
        "  my_total_time = (datetime.datetime.now() - start_time).total_seconds() \r\n",
        "  my_model_stats = ModelStats(model=my_model, scores=my_scores, total_time=my_total_time)\r\n",
        "  my_model_stats.save_hyperparams_scores_to_google_drive_tabular()\r\n",
        "  print(round(my_model_stats.scores['correlation'], 4), end=' ')\r\n",
        "  print(round(my_model_stats.scores['corr_sharpe'], 4))\r\n",
        "\r\n",
        "def load_saved_params():\r\n",
        "  return pd.read_csv(PATH_TO_SAVE_SCORES)\r\n",
        "\r\n",
        "\r\n",
        "def create_score_summary(model):\r\n",
        "  my_predictions = model.predict(VALIDATION_DATA[FEATURES])\r\n",
        "  valid_df = valid4score(VALIDATION_DATA, my_predictions, load_example=False, save=False)\r\n",
        "  return score_summary(valid_df)\r\n",
        "\r\n",
        "\r\n",
        "def generate_param_set():\r\n",
        "  \"\"\"\r\n",
        "  Create a set of hyper parameters to test on the validation data.  \r\n",
        "  Returns a list of dictionaries\r\n",
        "  \"\"\"\r\n",
        "  param_set=[]\r\n",
        "  for i in range(20):\r\n",
        "      for n_estimators in range(2800,4000,100):\r\n",
        "          param_set.append({\r\n",
        "                'n_estimators': n_estimators,\r\n",
        "                'objective': 'regression',\r\n",
        "                'boosting_type': 'gbdt',\r\n",
        "                'max_depth': 4,\r\n",
        "                'learning_rate': round(np.random.uniform(.02,.05),3),\r\n",
        "                'feature_fraction': round(np.random.uniform(0.1,.4),3), \r\n",
        "                'seed': 42 # exhaustive study has proven this to be the best possible seed. (joke)\r\n",
        "                  })\r\n",
        "  \r\n",
        "  return param_set\r\n",
        "\r\n",
        "\r\n",
        "def explore_lgbm():\r\n",
        "  \"\"\"\r\n",
        "    The main function to explore the hyper parameter space on the current data.\r\n",
        "  \"\"\"\r\n",
        "  param_set = generate_param_set()\r\n",
        "  for index, param in enumerate(param_set):\r\n",
        "    train_validate_store(param,TRAINING_DATA,VALIDATION_DATA)\r\n",
        "    print(f'Completed {index} models of {len(param_set)}')\r\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyHk97VIYFdj",
        "outputId": "f2e49cf5-b5ed-4b04-8735-c08c32a01cef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.0226 0.9108\n",
            "Completed 0 models of 240\n",
            "0.0239 0.9458\n",
            "Completed 1 models of 240\n",
            "0.0222 0.9735\n",
            "Completed 2 models of 240\n",
            "0.021 0.8563\n",
            "Completed 3 models of 240\n",
            "0.0221 0.944\n",
            "Completed 4 models of 240\n",
            "0.0237 0.9368\n",
            "Completed 5 models of 240\n",
            "0.022 0.8682\n",
            "Completed 6 models of 240\n",
            "0.0236 0.9158\n",
            "Completed 7 models of 240\n",
            "0.0202 0.8234\n",
            "Completed 8 models of 240\n",
            "0.0213 0.9102\n",
            "Completed 9 models of 240\n",
            "0.0233 0.888\n",
            "Completed 10 models of 240\n",
            "0.0196 0.7821\n",
            "Completed 11 models of 240\n",
            "0.0236 0.913\n",
            "Completed 12 models of 240\n",
            "0.0214 0.8742\n",
            "Completed 13 models of 240\n",
            "0.0236 0.9103\n",
            "Completed 14 models of 240\n",
            "0.0226 0.8939\n",
            "Completed 15 models of 240\n",
            "0.0221 0.8638\n",
            "Completed 16 models of 240\n",
            "0.0227 0.9511\n",
            "Completed 17 models of 240\n",
            "0.024 0.9309\n",
            "Completed 18 models of 240\n",
            "0.0226 0.9464\n",
            "Completed 19 models of 240\n",
            "0.0241 0.9058\n",
            "Completed 20 models of 240\n",
            "0.0226 0.9233\n",
            "Completed 21 models of 240\n",
            "0.0219 0.904\n",
            "Completed 22 models of 240\n",
            "0.021 0.844\n",
            "Completed 23 models of 240\n",
            "0.0228 0.8993\n",
            "Completed 24 models of 240\n",
            "0.0225 0.8856\n",
            "Completed 25 models of 240\n",
            "0.0223 0.8677\n",
            "Completed 26 models of 240\n",
            "0.0222 0.9073\n",
            "Completed 27 models of 240\n",
            "0.0223 0.8459\n",
            "Completed 28 models of 240\n",
            "0.0234 0.9133\n",
            "Completed 29 models of 240\n",
            "0.0204 0.8586\n",
            "Completed 30 models of 240\n",
            "0.0239 0.9651\n",
            "Completed 31 models of 240\n",
            "0.0237 0.9058\n",
            "Completed 32 models of 240\n",
            "0.0232 0.978\n",
            "Completed 33 models of 240\n",
            "0.021 0.7932\n",
            "Completed 34 models of 240\n",
            "0.0217 0.8844\n",
            "Completed 35 models of 240\n",
            "0.0223 0.8386\n",
            "Completed 36 models of 240\n",
            "0.0232 0.9304\n",
            "Completed 37 models of 240\n",
            "0.0245 0.945\n",
            "Completed 38 models of 240\n",
            "0.0239 0.9165\n",
            "Completed 39 models of 240\n",
            "0.022 0.9163\n",
            "Completed 40 models of 240\n",
            "0.0228 0.9983\n",
            "Completed 41 models of 240\n",
            "0.0227 0.8781\n",
            "Completed 42 models of 240\n",
            "0.0209 0.8347\n",
            "Completed 43 models of 240\n",
            "0.0214 0.8617\n",
            "Completed 44 models of 240\n",
            "0.0234 0.9214\n",
            "Completed 45 models of 240\n",
            "0.0213 0.8639\n",
            "Completed 46 models of 240\n",
            "0.0188 0.7752\n",
            "Completed 47 models of 240\n",
            "0.0221 0.8735\n",
            "Completed 48 models of 240\n",
            "0.0233 0.9783\n",
            "Completed 49 models of 240\n",
            "0.0204 0.7986\n",
            "Completed 50 models of 240\n",
            "0.0222 0.8841\n",
            "Completed 51 models of 240\n",
            "0.0203 0.8275\n",
            "Completed 52 models of 240\n",
            "0.0205 0.8739\n",
            "Completed 53 models of 240\n",
            "0.0215 0.8564\n",
            "Completed 54 models of 240\n",
            "0.0235 0.9724\n",
            "Completed 55 models of 240\n",
            "0.023 0.9232\n",
            "Completed 56 models of 240\n",
            "0.0216 0.9047\n",
            "Completed 57 models of 240\n",
            "0.0229 0.8897\n",
            "Completed 58 models of 240\n",
            "0.0207 0.8299\n",
            "Completed 59 models of 240\n",
            "0.0235 0.9108\n",
            "Completed 60 models of 240\n",
            "0.0204 0.7996\n",
            "Completed 61 models of 240\n",
            "0.0236 0.9551\n",
            "Completed 62 models of 240\n",
            "0.0215 0.8657\n",
            "Completed 63 models of 240\n",
            "0.0222 0.8907\n",
            "Completed 64 models of 240\n",
            "0.0233 0.9363\n",
            "Completed 65 models of 240\n",
            "0.0213 0.8515\n",
            "Completed 66 models of 240\n",
            "0.0218 0.8931\n",
            "Completed 67 models of 240\n",
            "0.0225 0.9283\n",
            "Completed 68 models of 240\n",
            "0.0212 0.8464\n",
            "Completed 69 models of 240\n",
            "0.0232 0.914\n",
            "Completed 70 models of 240\n",
            "0.0209 0.8085\n",
            "Completed 71 models of 240\n",
            "0.0235 0.9527\n",
            "Completed 72 models of 240\n",
            "0.0238 0.923\n",
            "Completed 73 models of 240\n",
            "0.0226 0.9356\n",
            "Completed 74 models of 240\n",
            "0.0245 0.9386\n",
            "Completed 75 models of 240\n",
            "0.0226 0.9576\n",
            "Completed 76 models of 240\n",
            "0.0225 0.8861\n",
            "Completed 77 models of 240\n",
            "0.024 0.9554\n",
            "Completed 78 models of 240\n",
            "0.0231 0.9234\n",
            "Completed 79 models of 240\n",
            "0.0211 0.8795\n",
            "Completed 80 models of 240\n",
            "0.023 0.8903\n",
            "Completed 81 models of 240\n",
            "0.0204 0.8088\n",
            "Completed 82 models of 240\n",
            "0.0215 0.8844\n",
            "Completed 83 models of 240\n",
            "0.0238 0.9572\n",
            "Completed 84 models of 240\n",
            "0.0222 0.8719\n",
            "Completed 85 models of 240\n",
            "0.0229 0.8447\n",
            "Completed 86 models of 240\n",
            "0.0212 0.8873\n",
            "Completed 87 models of 240\n",
            "0.0233 0.8995\n",
            "Completed 88 models of 240\n",
            "0.0209 0.8971\n",
            "Completed 89 models of 240\n",
            "0.0206 0.8252\n",
            "Completed 90 models of 240\n",
            "0.0223 0.8758\n",
            "Completed 91 models of 240\n",
            "0.0236 0.9315\n",
            "Completed 92 models of 240\n",
            "0.0212 0.8446\n",
            "Completed 93 models of 240\n",
            "0.0226 0.8901\n",
            "Completed 94 models of 240\n",
            "0.0191 0.7146\n",
            "Completed 95 models of 240\n",
            "0.0235 0.9291\n",
            "Completed 96 models of 240\n",
            "0.0217 0.8612\n",
            "Completed 97 models of 240\n",
            "0.0205 0.8075\n",
            "Completed 98 models of 240\n",
            "0.0244 0.9527\n",
            "Completed 99 models of 240\n",
            "0.024 0.9519\n",
            "Completed 100 models of 240\n",
            "0.0229 0.9084\n",
            "Completed 101 models of 240\n",
            "0.0219 0.8897\n",
            "Completed 102 models of 240\n",
            "0.0211 0.8146\n",
            "Completed 103 models of 240\n",
            "0.0212 0.8252\n",
            "Completed 104 models of 240\n"
          ]
        }
      ],
      "source": [
        "explore_lgbm()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "b364c12-MiKD"
      ],
      "machine_shape": "hm",
      "name": "Refactored Max Datapoints.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.8.3 64-bit ('flask_program': conda)",
      "name": "python383jvsc74a57bd0dd8651ccd8409a0bef7a44cb2e2575b0c1d32f8c24ba3ddcee5a1054a712ef8f"
    },
    "language_info": {
      "name": "python",
      "version": ""
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}