{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Refactored Max Datapoints.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "b364c12-MiKD",
        "7bWSBkk2_m3h",
        "4lBD0oUmBWzx"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parkerburchett/Numerai/blob/Refactor-Max-DataPoints/Refactored_Max_Datapoints.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7atJJ2fMiJ8"
      },
      "source": [
        "# This notebook explores the Hyper Parameter Space of the lightgbm library\n",
        "\n",
        "\n",
        "You might want to find out how corrilataed different seeds of the optimal hyper params are. Then submit 4 versions of it, that are the most un  corrilated. but 3 nmr on each of them\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0iP-UgesMiKB",
        "outputId": "31121ca7-404e-416c-a23e-70c5b4b0eb3f"
      },
      "source": [
        "!pip install numerapi\n",
        "import numerapi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting numerapi\n",
            "  Downloading https://files.pythonhosted.org/packages/fd/96/ebdbaff5a2fef49b212e4f40634166f59e45462a768c0136d148f00255c5/numerapi-2.4.5-py3-none-any.whl\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from numerapi) (2018.9)\n",
            "Requirement already satisfied: tqdm>=4.29.1 in /usr/local/lib/python3.7/dist-packages (from numerapi) (4.41.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from numerapi) (7.1.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from numerapi) (2.8.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from numerapi) (2.23.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil->numerapi) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->numerapi) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->numerapi) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->numerapi) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->numerapi) (3.0.4)\n",
            "Installing collected packages: numerapi\n",
            "Successfully installed numerapi-2.4.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4WFeO-ZMiKC"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os, sys\n",
        "import gc\n",
        "import pathlib\n",
        "import json\n",
        "import datetime\n",
        "from typing import List, NoReturn, Union, Tuple, Optional, Text, Generic, Callable, Dict\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, QuantileTransformer\n",
        "from sklearn.model_selection import KFold, StratifiedKFold, TimeSeriesSplit\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, log_loss, mean_squared_error, mean_absolute_error, f1_score\n",
        "from scipy.stats import spearmanr # -P I think this is corr. \n",
        "import joblib\n",
        "\n",
        "# model\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "import operator\n",
        "\n",
        "# visualize\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.style as style\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot\n",
        "from matplotlib.ticker import ScalarFormatter\n",
        "sns.set_context(\"talk\")\n",
        "style.use('seaborn-colorblind')\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0ELMvF79l3D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db04ecb8-8e31-4904-d424-bd91ddf10a20"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b364c12-MiKD"
      },
      "source": [
        "## Methods to Gather and Clean Incoming Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmV-K6xHoqxW"
      },
      "source": [
        "def create_global_variables()-> None:\n",
        "  \"\"\"\n",
        "    Create all global variables. \n",
        "    ROUND_NUMBER,FEATURES,TARGET,\n",
        "    TOURNAMENT_DATA,TRAINING_DATA,VALIDATION_DATA\n",
        "  \"\"\"\n",
        "  try:\n",
        "    if HAVE_GATHERED_DATA == FALSE:\n",
        "      ping_training_data()\n",
        "      ping_tournament_data()\n",
        "      create_validation_data(df = TOURNAMENT_DATA)\n",
        "\n",
        "      create_global_constants()\n",
        "      drop_data_type_columns()\n",
        "      HAVE_GATHERED_DATA = True\n",
        "  except NameError:\n",
        "      ping_training_data()\n",
        "      ping_tournament_data()\n",
        "      create_validation_data(df = TOURNAMENT_DATA)\n",
        "      \n",
        "      create_global_constants()\n",
        "      drop_data_type_columns()\n",
        "      HAVE_GATHERED_DATA = True\n",
        "\n",
        "def drop_data_type_columns():\n",
        "  TRAINING_DATA.drop(columns=[\"data_type\"], inplace=True)\n",
        "  VALIDATION_DATA.drop(columns=[\"data_type\"], inplace=True) #\n",
        "  TOURNAMENT_DATA.drop(columns=[\"data_type\"], inplace=True)\n",
        "\n",
        "def ping_training_data():\n",
        "  global TRAINING_DATA\n",
        "  TRAINING_DATA = read_data('train')\n",
        "  \n",
        "def ping_tournament_data():\n",
        "  global TOURNAMENT_DATA\n",
        "  TOURNAMENT_DATA = read_data('tournament')\n",
        "\n",
        "def create_validation_data(df):\n",
        "  global VALIDATION_DATA\n",
        "  VALIDATION_DATA  = df[df[\"data_type\"] == \"validation\"].reset_index(drop = True)\n",
        "\n",
        "def cast_eras_as_int(x): \n",
        "    try:\n",
        "        return int(x[3:]) # strip the first 3 characters from each era\n",
        "    except:\n",
        "        return -99\n",
        "\n",
        "def read_data(data):\n",
        "    if data == 'train':\n",
        "        df = pd.read_csv('https://numerai-public-datasets.s3-us-west-2.amazonaws.com/latest_numerai_training_data.csv.xz')\n",
        "    elif data == 'tournament':\n",
        "        df = pd.read_csv('https://numerai-public-datasets.s3-us-west-2.amazonaws.com/latest_numerai_tournament_data.csv.xz')\n",
        "        \n",
        "    feature_cols = df.columns[df.columns.str.startswith('feature')]\n",
        "    mapping = {0.0 : 0, 0.25 : 1, 0.5 : 2, 0.75 : 3, 1.0 : 4}\n",
        "\n",
        "    for c in feature_cols:\n",
        "        df[c] = df[c].map(mapping).astype(np.uint8)\n",
        "        \n",
        "    df[\"era\"] = df[\"era\"].apply(cast_eras_as_int)\n",
        "    return df\n",
        "\n",
        "\n",
        "def create_global_constants() -> None:\n",
        "  global TARGET\n",
        "  TARGET = get_target_constant(TOURNAMENT_DATA)\n",
        "  global FEATURES\n",
        "  FEATURES = get_features_constant(TOURNAMENT_DATA)\n",
        "  napi = open_api_access()\n",
        "  global ROUND_NUMBER\n",
        "  ROUND_NUMBER = napi.get_current_round()\n",
        "\n",
        "\n",
        "def get_target_constant(tournament_data: pd.DataFrame):\n",
        "  return tournament_data.columns[tournament_data.columns.str.startswith('target')].values.tolist()[0]\n",
        "\n",
        "\n",
        "def get_features_constant(tournament_data: pd.DataFrame):\n",
        "  return [column_names for column_names in tournament_data.columns.values.tolist() if 'feature' in column_names]\n",
        "\n",
        "\n",
        "\n",
        "def load_api_creds_into_dict():\n",
        "  creds  = open('/content/drive/MyDrive/creds.json','r') \n",
        "  api_keys_dict = json.load(creds)\n",
        "  creds.close()\n",
        "  return api_keys_dict\n",
        "\n",
        "\n",
        "def open_api_access():\n",
        "    api_keys_dict = load_api_creds_into_dict()\n",
        "    my_secret_key = api_keys_dict['secret_key']\n",
        "    my_public_id = api_keys_dict['public_id']\n",
        "    napi = numerapi.NumerAPI(secret_key=my_secret_key, public_id=my_public_id)\n",
        "    return napi\n",
        "\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBuYhojHFcQW"
      },
      "source": [
        "# Get the training and testing data and create the global variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3eMSLtXzspp",
        "outputId": "eb23e82f-637b-4269-8a54-bb316b9ff751"
      },
      "source": [
        "%%time\n",
        "create_global_variables()\n",
        "HAVE_GATHERED_DATA = True\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 6min 2s, sys: 14.2 s, total: 6min 16s\n",
            "Wall time: 6min 30s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4khTMbONzW-"
      },
      "source": [
        "### ModelStats Object\n",
        "\n",
        "1. Stores the Trained Model\n",
        "2. Stores the Hyper Parameters\n",
        "3. Stores the Validation Scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LCuE_km4Y_3"
      },
      "source": [
        "PATH_TO_SAVE_SCORES = '/content/drive/MyDrive/numerai_hyperparams_scores.csv'\n",
        "class ModelStats():\n",
        "  \"\"\"\n",
        "  An object that tracks Hyper Parameters, Time Costs and Scores. \n",
        "  \"\"\"\n",
        "  def __init__(self, model, scores:dict, total_time):\n",
        "        self.model = model \n",
        "        self.hyperparams = model.get_params() \n",
        "        self.scores = scores \n",
        "        self.total_time = total_time\n",
        "        self.params_scores_df = None \n",
        "\n",
        "\n",
        "  def create_params_scores_df(self):\n",
        "    \"\"\"\n",
        "    Create a DataFrame Representing the Hyper Parameters and Scores of this model\n",
        "    \"\"\"\n",
        "    if self.params_scores_df == None:\n",
        "      all_stats_dict = {}\n",
        "      all_stats_dict['total_time'] = self.total_time\n",
        "      all_stats_dict['round_number'] = ROUND_NUMBER\n",
        "      all_stats_dict.update(self.hyperparams) # dict.update(dict) merges two dictionaries\n",
        "      all_stats_dict.update(self.scores)\n",
        "      DECIMALS = 4 \n",
        "      for key in all_stats_dict.keys():\n",
        "          try:\n",
        "            all_stats_dict[key] = [round(all_stats_dict[key],DECIMALS)]\n",
        "          except:\n",
        "            all_stats_dict[key] = [all_stats_dict[key]]\n",
        "\n",
        "      self.params_scores_df = pd.DataFrame.from_dict(all_stats_dict)\n",
        "\n",
        "  \n",
        "  def save_hyperparams_scores_to_google_drive_tabular(self)-> None:\n",
        "    \"\"\"\n",
        "        Appends this model's scores into your Google Drive with the other scores.\n",
        "    \"\"\"\n",
        "    self.create_params_scores_df()\n",
        "    # try to load that current df into memory\n",
        "    disk_df = pd.read_csv(PATH_TO_SAVE_SCORES)\n",
        "    print(f'Read in new saved scores with {disk_df.shape} shape')\n",
        "    new_updated_disk_df = merge_dfs_horizontally(disk_df, self.params_scores_df)\n",
        "    print(f'added next line of scores with {new_updated_disk_df.shape} shape')\n",
        "    new_updated_disk_df.to_csv(PATH_TO_SAVE_SCORES, index=False)\n",
        "    print('Overwrote the new_updated_disk_df to your Google Drive')\n",
        "\n",
        "    try:\n",
        "      with open(PATH_TO_SAVE_SCORES, 'r') as scores_file:\n",
        "          lines = scores_file.readlines()\n",
        "          if len(lines) == 0:\n",
        "            print(\"the file does not exist. You are good to save your first score df\")\n",
        "    except:\n",
        "      self.params_scores_df.to_csv(PATH_TO_SAVE_SCORES, index=False)\n",
        "      # not exhaustively tested   \n",
        "           \n",
        "\n",
        "def merge_dfs_horizontally(df1 : pd.DataFrame, df2: pd.DataFrame)-> pd.DataFrame:\n",
        "  merged_df = pd.concat([df1, df2], axis=0)\n",
        "  return merged_df\n",
        "\n",
        "\n",
        "def train_LGBMRegressor(params: dict, train_data): \n",
        "  \"\"\"\n",
        "  Inputs: a dict of hyper paramaters for the model, \n",
        "  train_data: a pd.DataFrame of the Training Data\n",
        "\n",
        "  Returns a trained model\n",
        "  \"\"\"\n",
        "  model = lgb.LGBMRegressor(**params) \n",
        "  model.fit(train_data[FEATURES], train_data[TARGET])\n",
        "  return model"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bWSBkk2_m3h"
      },
      "source": [
        "#### Methods to Determine Validation Scores\n",
        "\n",
        "1. I did not write these. I added the English comments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szXbJM0mMiKJ"
      },
      "source": [
        "# naming conventions\n",
        "PREDICTION_NAME = 'prediction'\n",
        "TARGET_NAME = TARGET # 'target is the string named 'target'\n",
        "# EXAMPLE_PRED = 'example_prediction'\n",
        "\n",
        "# ---------------------------\n",
        "# Functions\n",
        "# ---------------------------\n",
        "def valid4score(valid : pd.DataFrame, pred : np.ndarray, load_example: bool=True, save : bool=False) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Generate new valid pandas dataframe for computing scores\n",
        "    \n",
        "    :INPUT:\n",
        "    - valid : pd.DataFrame extracted from tournament data (data_type='validation')\n",
        "    \n",
        "    \"\"\"\n",
        "    valid_df = valid.copy() # the validation dataframe you use this to test the CORR and other values\n",
        "\n",
        "    # Your model creates an array of floats [0,1] rank method converst them in a list of ints. \n",
        "\n",
        "    # your lis tof ints is then compared to their list of ints. \n",
        "    valid_df['prediction'] = pd.Series(pred).rank(pct=True, method=\"first\") # pred is the array of predictions your model creates for the set of validation vectors.  \n",
        "    # I am unsure if this preds is a float only only between 0,1,2,3,4. \n",
        "    valid_df.rename(columns={TARGET: 'target'}, inplace=True)\n",
        "    \n",
        "    # I don't know what the load example boolean is. I think you can use this to save predictions.\n",
        "    if load_example:\n",
        "        valid_df[EXAMPLE_PRED] = pd.read_csv(EXP_DIR + 'valid_df.csv')['prediction'].values\n",
        "    \n",
        "    if save==True:\n",
        "        valid_df.to_csv(OUTPUT_DIR + 'valid_df.csv', index=False)\n",
        "        print('Validation dataframe saved!')\n",
        "    \n",
        "    return valid_df\n",
        "\n",
        "def compute_corr(valid_df : pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Compute rank correlation\n",
        "\n",
        "    THIS IS WHAT YOU ARE PRIMARILY PAID ON \n",
        "    \n",
        "    :INPUT:\n",
        "    - valid_df : pd.DataFrame where at least 2 columns ('prediction' & 'target') exist\n",
        "    \n",
        "    \"\"\"\n",
        "    # this uses Person Correilation. \n",
        "    # I You are paid on spearman corrilation. That is where the ratio of change is important not the raw amount of change\n",
        "    # see: https://support.minitab.com/en-us/minitab-express/1/help-and-how-to/modeling-statistics/regression/supporting-topics/basics/a-comparison-of-the-pearson-and-spearman-correlation-methods/\n",
        "    return np.corrcoef(valid_df[\"target\"], valid_df['prediction'])[0, 1]\n",
        "\n",
        "def compute_max_drawdown(validation_correlations : pd.Series):\n",
        "    \"\"\"\n",
        "    Compute max drawdown\n",
        "    \n",
        "    :INPUT:\n",
        "    - validation_correaltions : pd.Series\n",
        "    \"\"\"\n",
        "    \n",
        "    rolling_max = (validation_correlations + 1).cumprod().rolling(window=100, min_periods=1).max()\n",
        "    daily_value = (validation_correlations + 1).cumprod()\n",
        "    max_drawdown = -(rolling_max - daily_value).max()\n",
        "    \n",
        "    return max_drawdown\n",
        "\n",
        "def compute_val_corr(valid_df : pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Compute rank correlation for valid periods\n",
        "    \n",
        "    :INPUT:\n",
        "    - valid_df : pd.DataFrame where at least 2 columns ('prediction' & 'target') exist\n",
        "    \"\"\"\n",
        "    \n",
        "    # all validation\n",
        "    correlation = compute_corr(valid_df)\n",
        "    #print(\"rank corr = {:.4f}\".format(correlation))\n",
        "    return correlation\n",
        "    \n",
        "def compute_val_sharpe(valid_df : pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Compute sharpe ratio for valid periods\n",
        "    \n",
        "    :INPUT:\n",
        "    - valid_df : pd.DataFrame where at least 2 columns ('prediction' & 'target') exist\n",
        "    \"\"\"\n",
        "    # all validation\n",
        "    d = valid_df.groupby('era')[['target', 'prediction']].corr().iloc[0::2,-1].reset_index()\n",
        "    me = d['prediction'].mean()\n",
        "    sd = d['prediction'].std()\n",
        "    max_drawdown = compute_max_drawdown(d['prediction'])\n",
        "    #print('sharpe ratio = {:.4f}, corr mean = {:.4f}, corr std = {:.4f}, max drawdown = {:.4f}'.format(me / sd, me, sd, max_drawdown))\n",
        "    \n",
        "    return me / sd, me, sd, max_drawdown\n",
        "    \n",
        "def feature_exposures(valid_df : pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Compute feature exposure\n",
        "    \n",
        "    :INPUT:\n",
        "    - valid_df : pd.DataFrame where at least 2 columns ('prediction' & 'target') exist\n",
        "    \"\"\"\n",
        "    feature_names = [f for f in valid_df.columns\n",
        "                     if f.startswith(\"feature\")]\n",
        "    exposures = []\n",
        "    for f in feature_names:\n",
        "        fe = spearmanr(valid_df['prediction'], valid_df[f])[0]\n",
        "        exposures.append(fe)\n",
        "    return np.array(exposures)\n",
        "\n",
        "def max_feature_exposure(fe : np.ndarray):\n",
        "    return np.max(np.abs(fe))\n",
        "\n",
        "def feature_exposure(fe : np.ndarray):\n",
        "    return np.sqrt(np.mean(np.square(fe)))\n",
        "\n",
        "def compute_val_feature_exposure(valid_df : pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Compute feature exposure for valid periods\n",
        "    \n",
        "    :INPUT:\n",
        "    - valid_df : pd.DataFrame where at least 2 columns ('prediction' & 'target') exist\n",
        "    \"\"\"\n",
        "    # all validation\n",
        "    fe = feature_exposures(valid_df)\n",
        "    fe1, fe2 = feature_exposure(fe), max_feature_exposure(fe)\n",
        "    #print('feature exposure = {:.4f}, max feature exposure = {:.4f}'.format(fe1, fe2))\n",
        "     \n",
        "    return fe1, fe2\n",
        "\n",
        "# to neutralize a column in a df by many other columns\n",
        "#         I have no idea what this method does. -P. need to read about it and write up a link to it. \n",
        "def neutralize(df, columns, by, proportion=1.0):\n",
        "    scores = df.loc[:, columns]\n",
        "    exposures = df[by].values\n",
        "\n",
        "    # constant column to make sure the series is completely neutral to exposures\n",
        "    exposures = np.hstack(\n",
        "        (exposures,\n",
        "         np.asarray(np.mean(scores)) * np.ones(len(exposures)).reshape(-1, 1)))\n",
        "\n",
        "    scores = scores - proportion * exposures.dot(\n",
        "        np.linalg.pinv(exposures).dot(scores))\n",
        "    return scores / scores.std()\n",
        "\n",
        "\n",
        "# to neutralize any series by any other series\n",
        "def neutralize_series(series, by, proportion=1.0):\n",
        "    scores = series.values.reshape(-1, 1)\n",
        "    exposures = by.values.reshape(-1, 1)\n",
        "\n",
        "    # this line makes series neutral to a constant column so that it's centered and for sure gets corr 0 with exposures\n",
        "    exposures = np.hstack(\n",
        "        (exposures,\n",
        "         np.array([np.mean(series)] * len(exposures)).reshape(-1, 1)))\n",
        "\n",
        "    correction = proportion * (exposures.dot(\n",
        "        np.linalg.lstsq(exposures, scores, rcond=None)[0]))\n",
        "    corrected_scores = scores - correction\n",
        "    neutralized = pd.Series(corrected_scores.ravel(), index=series.index)\n",
        "    return neutralized\n",
        "\n",
        "\n",
        "def unif(df):\n",
        "    x = (df.rank(method=\"first\") - 0.5) / len(df)\n",
        "    return pd.Series(x, index=df.index)\n",
        "\n",
        "def get_feature_neutral_mean(df):\n",
        "    feature_cols = [c for c in df.columns if c.startswith(\"feature\")]\n",
        "    df.loc[:, \"neutral_sub\"] = neutralize(df, [PREDICTION_NAME],\n",
        "                                          feature_cols)[PREDICTION_NAME]\n",
        "    scores = df.groupby(\"era\").apply(\n",
        "        lambda x: np.corrcoef(x[\"neutral_sub\"].rank(pct=True, method=\"first\"), x[TARGET_NAME])).mean()\n",
        "    return np.mean(scores)\n",
        "\n",
        "def compute_val_mmc(valid_df : pd.DataFrame):    \n",
        "    # MMC over validation\n",
        "    mmc_scores = []\n",
        "    corr_scores = []\n",
        "    for _, x in valid_df.groupby(\"era\"):\n",
        "        series = neutralize_series(pd.Series(unif(x[PREDICTION_NAME])),\n",
        "                                   pd.Series(unif(x[EXAMPLE_PRED])))\n",
        "        mmc_scores.append(np.cov(series, x[TARGET_NAME])[0, 1] / (0.29 ** 2)) # I have no idea what htis line does (0.29 ** 2)\n",
        "        corr_scores.append(np.corrcoef(unif(x[PREDICTION_NAME]).rank(pct=True, method=\"first\"), x[TARGET_NAME]))\n",
        "\n",
        "    val_mmc_mean = np.mean(mmc_scores)\n",
        "    val_mmc_std = np.std(mmc_scores)\n",
        "    val_mmc_sharpe = val_mmc_mean / val_mmc_std\n",
        "    corr_plus_mmcs = [c + m for c, m in zip(corr_scores, mmc_scores)]\n",
        "    corr_plus_mmc_sharpe = np.mean(corr_plus_mmcs) / np.std(corr_plus_mmcs)\n",
        "    corr_plus_mmc_mean = np.mean(corr_plus_mmcs)\n",
        "\n",
        "    #print(\"MMC Mean = {:.6f}, MMC Std = {:.6f}, CORR+MMC Sharpe = {:.4f}\".format(val_mmc_mean, val_mmc_std, corr_plus_mmc_sharpe))\n",
        "\n",
        "    # Check correlation with example predictions\n",
        "    corr_with_example_preds = np.corrcoef(valid_df[EXAMPLE_PRED].rank(pct=True, method=\"first\"),\n",
        "                                          valid_df[PREDICTION_NAME].rank(pct=True, method=\"first\"))[0, 1]\n",
        "    #print(\"Corr with example preds: {:.4f}\".format(corr_with_example_preds))\n",
        "    \n",
        "    return val_mmc_mean, val_mmc_std, corr_plus_mmc_sharpe, corr_with_example_preds\n",
        "\n",
        "\n",
        "# this is the main method. The rest are just called interanlly. \n",
        "def score_summary(valid_df : pd.DataFrame):\n",
        "    score_dict = {}\n",
        "    \n",
        "    try:\n",
        "        score_dict['correlation'] = compute_val_corr(valid_df)\n",
        "    except:\n",
        "        print('ERR: computing correlation')\n",
        "    try:\n",
        "        score_dict['corr_sharpe'], score_dict['corr_mean'], score_dict['corr_std'], score_dict['max_drawdown'] = compute_val_sharpe(valid_df)\n",
        "    except:\n",
        "        print('ERR: computing sharpe')\n",
        "    try:\n",
        "        score_dict['feature_exposure'], score_dict['max_feature_exposure'] = compute_val_feature_exposure(valid_df)\n",
        "    except:\n",
        "        print('ERR: computing feature exposure')\n",
        "    # try:\n",
        "    #     score_dict['mmc_mean'], score_dict['mmc_std'], score_dict['corr_mmc_sharpe'], score_dict['corr_with_example_xgb'] = compute_val_mmc(valid_df)\n",
        "    # except:\n",
        "    #     print('ERR: computing MMC')\n",
        "    \n",
        "    return score_dict"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lBD0oUmBWzx"
      },
      "source": [
        "### Main to train and track time, hyper parmas and scores."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80k6-VBtBWhf"
      },
      "source": [
        "def train_validate_store(params:dict, train_data: pd.DataFrame, validation_data: pd.DataFrame):\n",
        "  \"\"\"\n",
        "    Create a LGBM model based on the hyper paramters in params trained on train_data.\n",
        "    Compute validation scores from the validation_data.\n",
        "    Append the hyperparams and scores to a .csv file in your Google Drive.\n",
        "  \"\"\"\n",
        "  print('3')\n",
        "  start_time = datetime.datetime.now()\n",
        "  my_model = train_LGBMRegressor(params=params, train_data=train_data)\n",
        "  my_predictions = my_model.predict(validation_data[FEATURES])\n",
        "  valid_df = valid4score(validation_data, my_predictions, load_example=False, save=False)\n",
        "  my_scores = score_summary(valid_df)\n",
        "  my_total_time = (datetime.datetime.now() - start_time).total_seconds() \n",
        "  my_model_stats = ModelStats(model=my_model, scores=my_scores, total_time=my_total_time)\n",
        "  my_model_stats.save_hyperparams_scores_to_google_drive_tabular()\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mf0BRd77fZMA"
      },
      "source": [
        "# Debugging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HgmlpiI6RTfd",
        "outputId": "095b08ee-bdb5-444f-c922-994d651f935a"
      },
      "source": [
        "def tester():\n",
        "  \"\"\"\n",
        "  This is a simple way of making sure that the methods work.\n",
        "  \"\"\"\n",
        "  testing_param = {\n",
        "                  'n_estimators': 112,\n",
        "                  'objective': 'regression',\n",
        "                  'boosting_type': 'gbdt',\n",
        "                  'max_depth': 4,\n",
        "                  'learning_rate': .1,\n",
        "                  'feature_fraction': .01, \n",
        "                  'seed': 42 \n",
        "                  }\n",
        "  train_validate_store(testing_param,TRAINING_DATA,VALIDATION_DATA)\n",
        "\n",
        "tester()\n"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3\n",
            "Read in new saved scores with (4, 31) shape\n",
            "added next line of scores with (5, 31) shape\n",
            "Overwrote the new_updated_disk_df to your Google Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "K2tMd34z-F2r",
        "outputId": "0215da37-905b-4679-a91f-0c5cced6f4cc"
      },
      "source": [
        "def load_saved_params():\n",
        "  df = pd.read_csv(PATH_TO_SAVE_SCORES)\n",
        "  print(df.columns)\n",
        "  return df\n",
        "\n",
        "df = load_saved_params()\n",
        "df.loc[:,['correlation', 'corr_sharpe']]"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index(['total_time', 'round_number', 'boosting_type', 'class_weight',\n",
            "       'colsample_bytree', 'importance_type', 'learning_rate', 'max_depth',\n",
            "       'min_child_samples', 'min_child_weight', 'min_split_gain',\n",
            "       'n_estimators', 'n_jobs', 'num_leaves', 'objective', 'random_state',\n",
            "       'reg_alpha', 'reg_lambda', 'silent', 'subsample', 'subsample_for_bin',\n",
            "       'subsample_freq', 'feature_fraction', 'seed', 'correlation',\n",
            "       'corr_sharpe', 'corr_mean', 'corr_std', 'max_drawdown',\n",
            "       'feature_exposure', 'max_feature_exposure'],\n",
            "      dtype='object')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>correlation</th>\n",
              "      <th>corr_sharpe</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0131</td>\n",
              "      <td>0.4367</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0136</td>\n",
              "      <td>0.4422</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0139</td>\n",
              "      <td>0.4512</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0154</td>\n",
              "      <td>0.5276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0152</td>\n",
              "      <td>0.5255</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   correlation  corr_sharpe\n",
              "0       0.0131       0.4367\n",
              "1       0.0136       0.4422\n",
              "2       0.0139       0.4512\n",
              "3       0.0154       0.5276\n",
              "4       0.0152       0.5255"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQM5saclfXq6",
        "outputId": "53bcb7bb-ceaa-47a7-b27c-b6970941af88"
      },
      "source": [
        "%%time\n",
        "\n",
        "# I used this to look around at round 260\n",
        "def explore_param_space1():\n",
        "  param_set=[]\n",
        "  for depth in range(4,6):\n",
        "    for learning_rate in range(24,34,1):\n",
        "      for feature_fraction in range(95,120,5):\n",
        "        param_set.append({\n",
        "              'n_estimators': 3000,\n",
        "              'objective': 'regression',\n",
        "              'boosting_type': 'gbdt',\n",
        "              'max_depth': depth,\n",
        "              'learning_rate': learning_rate / 1000,\n",
        "              'feature_fraction': feature_fraction/ 1000, \n",
        "              'seed': 422 \n",
        "                })\n",
        "\n",
        "  record_df = None\n",
        "\n",
        "  for param in param_set:\n",
        "    my_model_stats = train_validate_store(param, \n",
        "                        train_data=TRAINING_DATA, \n",
        "                        validation_data = VALIDATION_DATA, \n",
        "                        features=FEATURES)\n",
        "    if type(record_df) == None:\n",
        "      record_df = create_DataFrame_from_ModelStats(my_model_stats) # also include a try pd.load(csv, index =false)\n",
        "    else:\n",
        "      new_df = create_DataFrame_from_ModelStats(my_model_stats)\n",
        "      record_df = merge_df_vertically(record_df,new_df)\n",
        "      # overwrite the record_df saved in drive\n",
        "    record_df.to_csv(f'/content/drive/MyDrive/{ROUND_NUMBER}v3scores.csv', index=False)\n",
        "    print(record_df.shape) # might also want to print the corr socres\n",
        "    print(new_df['correlation'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 31)\n",
            "(2, 31)\n",
            "(3, 31)\n",
            "(4, 31)\n",
            "(5, 31)\n",
            "(6, 31)\n",
            "(7, 31)\n",
            "(8, 31)\n",
            "(9, 31)\n",
            "(10, 31)\n",
            "(11, 31)\n",
            "(12, 31)\n",
            "(13, 31)\n",
            "(14, 31)\n",
            "(15, 31)\n",
            "(16, 31)\n",
            "(17, 31)\n",
            "(18, 31)\n",
            "(19, 31)\n",
            "(20, 31)\n",
            "(21, 31)\n",
            "(22, 31)\n",
            "(23, 31)\n",
            "(24, 31)\n",
            "(25, 31)\n",
            "(26, 31)\n",
            "(27, 31)\n",
            "(28, 31)\n",
            "(29, 31)\n",
            "(30, 31)\n",
            "(31, 31)\n",
            "(32, 31)\n",
            "(33, 31)\n",
            "(34, 31)\n",
            "(35, 31)\n",
            "(36, 31)\n",
            "(37, 31)\n",
            "(38, 31)\n",
            "(39, 31)\n",
            "(40, 31)\n",
            "(41, 31)\n",
            "(42, 31)\n",
            "(43, 31)\n",
            "(44, 31)\n",
            "(45, 31)\n",
            "(46, 31)\n",
            "(47, 31)\n",
            "(48, 31)\n",
            "(49, 31)\n",
            "(50, 31)\n",
            "(51, 31)\n",
            "(52, 31)\n",
            "(53, 31)\n",
            "(54, 31)\n",
            "(55, 31)\n",
            "(56, 31)\n",
            "(57, 31)\n",
            "(58, 31)\n",
            "(59, 31)\n",
            "(60, 31)\n",
            "(61, 31)\n",
            "(62, 31)\n",
            "(63, 31)\n",
            "(64, 31)\n",
            "(65, 31)\n",
            "(66, 31)\n",
            "(67, 31)\n",
            "(68, 31)\n",
            "(69, 31)\n",
            "(70, 31)\n",
            "(71, 31)\n",
            "(72, 31)\n",
            "(73, 31)\n",
            "(74, 31)\n",
            "(75, 31)\n",
            "(76, 31)\n",
            "(77, 31)\n",
            "(78, 31)\n",
            "(79, 31)\n",
            "(80, 31)\n",
            "(81, 31)\n",
            "(82, 31)\n",
            "(83, 31)\n",
            "(84, 31)\n",
            "(85, 31)\n",
            "(86, 31)\n",
            "(87, 31)\n",
            "(88, 31)\n",
            "(89, 31)\n",
            "(90, 31)\n",
            "(91, 31)\n",
            "(92, 31)\n",
            "(93, 31)\n",
            "(94, 31)\n",
            "(95, 31)\n",
            "(96, 31)\n",
            "(97, 31)\n",
            "(98, 31)\n",
            "(99, 31)\n",
            "(100, 31)\n",
            "CPU times: user 15h 42min 43s, sys: 1min 36s, total: 15h 44min 20s\n",
            "Wall time: 4h 16min 44s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZHh0Tsw4CcF",
        "outputId": "9865fb9c-c735-455a-b601-f0e01a340eca"
      },
      "source": [
        "record_df['correlation']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    0.0243\n",
              "0    0.0240\n",
              "0    0.0244\n",
              "0    0.0238\n",
              "0    0.0229\n",
              "      ...  \n",
              "0    0.0232\n",
              "0    0.0223\n",
              "0    0.0220\n",
              "0    0.0231\n",
              "0    0.0231\n",
              "Name: correlation, Length: 100, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSo8yDCxTBB1"
      },
      "source": [
        "### Methods to handle submission"
      ]
    }
  ]
}